{"paragraphs":[{"text":"%md\n\n# 基于spark的大数据日志文件分析\n\n大数据日志分析是互联网场景中非常常见的应用，在很多应用中，我们都会持续不断地把信息写入日志文件中，而后续的一些数据驱动的方法(比如数据分析、数据AI建模)都依赖于从日志中解析出来的结构化数据。\n\n由于互联网的业务体量，通常日志文件非常大，我们都需要使用大数据的相关工具进行解析和信息抽取，包括可以完成大体量数据下的数据分析和可视化，这里就介绍基于spark的DataFrame形态数据，我们如何对大数据进行分析，也为后续的各种各样的算法应用(比如推荐系统、风控反作弊、商业智能)等提供数据支撑。","user":"anonymous","dateUpdated":"2019-03-01T16:17:03+0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>基于spark的大数据日志文件分析</h1>\n<p>大数据日志分析是互联网场景中非常常见的应用，在很多应用中，我们都会持续不断地把信息写入日志文件中，而后续的一些数据驱动的方法(比如数据分析、数据AI建模)都依赖于从日志中解析出来的结构化数据。</p>\n<p>由于互联网的业务体量，通常日志文件非常大，我们都需要使用大数据的相关工具进行解析和信息抽取，包括可以完成大体量数据下的数据分析和可视化，这里就介绍基于spark的DataFrame形态数据，我们如何对大数据进行分析，也为后续的各种各样的算法应用(比如推荐系统、风控反作弊、商业智能)等提供数据支撑。</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542508069125_-1705364524","id":"20160721-205608_398995935","dateCreated":"2018-11-18T10:27:49+0800","dateStarted":"2019-03-01T16:17:03+0800","dateFinished":"2019-03-01T16:17:03+0800","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:317"},{"text":"%md\n\n## 内容大纲\n\n整个大数据清洗、处理 和 分析的过程，分成了5部分:\n\n* Part 1: 背景介绍与工具库引入\n* Part 2: 数据一览与解析\n* Part 3: 日志总体分析\n* Part 4: 日志细节字段挖掘与时序分析\n* Part 5: 关于返回404状态码的请求分析","user":"anonymous","dateUpdated":"2019-03-01T16:24:54+0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>内容大纲</h2>\n<p>整个大数据清洗、处理 和 分析的过程，分成了5部分:</p>\n<ul>\n  <li>Part 1: 背景介绍与工具库引入</li>\n  <li>Part 2: 数据一览与解析</li>\n  <li>Part 3: 日志总体分析</li>\n  <li>Part 4: 日志细节字段挖掘与时序分析</li>\n  <li>Part 5: 关于返回404状态码的请求分析</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1542508069126_65150430","id":"20160721-205649_1921894344","dateCreated":"2018-11-18T10:27:49+0800","dateStarted":"2019-03-01T16:24:54+0800","dateFinished":"2019-03-01T16:24:54+0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:318"},{"text":"%md\n\n## Part 1: 背景介绍与工具库引入\n\n这是一个很典型的互联网大数据日志分析问题，我们这里需要用到一些工具库，进而完成数据的解析，包括：\n\n* datetime:对时间日期处理的python工具库\n* re: 正则表达式工具库","user":"anonymous","dateUpdated":"2019-03-01T16:26:28+0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Part 1: 背景介绍与工具库引入</h2>\n<p>这是一个很典型的互联网大数据日志分析问题，我们这里需要用到一些工具库，进而完成数据的解析，包括：</p>\n<ul>\n  <li>datetime:对时间日期处理的python工具库</li>\n  <li>re: 正则表达式工具库</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1542508069127_1587011527","id":"20160721-210321_1736487949","dateCreated":"2018-11-18T10:27:49+0800","dateStarted":"2019-03-01T16:26:28+0800","dateFinished":"2019-03-01T16:26:28+0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:319"},{"text":"%sh\n\n#wget -O apache.access.log.PROJECT.gz https://www.dropbox.com/s/n13homy2ssyd1n2/apache.access.log.PROJECT.gz?dl=0\n\nrm -f apache.access.log.PROJECT\ngunzip apache.access.log.PROJECT.gz","user":"anonymous","dateUpdated":"2019-03-01T11:30:02+0800","config":{"colWidth":12,"editorMode":"ace/mode/sh","results":[],"enabled":true,"fontSize":9,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1542508069128_-1855662837","id":"20160725-204705_200637156","dateCreated":"2018-11-18T10:27:49+0800","dateStarted":"2019-03-01T11:30:02+0800","dateFinished":"2019-03-01T11:30:02+0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:320"},{"text":"%sh\n\nhdfs dfs -rm -f -skipTrash /tmp/apache.access.log.PROJECT\nhdfs dfs -put apache.access.log.PROJECT /tmp\nhdfs dfs -ls /tmp/apache.access.log.PROJECT\n","user":"anonymous","dateUpdated":"2019-03-01T16:26:55+0800","config":{"colWidth":12,"editorMode":"ace/mode/sh","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Deleted /tmp/apache.access.log.PROJECT\n-rw-r-----   2 root hadoop  111503503 2019-03-01 16:26 /tmp/apache.access.log.PROJECT\n"}]},"apps":[],"jobName":"paragraph_1542508069128_86845153","id":"20160721-202134_1390334492","dateCreated":"2018-11-18T10:27:49+0800","dateStarted":"2019-03-01T16:26:55+0800","dateFinished":"2019-03-01T16:27:02+0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:321"},{"text":"%spark\n\nsc\nsqlContext","user":"anonymous","dateUpdated":"2019-03-01T16:33:40+0800","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res4: org.apache.spark.SparkContext = org.apache.spark.SparkContext@e1da65a\nres5: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@21cc610a\n"}]},"apps":[],"jobName":"paragraph_1542508069129_-2139513911","id":"20160724-010528_127523330","dateCreated":"2018-11-18T10:27:49+0800","dateStarted":"2019-03-01T16:33:40+0800","dateFinished":"2019-03-01T16:34:10+0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:322"},{"text":"%pyspark\nimport datetime\n\n# 测试工具库\nprint 'This was last run on: {0}'.format(datetime.datetime.now())","user":"anonymous","dateUpdated":"2019-03-01T16:33:47+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"This was last run on: 2019-03-01 16:34:10.647261\n"}]},"apps":[],"jobName":"paragraph_1542508069130_-23671943","id":"20160718-191223_2111119137","dateCreated":"2018-11-18T10:27:49+0800","dateStarted":"2019-03-01T16:33:48+0800","dateFinished":"2019-03-01T16:34:10+0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:323"},{"text":"%pyspark\n# 测试正则表达式\nimport re\n\nm = re.search('(?<=abc)def', 'abcdef')\n\nprint m.group(0)","user":"anonymous","dateUpdated":"2019-03-01T16:34:37+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"def\n"}]},"apps":[],"jobName":"paragraph_1542508069130_-1747273439","id":"20160718-190935_95514384","dateCreated":"2018-11-18T10:27:49+0800","dateStarted":"2019-03-01T11:30:57+0800","dateFinished":"2019-03-01T11:30:57+0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:324"},{"text":"%md\n\n## Part 2:数据一览与解析\n\n下一步我们就可以在ready的环境中开始进行数据的加载和解析了，这里用到的日志是NASA Kennedy Space Center的web服务器日志。整个日志文件大家可以在<http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html>取到，它包含了2个月的完整HTTP请求。我们这里给大家讲解，用到其中的一部分日志。","user":"anonymous","dateUpdated":"2019-03-01T16:36:40+0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Part 2:数据一览与解析</h2>\n<p>下一步我们就可以在ready的环境中开始进行数据的加载和解析了，这里用到的日志是NASA Kennedy Space Center的web服务器日志。整个日志文件大家可以在<a href=\"http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html\">http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html</a>取到，它包含了2个月的完整HTTP请求。我们这里给大家讲解，用到其中的一部分日志。</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542508069131_-542205860","id":"20160721-210530_1635089066","dateCreated":"2018-11-18T10:27:49+0800","dateStarted":"2019-03-01T16:36:40+0800","dateFinished":"2019-03-01T16:36:42+0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:325"},{"text":"%md\n\n### (2a) 加载日志文件\n\n我们把数据加载成大家熟悉和方便处理的DataFrame数据格式。","user":"anonymous","dateUpdated":"2019-03-01T16:37:20+0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>(2a) 加载日志文件</h3>\n<p>我们把数据加载成大家熟悉和方便处理的DataFrame数据格式。</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542508069132_-1271027312","id":"20160721-210741_1305148056","dateCreated":"2018-11-18T10:27:49+0800","dateStarted":"2019-03-01T16:37:20+0800","dateFinished":"2019-03-01T16:37:20+0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:326"},{"text":"%pyspark\n\n# Specify path to downloaded log file\nimport sys\nimport os\n\nlog_file_path = 'hdfs:///' + os.path.join('tmp', 'apache.access.log.PROJECT')\n\nprint log_file_path\n","user":"anonymous","dateUpdated":"2019-03-01T11:31:10+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"hdfs:///tmp/apache.access.log.PROJECT\n"}]},"apps":[],"jobName":"paragraph_1542508069133_471262833","id":"20160718-191015_2140118453","dateCreated":"2018-11-18T10:27:49+0800","dateStarted":"2019-03-01T11:31:10+0800","dateFinished":"2019-03-01T11:31:10+0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:327"},{"text":"%pyspark\n\nbase_df =  sqlContext.read.text(log_file_path)\n# Let's look at the schema\nbase_df.printSchema()","user":"anonymous","dateUpdated":"2019-03-01T11:31:16+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- value: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1542508069133_-250112721","id":"20160718-195018_1400250065","dateCreated":"2018-11-18T10:27:49+0800","dateStarted":"2019-03-01T11:31:16+0800","dateFinished":"2019-03-01T11:31:19+0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:328"},{"text":"%md\n\nLet's take a look at some of the data","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Let's take a look at some of the data</p>\n"}]},"apps":[],"jobName":"paragraph_1542508069134_25766518","id":"20160721-210826_262995281","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:329"},{"text":"%pyspark\n\nbase_df.show(truncate=False)","user":"anonymous","dateUpdated":"2019-03-01T11:31:23+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------------------------------------------------------------------------------------------------------------------------+\n|value                                                                                                                          |\n+-------------------------------------------------------------------------------------------------------------------------------+\n|in24.inetnebr.com - - [01/Aug/1995:00:00:01 -0400] \"GET /shuttle/missions/sts-68/news/sts-68-mcc-05.txt HTTP/1.0\" 200 1839     |\n|uplherc.upl.com - - [01/Aug/1995:00:00:07 -0400] \"GET / HTTP/1.0\" 304 0                                                        |\n|uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] \"GET /images/ksclogo-medium.gif HTTP/1.0\" 304 0                               |\n|uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] \"GET /images/MOSAIC-logosmall.gif HTTP/1.0\" 304 0                             |\n|uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] \"GET /images/USA-logosmall.gif HTTP/1.0\" 304 0                                |\n|ix-esc-ca2-07.ix.netcom.com - - [01/Aug/1995:00:00:09 -0400] \"GET /images/launch-logo.gif HTTP/1.0\" 200 1713                   |\n|uplherc.upl.com - - [01/Aug/1995:00:00:10 -0400] \"GET /images/WORLD-logosmall.gif HTTP/1.0\" 304 0                              |\n|slppp6.intermind.net - - [01/Aug/1995:00:00:10 -0400] \"GET /history/skylab/skylab.html HTTP/1.0\" 200 1687                      |\n|piweba4y.prodigy.com - - [01/Aug/1995:00:00:10 -0400] \"GET /images/launchmedium.gif HTTP/1.0\" 200 11853                        |\n|slppp6.intermind.net - - [01/Aug/1995:00:00:11 -0400] \"GET /history/skylab/skylab-small.gif HTTP/1.0\" 200 9202                 |\n|slppp6.intermind.net - - [01/Aug/1995:00:00:12 -0400] \"GET /images/ksclogosmall.gif HTTP/1.0\" 200 3635                         |\n|ix-esc-ca2-07.ix.netcom.com - - [01/Aug/1995:00:00:12 -0400] \"GET /history/apollo/images/apollo-logo1.gif HTTP/1.0\" 200 1173   |\n|slppp6.intermind.net - - [01/Aug/1995:00:00:13 -0400] \"GET /history/apollo/images/apollo-logo.gif HTTP/1.0\" 200 3047           |\n|uplherc.upl.com - - [01/Aug/1995:00:00:14 -0400] \"GET /images/NASA-logosmall.gif HTTP/1.0\" 304 0                               |\n|133.43.96.45 - - [01/Aug/1995:00:00:16 -0400] \"GET /shuttle/missions/sts-69/mission-sts-69.html HTTP/1.0\" 200 10566            |\n|kgtyk4.kj.yamagata-u.ac.jp - - [01/Aug/1995:00:00:17 -0400] \"GET / HTTP/1.0\" 200 7280                                          |\n|kgtyk4.kj.yamagata-u.ac.jp - - [01/Aug/1995:00:00:18 -0400] \"GET /images/ksclogo-medium.gif HTTP/1.0\" 200 5866                 |\n|d0ucr6.fnal.gov - - [01/Aug/1995:00:00:19 -0400] \"GET /history/apollo/apollo-16/apollo-16.html HTTP/1.0\" 200 2743              |\n|ix-esc-ca2-07.ix.netcom.com - - [01/Aug/1995:00:00:19 -0400] \"GET /shuttle/resources/orbiters/discovery.html HTTP/1.0\" 200 6849|\n|d0ucr6.fnal.gov - - [01/Aug/1995:00:00:20 -0400] \"GET /history/apollo/apollo-16/apollo-16-patch-small.gif HTTP/1.0\" 200 14897  |\n+-------------------------------------------------------------------------------------------------------------------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1542508069134_948920566","id":"20160718-195215_169877856","dateCreated":"2018-11-18T10:27:49+0800","dateStarted":"2019-03-01T11:31:24+0800","dateFinished":"2019-03-01T11:31:27+0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:330"},{"text":"%md\n\n### (2b) 解析日志","user":"anonymous","dateUpdated":"2019-03-01T16:37:37+0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>(2b) 解析日志</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1542508069135_-43760057","id":"20160721-212802_988981671","dateCreated":"2018-11-18T10:27:49+0800","dateStarted":"2019-03-01T16:37:37+0800","dateFinished":"2019-03-01T16:37:37+0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:331"},{"text":"%md\n\n接触过web开发的同学，可能会对日志的格式比较熟悉，这里是标准的\n[Common Log Format](https://www.w3.org/Daemon/User/Config/Logging.html#common-logfile-format)日志格式。\n\n包含以下一些字段：\n\nremotehost, rfc931, authuser, [date], \"request\", status, bytes\n\n\n    | field       | meaning                                                                |\n    |-------------|------------------------------------------------------------------------|\n    | remotehost  | Remote hostname (or IP number if DNS hostname is not available).       |\n    | rfc931      | The remote logname of the user. We don't really care about this field. |\n    | authuser    | The username of the remote user, as authenticated by the HTTP server.  |\n    | [date]      | The date and time of the request.                                      |\n    | \"request\"   | The request, exactly as it came from the browser or client.            |\n    | status      | The HTTP status code the server sent back to the client.               |\n    | bytes       | The number of bytes (`Content-Length`) transferred to the client.      |\n\n\n\n下面我们就对其中的每一列进行解析，我们这里会使用到正则表达式工具，对正则表达式不熟悉的同学，可以看看之前的课程内容","user":"anonymous","dateUpdated":"2019-03-01T16:39:59+0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>接触过web开发的同学，可能会对日志的格式比较熟悉，这里是标准的<br/><a href=\"https://www.w3.org/Daemon/User/Config/Logging.html#common-logfile-format\">Common Log Format</a>日志格式。</p>\n<p>包含以下一些字段：</p>\n<p>remotehost, rfc931, authuser, [date], &ldquo;request&rdquo;, status, bytes</p>\n<pre><code>| field       | meaning                                                                |\n|-------------|------------------------------------------------------------------------|\n| remotehost  | Remote hostname (or IP number if DNS hostname is not available).       |\n| rfc931      | The remote logname of the user. We don&#39;t really care about this field. |\n| authuser    | The username of the remote user, as authenticated by the HTTP server.  |\n| [date]      | The date and time of the request.                                      |\n| &quot;request&quot;   | The request, exactly as it came from the browser or client.            |\n| status      | The HTTP status code the server sent back to the client.               |\n| bytes       | The number of bytes (`Content-Length`) transferred to the client.      |\n</code></pre>\n<p>下面我们就对其中的每一列进行解析，我们这里会使用到正则表达式工具，对正则表达式不熟悉的同学，可以看看之前的课程内容</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542508069136_-1666778984","id":"20160721-210934_867340648","dateCreated":"2018-11-18T10:27:49+0800","dateStarted":"2019-03-01T16:39:59+0800","dateFinished":"2019-03-01T16:39:59+0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:332"},{"text":"%pyspark\n\nfrom pyspark.sql.functions import split, regexp_extract\nsplit_df = base_df.select(regexp_extract('value', r'^([^\\s]+\\s)', 1).alias('host'),\n                          regexp_extract('value', r'^.*\\[(\\d\\d/\\w{3}/\\d{4}:\\d{2}:\\d{2}:\\d{2} -\\d{4})]', 1).alias('timestamp'),\n                          regexp_extract('value', r'^.*\"\\w+\\s+([^\\s]+)\\s+HTTP.*\"', 1).alias('path'),\n                          regexp_extract('value', r'^.*\"\\s+([^\\s]+)', 1).cast('integer').alias('status'),\n                          regexp_extract('value', r'^.*\\s+(\\d+)$', 1).cast('integer').alias('content_size'))\nsplit_df.show(truncate=False)\n","user":"anonymous","dateUpdated":"2019-03-01T11:31:33+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------------------------+--------------------------+---------------------------------------------------+------+------------+\n|host                        |timestamp                 |path                                               |status|content_size|\n+----------------------------+--------------------------+---------------------------------------------------+------+------------+\n|in24.inetnebr.com           |01/Aug/1995:00:00:01 -0400|/shuttle/missions/sts-68/news/sts-68-mcc-05.txt    |200   |1839        |\n|uplherc.upl.com             |01/Aug/1995:00:00:07 -0400|/                                                  |304   |0           |\n|uplherc.upl.com             |01/Aug/1995:00:00:08 -0400|/images/ksclogo-medium.gif                         |304   |0           |\n|uplherc.upl.com             |01/Aug/1995:00:00:08 -0400|/images/MOSAIC-logosmall.gif                       |304   |0           |\n|uplherc.upl.com             |01/Aug/1995:00:00:08 -0400|/images/USA-logosmall.gif                          |304   |0           |\n|ix-esc-ca2-07.ix.netcom.com |01/Aug/1995:00:00:09 -0400|/images/launch-logo.gif                            |200   |1713        |\n|uplherc.upl.com             |01/Aug/1995:00:00:10 -0400|/images/WORLD-logosmall.gif                        |304   |0           |\n|slppp6.intermind.net        |01/Aug/1995:00:00:10 -0400|/history/skylab/skylab.html                        |200   |1687        |\n|piweba4y.prodigy.com        |01/Aug/1995:00:00:10 -0400|/images/launchmedium.gif                           |200   |11853       |\n|slppp6.intermind.net        |01/Aug/1995:00:00:11 -0400|/history/skylab/skylab-small.gif                   |200   |9202        |\n|slppp6.intermind.net        |01/Aug/1995:00:00:12 -0400|/images/ksclogosmall.gif                           |200   |3635        |\n|ix-esc-ca2-07.ix.netcom.com |01/Aug/1995:00:00:12 -0400|/history/apollo/images/apollo-logo1.gif            |200   |1173        |\n|slppp6.intermind.net        |01/Aug/1995:00:00:13 -0400|/history/apollo/images/apollo-logo.gif             |200   |3047        |\n|uplherc.upl.com             |01/Aug/1995:00:00:14 -0400|/images/NASA-logosmall.gif                         |304   |0           |\n|133.43.96.45                |01/Aug/1995:00:00:16 -0400|/shuttle/missions/sts-69/mission-sts-69.html       |200   |10566       |\n|kgtyk4.kj.yamagata-u.ac.jp  |01/Aug/1995:00:00:17 -0400|/                                                  |200   |7280        |\n|kgtyk4.kj.yamagata-u.ac.jp  |01/Aug/1995:00:00:18 -0400|/images/ksclogo-medium.gif                         |200   |5866        |\n|d0ucr6.fnal.gov             |01/Aug/1995:00:00:19 -0400|/history/apollo/apollo-16/apollo-16.html           |200   |2743        |\n|ix-esc-ca2-07.ix.netcom.com |01/Aug/1995:00:00:19 -0400|/shuttle/resources/orbiters/discovery.html         |200   |6849        |\n|d0ucr6.fnal.gov             |01/Aug/1995:00:00:20 -0400|/history/apollo/apollo-16/apollo-16-patch-small.gif|200   |14897       |\n+----------------------------+--------------------------+---------------------------------------------------+------+------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1542508069136_782168213","id":"20160718-195552_1528005254","dateCreated":"2018-11-18T10:27:49+0800","dateStarted":"2019-03-01T11:31:33+0800","dateFinished":"2019-03-01T11:31:35+0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:333"},{"text":"%md\n\n### (2c)数据清洗\n\n下面我们需要做一些数据清洗的工作，这里包括缺失值的处理等","user":"anonymous","dateUpdated":"2019-03-01T16:40:42+0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>(2c)数据清洗</h3>\n<p>下面我们需要做一些数据清洗的工作，这里包括缺失值的处理等</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542508069137_1008629739","id":"20160721-212703_1932014155","dateCreated":"2018-11-18T10:27:49+0800","dateStarted":"2019-03-01T16:40:42+0800","dateFinished":"2019-03-01T16:40:42+0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:334"},{"text":"%pyspark\n\nprint base_df.filter(base_df['value'].isNull()).count()","user":"anonymous","dateUpdated":"2019-03-01T11:31:52+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"0\n"}]},"apps":[],"jobName":"paragraph_1542508069138_-1098961749","id":"20160718-200153_1865911463","dateCreated":"2018-11-18T10:27:49+0800","dateStarted":"2019-03-01T11:31:52+0800","dateFinished":"2019-03-01T11:31:55+0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:335"},{"text":"%md\n\n如果我们的日志解析过程写得非常完整和正确，应该不会有缺失值，我们来看一下实际的情况。","user":"anonymous","dateUpdated":"2019-03-01T16:41:24+0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>如果我们的日志解析过程写得非常完整和正确，应该不会有缺失值，我们来看一下实际的情况。</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542508069139_1114149995","id":"20160721-212848_1283299327","dateCreated":"2018-11-18T10:27:49+0800","dateStarted":"2019-03-01T16:41:24+0800","dateFinished":"2019-03-01T16:41:24+0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:336"},{"text":"%pyspark\n\nbad_rows_df = split_df.filter(split_df['host'].isNull() |\n                              split_df['timestamp'].isNull() |\n                              split_df['path'].isNull() |\n                              split_df['status'].isNull() |\n                             split_df['content_size'].isNull())\n\nprint bad_rows_df.count()","user":"anonymous","dateUpdated":"2019-03-01T11:32:01+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"8756\n"}]},"apps":[],"jobName":"paragraph_1542508069139_2006108996","id":"20160718-200217_1205719967","dateCreated":"2018-11-18T10:27:49+0800","dateStarted":"2019-03-01T11:32:01+0800","dateFinished":"2019-03-01T11:32:08+0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:337"},{"text":"%md\n\n实际上有一些数据解析出来了缺失值，我们来看看到底是什么原因导致了这个问题","user":"anonymous","dateUpdated":"2019-03-01T16:41:54+0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>实际上有一些数据解析出来了缺失值，我们来看看到底是什么原因导致了这个问题</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542508069140_1126850245","id":"20160721-212919_864646186","dateCreated":"2018-11-18T10:27:49+0800","dateStarted":"2019-03-01T16:41:54+0800","dateFinished":"2019-03-01T16:41:54+0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:338"},{"text":"%pyspark\n\nfrom pyspark.sql.functions import col, sum\n\ndef count_null(col_name):\n  return sum(col(col_name).isNull().cast('integer')).alias(col_name)\n\n# Build up a list of column expressions, one per colum","user":"anonymous","dateUpdated":"2018-11-18T11:48:48+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1542508069141_852449580","id":"20160718-200250_1860290921","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:339"},{"text":"%pyspark\n\nexprs = []\nfor col_name in split_df.columns:\n  exprs.append(count_null(col_name))\n  \nprint exprs\n","user":"anonymous","dateUpdated":"2018-11-18T11:48:45+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[Column<(sum(cast(isnull(host) as int)),mode=Complete,isDistinct=false) AS host#14>, Column<(sum(cast(isnull(timestamp) as int)),mode=Complete,isDistinct=false) AS timestamp#15>, Column<(sum(cast(isnull(path) as int)),mode=Complete,isDistinct=false) AS path#16>, Column<(sum(cast(isnull(status) as int)),mode=Complete,isDistinct=false) AS status#17>, Column<(sum(cast(isnull(content_size) as int)),mode=Complete,isDistinct=false) AS content_size#18>]\n"}]},"apps":[],"jobName":"paragraph_1542508069141_1497442166","id":"20160718-200341_292449305","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:340"},{"text":"%pyspark\n\nsplit_df.agg(*exprs).show()","user":"anonymous","dateUpdated":"2018-11-18T11:48:41+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----+---------+----+------+------------+\n|host|timestamp|path|status|content_size|\n+----+---------+----+------+------------+\n|   0|        0|   0|     0|        8756|\n+----+---------+----+------+------------+\n\n"}]},"apps":[],"jobName":"paragraph_1542508069142_1071117129","id":"20160718-200402_1743180176","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:341"},{"text":"%md\n\n我们发现缺失值都出现在 `content_size` 字段，我们来看看发生了什么，使得最终的结果有不少缺失值，检查一下我们的原始正则表达式:\n\n```\nregexp_extract('value', r'^.*\\s+(\\d+)$', 1).cast('integer').alias('content_size')\n```\n\n注意到尾部的 `\\d+` 表示选择1个或者多个数字，捕获的是返回数据的大小，说不定有些日志的这个字段并不是标准的数字，我们来看一下\n\n**Note**: In the expression below, `~` means \"not\".","user":"anonymous","dateUpdated":"2019-03-01T16:44:39+0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>我们发现缺失值都出现在 <code>content_size</code> 字段，我们来看看发生了什么，使得最终的结果有不少缺失值，检查一下我们的原始正则表达式:</p>\n<pre><code>regexp_extract(&#39;value&#39;, r&#39;^.*\\s+(\\d+)$&#39;, 1).cast(&#39;integer&#39;).alias(&#39;content_size&#39;)\n</code></pre>\n<p>注意到尾部的 <code>\\d+</code> 表示选择1个或者多个数字，捕获的是返回数据的大小，说不定有些日志的这个字段并不是标准的数字，我们来看一下</p>\n<p><strong>Note</strong>: In the expression below, <code>~</code> means &ldquo;not&rdquo;.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542508069143_-1385373924","id":"20160721-213021_496415433","dateCreated":"2018-11-18T10:27:49+0800","dateStarted":"2019-03-01T16:44:39+0800","dateFinished":"2019-03-01T16:44:39+0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:342"},{"text":"%pyspark\n\nbad_content_size_df = base_df.filter(~ base_df['value'].rlike(r'\\d+$'))\n\nprint bad_content_size_df.count()","user":"anonymous","dateUpdated":"2018-11-18T11:49:40+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"8756\n"}]},"apps":[],"jobName":"paragraph_1542508069144_220805866","id":"20160718-200415_1670913707","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:343"},{"text":"%md\n\n我们发现它的数量和我们没有解析出来的样本数量完全匹配，所以我们找到了问题所在，我们来做一个修正。","user":"anonymous","dateUpdated":"2019-03-01T16:45:26+0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>我们发现它的数量和我们没有解析出来的样本数量完全匹配，所以我们找到了问题所在，我们来做一个修正。</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542508069144_-879022270","id":"20160721-213051_622513747","dateCreated":"2018-11-18T10:27:49+0800","dateStarted":"2019-03-01T16:45:26+0800","dateFinished":"2019-03-01T16:45:26+0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:344"},{"text":"%pyspark\n\nfrom pyspark.sql.functions import lit, concat\n\nbad_content_size_df.select(concat(bad_content_size_df['value'], lit('$'))).show(truncate=False)\n","user":"anonymous","dateUpdated":"2018-11-18T11:49:56+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------------------------------------------------------------------------------------------------------------------------+\n|concat(value,$)                                                                                                             |\n+----------------------------------------------------------------------------------------------------------------------------+\n|gw1.att.com - - [01/Aug/1995:00:03:53 -0400] \"GET /shuttle/missions/sts-73/news HTTP/1.0\" 302 -$                            |\n|js002.cc.utsunomiya-u.ac.jp - - [01/Aug/1995:00:07:33 -0400] \"GET /shuttle/resources/orbiters/discovery.gif HTTP/1.0\" 404 -$|\n|tia1.eskimo.com - - [01/Aug/1995:00:28:41 -0400] \"GET /pub/winvn/release.txt HTTP/1.0\" 404 -$                               |\n|itws.info.eng.niigata-u.ac.jp - - [01/Aug/1995:00:38:01 -0400] \"GET /ksc.html/facts/about_ksc.html HTTP/1.0\" 403 -$         |\n|grimnet23.idirect.com - - [01/Aug/1995:00:50:12 -0400] \"GET /www/software/winvn/winvn.html HTTP/1.0\" 404 -$                 |\n|miriworld.its.unimelb.edu.au - - [01/Aug/1995:01:04:54 -0400] \"GET /history/history.htm HTTP/1.0\" 404 -$                    |\n|ras38.srv.net - - [01/Aug/1995:01:05:14 -0400] \"GET /elv/DELTA/uncons.htm HTTP/1.0\" 404 -$                                  |\n|cs1-06.leh.ptd.net - - [01/Aug/1995:01:17:38 -0400] \"GET /sts-71/launch/\" 404 -$                                            |\n|www-b2.proxy.aol.com - - [01/Aug/1995:01:22:07 -0400] \"GET /shuttle/countdown HTTP/1.0\" 302 -$                              |\n|maui56.maui.net - - [01/Aug/1995:01:31:56 -0400] \"GET /shuttle HTTP/1.0\" 302 -$                                             |\n|dialip-24.athenet.net - - [01/Aug/1995:01:33:02 -0400] \"GET /history/apollo/apollo-13.html HTTP/1.0\" 404 -$                 |\n|h96-158.ccnet.com - - [01/Aug/1995:01:35:50 -0400] \"GET /history/apollo/a-001/a-001-patch-small.gif HTTP/1.0\" 404 -$        |\n|h96-158.ccnet.com - - [01/Aug/1995:01:36:23 -0400] \"GET /history/apollo/a-001/movies/ HTTP/1.0\" 404 -$                      |\n|h96-158.ccnet.com - - [01/Aug/1995:01:36:30 -0400] \"GET /history/apollo/a-001/a-001-patch-small.gif HTTP/1.0\" 404 -$        |\n|h96-158.ccnet.com - - [01/Aug/1995:01:36:38 -0400] \"GET /history/apollo/a-001/movies/ HTTP/1.0\" 404 -$                      |\n|h96-158.ccnet.com - - [01/Aug/1995:01:36:42 -0400] \"GET /history/apollo/a-001/a-001-patch-small.gif HTTP/1.0\" 404 -$        |\n|h96-158.ccnet.com - - [01/Aug/1995:01:36:44 -0400] \"GET /history/apollo/a-001/images/ HTTP/1.0\" 404 -$                      |\n|h96-158.ccnet.com - - [01/Aug/1995:01:36:47 -0400] \"GET /history/apollo/a-001/a-001-patch-small.gif HTTP/1.0\" 404 -$        |\n|h96-158.ccnet.com - - [01/Aug/1995:01:37:04 -0400] \"GET /history/apollo/a-004/a-004-patch-small.gif HTTP/1.0\" 404 -$        |\n|h96-158.ccnet.com - - [01/Aug/1995:01:37:05 -0400] \"GET /history/apollo/a-004/movies/ HTTP/1.0\" 404 -$                      |\n+----------------------------------------------------------------------------------------------------------------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1542508069145_761991120","id":"20160718-200445_212592333","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:345"},{"text":"%md\n\n### (2d) Fix the rows with null content\\_size\n\nThe easiest solution is to replace the null values in `split_df` with 0. The DataFrame API provides a set of functions and fields specifically designed for working with null values, among them:\n\n* [fillna()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.fillna), which fills null values with specified non-null values.\n* [na](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.na), which returns a [DataFrameNaFunctions](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameNaFunctions) object with many functions for operating on null columns.\n\nWe'll use `fillna()`, because it's simple. There are several ways to invoke this function. The easiest is just to replace _all_ null columns with known values. But, for safety, it's better to pass a Python dictionary containing (column\\_name, value) mappings. That's what we'll do.","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>(2d) Fix the rows with null content_size</h3>\n<p>The easiest solution is to replace the null values in <code>split_df</code> with 0. The DataFrame API provides a set of functions and fields specifically designed for working with null values, among them:</p>\n<ul>\n<li><a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.fillna\">fillna()</a>, which fills null values with specified non-null values.</li>\n<li><a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.na\">na</a>, which returns a <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameNaFunctions\">DataFrameNaFunctions</a> object with many functions for operating on null columns.</li>\n</ul>\n<p>We'll use <code>fillna()</code>, because it's simple. There are several ways to invoke this function. The easiest is just to replace <em>all</em> null columns with known values. But, for safety, it's better to pass a Python dictionary containing (column_name, value) mappings. That's what we'll do.</p>\n"}]},"apps":[],"jobName":"paragraph_1542508069146_-1876170662","id":"20160721-213152_1800712122","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:346"},{"text":"%pyspark\n\n# Replace all null content_size values with 0.\ncleaned_df = split_df.fillna({'content_size': 0})","user":"anonymous","dateUpdated":"2018-11-18T11:50:11+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1542508069146_-1495214744","id":"20160718-200611_314088170","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:347"},{"text":"%pyspark\n\n# Ensure that there are no nulls left.\nexprs = []\nfor col_name in cleaned_df.columns:\n  exprs.append(count_null(col_name))\n\ncleaned_df.agg(*exprs).show()","user":"anonymous","dateUpdated":"2018-11-18T11:50:15+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----+---------+----+------+------------+\n|host|timestamp|path|status|content_size|\n+----+---------+----+------+------------+\n|   0|        0|   0|     0|           0|\n+----+---------+----+------+------------+\n\n"}]},"apps":[],"jobName":"paragraph_1542508069147_-1232455256","id":"20160718-200631_2138789122","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:348"},{"text":"%md\n\n### (2e) Parsing the timestamp.\n\nOkay, now that we have a clean, parsed DataFrame, we have to parse the timestamp field into an actual timestamp. The Common Log Format time is somewhat non-standard. A User-Defined Function (UDF) is the most straightforward way to parse it.","user":"anonymous","dateUpdated":"2018-11-18T11:50:22+0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":false,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>(2e) Parsing the timestamp.</h3>\n<p>Okay, now that we have a clean, parsed DataFrame, we have to parse the timestamp field into an actual timestamp. The Common Log Format time is somewhat non-standard. A User-Defined Function (UDF) is the most straightforward way to parse it.</p>\n"}]},"apps":[],"jobName":"paragraph_1542508069148_-627819791","id":"20160721-213247_360874576","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:349"},{"text":"%pyspark\n\nfrom pyspark.sql.functions import udf\n\nmonth_map = {\n  'Jan': 1, 'Feb': 2, 'Mar':3, 'Apr':4, 'May':5, 'Jun':6, 'Jul':7,\n  'Aug':8,  'Sep': 9, 'Oct':10, 'Nov': 11, 'Dec': 12\n}\n\ndef parse_clf_time(s):\n    \"\"\" Convert Common Log time format into a Python datetime object\n    Args:\n        s (str): date and time in Apache time format [dd/mmm/yyyy:hh:mm:ss (+/-)zzzz]\n    Returns:\n        a string suitable for passing to CAST('timestamp')\n    \"\"\"\n    # NOTE: We're ignoring time zone here. In a production application, you'd want to handle that.\n    return \"{0:04d}-{1:02d}-{2:02d} {3:02d}:{4:02d}:{5:02d}\".format(\n      int(s[7:11]),\n      month_map[s[3:6]],\n      int(s[0:2]),\n      int(s[12:14]),\n      int(s[15:17]),\n      int(s[18:20])\n    )\n\nu_parse_time = udf(parse_clf_time)\n\nlogs_df = cleaned_df.select('*', u_parse_time(cleaned_df['timestamp']).cast('timestamp').alias('time')).drop('timestamp')\ntotal_log_entries = logs_df.count()\n\nprint total_log_entries","user":"anonymous","dateUpdated":"2018-11-18T11:50:25+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"1043177\n"}]},"apps":[],"jobName":"paragraph_1542508069148_-1366264166","id":"20160718-200648_780067911","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:350"},{"text":"%pyspark\n\nlogs_df.printSchema()","user":"anonymous","dateUpdated":"2018-11-18T11:51:15+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- host: string (nullable = true)\n |-- path: string (nullable = true)\n |-- status: integer (nullable = true)\n |-- content_size: integer (nullable = false)\n |-- time: timestamp (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1542508069149_-1823420526","id":"20160718-200730_1083966913","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:351"},{"text":"%pyspark\n\nlogs_df.show(truncate = False)","user":"anonymous","dateUpdated":"2018-11-18T11:51:23+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------------------------+---------------------------------------------------+------+------------+---------------------+\n|host                        |path                                               |status|content_size|time                 |\n+----------------------------+---------------------------------------------------+------+------------+---------------------+\n|in24.inetnebr.com           |/shuttle/missions/sts-68/news/sts-68-mcc-05.txt    |200   |1839        |1995-08-01 00:00:01.0|\n|uplherc.upl.com             |/                                                  |304   |0           |1995-08-01 00:00:07.0|\n|uplherc.upl.com             |/images/ksclogo-medium.gif                         |304   |0           |1995-08-01 00:00:08.0|\n|uplherc.upl.com             |/images/MOSAIC-logosmall.gif                       |304   |0           |1995-08-01 00:00:08.0|\n|uplherc.upl.com             |/images/USA-logosmall.gif                          |304   |0           |1995-08-01 00:00:08.0|\n|ix-esc-ca2-07.ix.netcom.com |/images/launch-logo.gif                            |200   |1713        |1995-08-01 00:00:09.0|\n|uplherc.upl.com             |/images/WORLD-logosmall.gif                        |304   |0           |1995-08-01 00:00:10.0|\n|slppp6.intermind.net        |/history/skylab/skylab.html                        |200   |1687        |1995-08-01 00:00:10.0|\n|piweba4y.prodigy.com        |/images/launchmedium.gif                           |200   |11853       |1995-08-01 00:00:10.0|\n|slppp6.intermind.net        |/history/skylab/skylab-small.gif                   |200   |9202        |1995-08-01 00:00:11.0|\n|slppp6.intermind.net        |/images/ksclogosmall.gif                           |200   |3635        |1995-08-01 00:00:12.0|\n|ix-esc-ca2-07.ix.netcom.com |/history/apollo/images/apollo-logo1.gif            |200   |1173        |1995-08-01 00:00:12.0|\n|slppp6.intermind.net        |/history/apollo/images/apollo-logo.gif             |200   |3047        |1995-08-01 00:00:13.0|\n|uplherc.upl.com             |/images/NASA-logosmall.gif                         |304   |0           |1995-08-01 00:00:14.0|\n|133.43.96.45                |/shuttle/missions/sts-69/mission-sts-69.html       |200   |10566       |1995-08-01 00:00:16.0|\n|kgtyk4.kj.yamagata-u.ac.jp  |/                                                  |200   |7280        |1995-08-01 00:00:17.0|\n|kgtyk4.kj.yamagata-u.ac.jp  |/images/ksclogo-medium.gif                         |200   |5866        |1995-08-01 00:00:18.0|\n|d0ucr6.fnal.gov             |/history/apollo/apollo-16/apollo-16.html           |200   |2743        |1995-08-01 00:00:19.0|\n|ix-esc-ca2-07.ix.netcom.com |/shuttle/resources/orbiters/discovery.html         |200   |6849        |1995-08-01 00:00:19.0|\n|d0ucr6.fnal.gov             |/history/apollo/apollo-16/apollo-16-patch-small.gif|200   |14897       |1995-08-01 00:00:20.0|\n+----------------------------+---------------------------------------------------+------+------------+---------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1542508069150_-670026169","id":"20160718-201656_1316433772","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:352"},{"text":"%md\n\nLet's cache logs_df. We're going to be using it quite a bit from here forward.","user":"anonymous","dateUpdated":"2018-11-18T11:51:40+0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":false,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Let's cache logs_df. We're going to be using it quite a bit from here forward.</p>\n"}]},"apps":[],"jobName":"paragraph_1542508069150_1078049573","id":"20160721-213319_1843703416","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:353"},{"text":"%pyspark\n\nlogs_df.cache()","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1542508069151_-406941299","id":"20160718-201706_792557368","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:354"},{"text":"%md\n\n## Part 3: 日志总体分析\n\nNow that we have a DataFrame containing the parsed log file as a set of Row objects, we can perform various analyses.\n\n### (3a) Example: Content Size Statistics\n\nLet's compute some statistics about the sizes of content being returned by the web server. In particular, we'd like to know what are the average, minimum, and maximum content sizes.\n\nWe can compute the statistics by calling `.describe()` on the `content_size` column of `logs_df`.  The `.describe()` function returns the count, mean, stddev, min, and max of a given column.","user":"anonymous","dateUpdated":"2019-03-01T16:47:05+0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Part 3: 日志总体分析</h2>\n<p>Now that we have a DataFrame containing the parsed log file as a set of Row objects, we can perform various analyses.</p>\n<h3>(3a) Example: Content Size Statistics</h3>\n<p>Let&rsquo;s compute some statistics about the sizes of content being returned by the web server. In particular, we&rsquo;d like to know what are the average, minimum, and maximum content sizes.</p>\n<p>We can compute the statistics by calling <code>.describe()</code> on the <code>content_size</code> column of <code>logs_df</code>. The <code>.describe()</code> function returns the count, mean, stddev, min, and max of a given column.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542508069152_244902898","id":"20160721-213338_215734708","dateCreated":"2018-11-18T10:27:49+0800","dateStarted":"2019-03-01T16:47:05+0800","dateFinished":"2019-03-01T16:47:05+0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:355"},{"text":"%pyspark\n\n# Calculate statistics based on the content size.\ncontent_size_summary_df = logs_df.describe(['content_size'])\ncontent_size_summary_df.show()","user":"anonymous","dateUpdated":"2018-11-18T11:51:45+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+------------------+\n|summary|      content_size|\n+-------+------------------+\n|  count|           1043177|\n|   mean|17531.555702435926|\n| stddev| 68561.99906264187|\n|    min|                 0|\n|    max|           3421948|\n+-------+------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1542508069152_-1420899897","id":"20160718-202001_878461975","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:356"},{"text":"%md\n\nAlternatively, we can use SQL to directly calculate these statistics.  You can explore the many useful functions within the `pyspark.sql.functions` module in the [documentation](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions).\n\nAfter we apply the `.agg()` function, we call `.first()` to extract the first value, which is equivalent to `.take(1)[0]`.","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Alternatively, we can use SQL to directly calculate these statistics.  You can explore the many useful functions within the <code>pyspark.sql.functions</code> module in the <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions\">documentation</a>.</p>\n<p>After we apply the <code>.agg()</code> function, we call <code>.first()</code> to extract the first value, which is equivalent to <code>.take(1)[0]</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1542508069153_-475571601","id":"20160721-213401_647137257","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:357"},{"text":"%pyspark\n\nfrom pyspark.sql import functions as sqlFunctions\n\nstats =  (logs_df\n            .agg(sqlFunctions.min(logs_df['content_size']),\n                 sqlFunctions.avg(logs_df['content_size']),\n                 sqlFunctions.max(logs_df['content_size']))\n            .first())\n\nprint 'Using SQL functions:'\nprint 'Content Size Avg: %.02f Min: %.02f; Max: %.02f' % (stats['avg(content_size)'], stats['min(content_size)'], stats['max(content_size)'])\n","user":"anonymous","dateUpdated":"2018-11-18T11:52:21+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Using SQL functions:\nContent Size Avg: 17531.56 Min: 0.00; Max: 3421948.00\n"}]},"apps":[],"jobName":"paragraph_1542508069154_1231129464","id":"20160718-202032_221945837","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:358"},{"text":"%md\n\n### (3b) Example: HTTP Status Analysis\n\nNext, let's look at the status values that appear in the log. We want to know which status values appear in the data and how many times.  We again start with `logs_df`, then group by the `status` column, apply the `.count()` aggregation function, and sort by the `status` column.","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>(3b) Example: HTTP Status Analysis</h3>\n<p>Next, let's look at the status values that appear in the log. We want to know which status values appear in the data and how many times.  We again start with <code>logs_df</code>, then group by the <code>status</code> column, apply the <code>.count()</code> aggregation function, and sort by the <code>status</code> column.</p>\n"}]},"apps":[],"jobName":"paragraph_1542508069154_343067011","id":"20160721-213426_1090267607","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:359"},{"text":"%pyspark\n\nstatus_to_count_df =(logs_df\n                     .groupBy('status')\n                     .count()\n                     .sort('status')\n                     .cache())\n\nstatus_to_count_length = status_to_count_df.count()\nprint 'Found %d response codes' % status_to_count_length\nstatus_to_count_df.show()\n","user":"anonymous","dateUpdated":"2018-11-18T11:52:30+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 7 response codes\n+------+------+\n|status| count|\n+------+------+\n|   200|940847|\n|   302| 16244|\n|   304| 79824|\n|   403|    58|\n|   404|  6185|\n|   500|     2|\n|   501|    17|\n+------+------+\n\n"}]},"apps":[],"jobName":"paragraph_1542508069155_-1089136026","id":"20160718-202106_1792027802","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:360"},{"text":"%pyspark\n\nassert status_to_count_length == 7\nassert status_to_count_df.take(100) == [(200, 940847), (302, 16244), (304, 79824), (403, 58), (404, 6185), (500, 2), (501, 17)]\n","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1542508069156_-567529151","id":"20160718-203631_1933692996","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:361"},{"text":"%md\n\n### (3c) Example: Status Graphing\n\nNow, let's visualize the results from the last example.  We can seamlessly switch over to the '%sql' interpreter to show a bar chart of the count for each response code. You can see that this is not a very effective plot.  Due to the large number of '200' codes, it is very hard to see the relative number of the others.  We can alleviate this by taking the logarithm of the count, adding that as a column to our DataFrame and displaying the result.\n\n","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>(3c) Example: Status Graphing</h3>\n<p>Now, let's visualize the results from the last example.  We can seamlessly switch over to the '%sql' interpreter to show a bar chart of the count for each response code. You can see that this is not a very effective plot.  Due to the large number of '200' codes, it is very hard to see the relative number of the others.  We can alleviate this by taking the logarithm of the count, adding that as a column to our DataFrame and displaying the result.</p>\n"}]},"apps":[],"jobName":"paragraph_1542508069156_826074196","id":"20160721-213602_558712068","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:362"},{"text":"%pyspark\n\nlog_status_to_count_df = status_to_count_df.withColumn('log_count', sqlFunctions.log(status_to_count_df['count']))\nlog_status_to_count_df.show()","user":"anonymous","dateUpdated":"2018-11-18T11:53:08+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+------+------------------+\n|status| count|         log_count|\n+------+------+------------------+\n|   200|940847| 13.75453581236166|\n|   302| 16244|  9.69547888880619|\n|   304| 79824|11.287579490100818|\n|   403|    58| 4.060443010546419|\n|   404|  6185| 8.729882284826589|\n|   500|     2|0.6931471805599453|\n|   501|    17| 2.833213344056216|\n+------+------+------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1542508069157_-1310874915","id":"20160718-203658_1473188241","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:363"},{"text":"%md\n\nNext, we'll register the contents of the DataFrame as a temp table, backed by Hive metastore, so we can write sql queries against the data. \n\nAfter running the SQL 'select' cell below, choose one or more of the display options available, and then open the \"settings\" tab and drag `status` to the key entry field and drag `log_count` to the value entry field. See the diagram, below, for an example.","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Next, we'll register the contents of the DataFrame as a temp table, backed by Hive metastore, so we can write sql queries against the data.</p>\n<p>After running the SQL 'select' cell below, choose one or more of the display options available, and then open the &ldquo;settings&rdquo; tab and drag <code>status</code> to the key entry field and drag <code>log_count</code> to the value entry field. See the diagram, below, for an example.</p>\n"}]},"apps":[],"jobName":"paragraph_1542508069158_-1159720293","id":"20160722-191107_1130696257","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:364"},{"text":"%pyspark\n\nprint log_status_to_count_df.columns\nlog_status_to_count_df.registerTempTable(\"logstatus\")\nprint sqlContext.read.table('logstatus').dtypes","user":"anonymous","dateUpdated":"2018-11-18T11:53:29+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"['status', 'count', 'log_count']\n[('status', 'int'), ('count', 'bigint'), ('log_count', 'double')]\n"}]},"apps":[],"jobName":"paragraph_1542508069158_-2009277892","id":"20160722-154558_417576869","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:365"},{"text":"%sql\n\nselect status, count, log_count from logstatus","user":"anonymous","dateUpdated":"2019-03-01T11:32:16+0800","config":{"colWidth":12,"editorMode":"ace/mode/sql","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"status","index":0,"aggr":"sum"}],"values":[{"name":"count","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"status","index":0,"aggr":"sum"}},"setting":{"multiBarChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"status":"string","count":"string","log_count":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false},"scatterChart":{"xAxis":{"name":"status","index":0,"aggr":"sum"},"yAxis":{"name":"count","index":1,"aggr":"sum"},"group":{"name":"status","index":0,"aggr":"sum"},"size":{"name":"count","index":1,"aggr":"sum"}}},"commonSetting":{}},"helium":{}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"status\tcount\tlog_count\n200\t940847\t13.75453581236166\n302\t16244\t9.69547888880619\n304\t79824\t11.287579490100818\n403\t58\t4.060443010546419\n404\t6185\t8.729882284826589\n500\t2\t0.6931471805599453\n501\t17\t2.833213344056216\n"}]},"apps":[],"jobName":"paragraph_1542508069159_-793212541","id":"20160722-154539_189658824","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:366"},{"text":"%md\n\n### (3d) Example: Frequent Hosts\n\nLet's look at hosts that have accessed the server frequently (e.g., more than ten times). As with the response code analysis in (3b), we create a new DataFrame by grouping `successLogsDF` by the 'host' column and aggregating by count.\n\nWe then filter the result based on the count of accesses by each host being greater than ten.  Then, we select the 'host' column and show 20 elements from the result.","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>(3d) Example: Frequent Hosts</h3>\n<p>Let's look at hosts that have accessed the server frequently (e.g., more than ten times). As with the response code analysis in (3b), we create a new DataFrame by grouping <code>successLogsDF</code> by the 'host' column and aggregating by count.</p>\n<p>We then filter the result based on the count of accesses by each host being greater than ten.  Then, we select the 'host' column and show 20 elements from the result.</p>\n"}]},"apps":[],"jobName":"paragraph_1542508069160_51883137","id":"20160722-191832_137511351","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:367"},{"text":"%pyspark\n\n# Any hosts that has accessed the server more than 10 times.\nhost_sum_df =(logs_df\n              .groupBy('host')\n              .count())\n\nhost_more_than_10_df = (host_sum_df\n                        .filter(host_sum_df['count'] > 10)\n                        .select(host_sum_df['host']))\n\nprint 'Any 20 hosts that have accessed more then 10 times:\\n'\nhost_more_than_10_df.show(truncate=False)","user":"anonymous","dateUpdated":"2018-11-18T11:55:59+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Any 20 hosts that have accessed more then 10 times:\n\n+-----------------------------+\n|host                         |\n+-----------------------------+\n|gcl-s2.aero.kyushu-u.ac.jp   |\n|dd09-015.compuserve.com      |\n|sun8.hrz.th-darmstadt.de     |\n|128.159.144.47               |\n|160.151.233.33               |\n|128.159.132.13               |\n|s025n217.ummed.edu           |\n|204.126.175.80               |\n|n1043367.ksc.nasa.gov        |\n|128.159.140.124              |\n|hermes.rz.uni-duesseldorf.de |\n|csa.bu.edu                   |\n|139.169.136.137              |\n|knet.kntl.co.kr              |\n|pcmnbib03.uio.no             |\n|ppp2_100.bekkoame.or.jp      |\n|hp3.lsw.uni-heidelberg.de    |\n|163.205.105.9                |\n|netcom9.netcom.com           |\n|dd24-025.compuserve.com      |\n+-----------------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1542508069160_187544319","id":"20160718-203737_92678960","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:368"},{"text":"%md\n\n### (3e) Example: Visualizing Paths\n\nNow, let's visualize the number of hits to paths (URIs) in the log. To perform this task, we start with our `logs_df` and group by the `path` column, aggregate by count, and sort in descending order.\n","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>(3e) Example: Visualizing Paths</h3>\n<p>Now, let's visualize the number of hits to paths (URIs) in the log. To perform this task, we start with our <code>logs_df</code> and group by the <code>path</code> column, aggregate by count, and sort in descending order.</p>\n"}]},"apps":[],"jobName":"paragraph_1542508069161_351948994","id":"20160722-191924_1596254266","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:369"},{"text":"%pyspark\n\npaths_df = (logs_df\n            .groupBy('path')\n            .count()\n            .sort('count', ascending=False))\n\npaths_counts = (paths_df\n                .select('path', 'count')\n                .map(lambda r: (r[0], r[1]))\n                .collect())\n\n","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1542508069161_48591440","id":"20160718-211843_299092782","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:370"},{"text":"%md\n\nOptional: We can extract the paths and the counts, and unpack the resulting list of `Rows` using a `map` function and `lambda` expression. This would allow us to find particular paths, such as those that describe the fateful Apollo 13 mission.","user":"anonymous","dateUpdated":"2018-11-18T11:56:16+0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Optional: We can extract the paths and the counts, and unpack the resulting list of <code>Rows</code> using a <code>map</code> function and <code>lambda</code> expression. This would allow us to find particular paths, such as those that describe the fateful Apollo 13 mission.</p>\n"}]},"apps":[],"jobName":"paragraph_1542508069162_1663488558","id":"20160722-193401_1732241902","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:371"},{"text":"%pyspark\n\npaths, counts = zip(*paths_counts)\n\nfor p in paths:\n    if p.startswith('/history/apollo/apollo13'):\n        print p","user":"anonymous","dateUpdated":"2018-11-18T11:56:14+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"/history/apollo/apollo13/apollo-13-info.html\n/history/apollo/apollo13/apollo-13.html\n/history/apollo/apollo13/apollo13.html\n/history/apollo/apollo13\n/history/apollo/apollo13/movies/apo13home.mpg\n"}]},"apps":[],"jobName":"paragraph_1542508069163_1432015904","id":"20160722-193156_1307759391","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:372"},{"text":"%md\n\n### (3f) Example: Top Paths\n\nFor the final example, we'll find the top paths (URIs) in the log.  Because we sorted `paths_df` for plotting, all we need to do is call `.show()` and pass in `n=10` and `truncate=False` as the parameters to show the top ten paths without truncating.","user":"anonymous","dateUpdated":"2018-11-18T11:56:28+0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":false,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>(3f) Example: Top Paths</h3>\n<p>For the final example, we'll find the top paths (URIs) in the log.  Because we sorted <code>paths_df</code> for plotting, all we need to do is call <code>.show()</code> and pass in <code>n=10</code> and <code>truncate=False</code> as the parameters to show the top ten paths without truncating.</p>\n"}]},"apps":[],"jobName":"paragraph_1542508069163_1364924850","id":"20160722-193447_577294938","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:373"},{"text":"%pyspark\n\nprint 'Top Ten Paths:'\npaths_df.show(n=10, truncate=False)\n\nexpected = [\n  (u'/images/NASA-logosmall.gif', 59666),\n  (u'/images/KSC-logosmall.gif', 50420),\n  (u'/images/MOSAIC-logosmall.gif', 43831),\n  (u'/images/USA-logosmall.gif', 43604),\n  (u'/images/WORLD-logosmall.gif', 43217),\n  (u'/images/ksclogo-medium.gif', 41267),\n  (u'/ksc.html', 28536),\n  (u'/history/apollo/images/apollo-logo1.gif', 26766),\n  (u'/images/launch-logo.gif', 24742),\n  (u'/', 20173)\n]\nassert paths_df.take(10) == expected, 'incorrect Top Ten Paths'\n","user":"anonymous","dateUpdated":"2018-11-18T11:56:30+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Top Ten Paths:\n+---------------------------------------+-----+\n|path                                   |count|\n+---------------------------------------+-----+\n|/images/NASA-logosmall.gif             |59666|\n|/images/KSC-logosmall.gif              |50420|\n|/images/MOSAIC-logosmall.gif           |43831|\n|/images/USA-logosmall.gif              |43604|\n|/images/WORLD-logosmall.gif            |43217|\n|/images/ksclogo-medium.gif             |41267|\n|/ksc.html                              |28536|\n|/history/apollo/images/apollo-logo1.gif|26766|\n|/images/launch-logo.gif                |24742|\n|/                                      |20173|\n+---------------------------------------+-----+\nonly showing top 10 rows\n\n"}]},"apps":[],"jobName":"paragraph_1542508069164_1068064399","id":"20160718-211914_242228510","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:374"},{"text":"%md\n\n## Part 4: 日志细节字段挖掘与时序分析\n\nNow it is your turn to perform analyses on the web server log files.","user":"anonymous","dateUpdated":"2019-03-01T16:47:27+0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Part 4: 日志细节字段挖掘与时序分析</h2>\n<p>Now it is your turn to perform analyses on the web server log files.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542508069165_-1470461132","id":"20160722-193742_1517160612","dateCreated":"2018-11-18T10:27:49+0800","dateStarted":"2019-03-01T16:47:27+0800","dateFinished":"2019-03-01T16:47:27+0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:375"},{"text":"%md\n\n**(4a) Exercise: Top Ten Error Paths**\n\nWhat are the top ten paths which did not have return code 200? Create a sorted list containing the paths and the number of times that they were accessed with a non-200 return code and show the top ten.\n\nThink about the steps that you need to perform to determine which paths did not have a 200 return code, how you will uniquely count those paths and sort the list.","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><strong>(4a) Exercise: Top Ten Error Paths</strong></p>\n<p>What are the top ten paths which did not have return code 200? Create a sorted list containing the paths and the number of times that they were accessed with a non-200 return code and show the top ten.</p>\n<p>Think about the steps that you need to perform to determine which paths did not have a 200 return code, how you will uniquely count those paths and sort the list.</p>\n"}]},"apps":[],"jobName":"paragraph_1542508069166_-2078026263","id":"20160722-193809_1481595131","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:376"},{"text":"%pyspark\n\n# DataFrame containing all accesses that did not return a code 200\nfrom pyspark.sql.functions import desc, log\n\nnot200DF = logs_df.filter(logs_df['status'] != 200)","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1542508069166_-1286111685","id":"20160718-212016_221401993","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:377"},{"text":"%pyspark\n\n# Sorted DataFrame containing all paths and the number of times they were accessed with non-200 return code\nlogs_sum_df = not200DF.groupBy('path').count().sort('count', ascending=False)\n\nprint 'Top Ten failed URLs:'\nlogs_sum_df.show(10, False)\n","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Top Ten failed URLs:\n+---------------------------------------+-----+\n|path                                   |count|\n+---------------------------------------+-----+\n|/images/NASA-logosmall.gif             |8761 |\n|/images/KSC-logosmall.gif              |7236 |\n|/images/MOSAIC-logosmall.gif           |5197 |\n|/images/USA-logosmall.gif              |5157 |\n|/images/WORLD-logosmall.gif            |5020 |\n|/images/ksclogo-medium.gif             |4728 |\n|/history/apollo/images/apollo-logo1.gif|2907 |\n|/images/launch-logo.gif                |2811 |\n|/                                      |2199 |\n|/images/ksclogosmall.gif               |1622 |\n+---------------------------------------+-----+\nonly showing top 10 rows\n\n"}]},"apps":[],"jobName":"paragraph_1542508069167_1213473091","id":"20160718-212055_410692090","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:378"},{"text":"%pyspark\n\ntop_10_err_urls = [(row[0], row[1]) for row in logs_sum_df.take(10)]\ntop_10_err_expected = [\n  (u'/images/NASA-logosmall.gif', 8761),\n  (u'/images/KSC-logosmall.gif', 7236),\n  (u'/images/MOSAIC-logosmall.gif', 5197),\n  (u'/images/USA-logosmall.gif', 5157),\n  (u'/images/WORLD-logosmall.gif', 5020),\n  (u'/images/ksclogo-medium.gif', 4728),\n  (u'/history/apollo/images/apollo-logo1.gif', 2907),\n  (u'/images/launch-logo.gif', 2811),\n  (u'/', 2199),\n  (u'/images/ksclogosmall.gif', 1622)\n]\n\nassert logs_sum_df.count() == 7675, 'incorrect count for logs_sum_df'\nassert top_10_err_urls == top_10_err_expected, 'incorrect Top Ten failed URLs'\n","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1542508069167_1501586214","id":"20160718-212113_968538624","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:379"},{"text":"%md\n\n### (4b) Exercise: Number of Unique Hosts\n\nHow many unique hosts are there in the entire log?\n\nThere are multiple ways to find this.  Try to find a more optimal way than grouping by 'host'.","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>(4b) Exercise: Number of Unique Hosts</h3>\n<p>How many unique hosts are there in the entire log?</p>\n<p>There are multiple ways to find this.  Try to find a more optimal way than grouping by 'host'.</p>\n"}]},"apps":[],"jobName":"paragraph_1542508069168_-123587210","id":"20160722-194321_2111382895","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:380"},{"text":"%pyspark\n\nunique_host_count = logs_df.select('host').distinct().count()\nprint 'Unique hosts: {0}'.format(unique_host_count)","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Unique hosts: 54507\n"}]},"apps":[],"jobName":"paragraph_1542508069169_843464291","id":"20160718-212129_286712384","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:381"},{"text":"%pyspark\n\nassert unique_host_count == 54507, 'incorrect unique_host_count'","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1542508069169_1445109408","id":"20160718-212331_1613083931","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:382"},{"text":"%md\n\n### (4c) Exercise: Number of Unique Daily Hosts\n\nFor an advanced exercise, let's determine the number of unique hosts in the entire log on a day-by-day basis. This computation will give us counts of the number of unique daily hosts. We'd like a DataFrame sorted by increasing day of the month which includes the day of the month and the associated number of unique hosts for that day. Make sure you cache the resulting DataFrame `daily_hosts_df` so that we can reuse it in the next exercise.\n\nThink about the steps that you need to perform to count the number of different hosts that make requests *each* day.\n*Since the log only covers a single month, you can ignore the month.*  You may want to use the [`dayofmonth` function](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.dayofmonth) in the `pyspark.sql.functions` module.\n\n**Description of each variable**\n\n**`day_to_host_pair_df`**\n\nA DataFrame with two columns\n\n    | column | explanation          |\n    | ------ | -------------------- |\n    | `host` | the host name        |\n    | `day`  | the day of the month |\n\nThere will be one row in this DataFrame for each row in `logs_df`. Essentially, you're just trimming and transforming each row of `logs_df`. For example, for this row in `logs_df`:\n\n```\ngw1.att.com - - [23/Aug/1995:00:03:53 -0400] \"GET /shuttle/missions/sts-73/news HTTP/1.0\" 302 -\n```\n\nyour `day_to_host_pair_df` should have:\n\n```\ngw1.att.com 23\n```\n\n**`day_group_hosts_df`**\n\nThis DataFrame has the same columns as `day_to_host_pair_df`, but with duplicate (`day`, `host`) rows removed.\n\n**`daily_hosts_df`**\n\nA DataFrame with two columns:\n\n    | column  | explanation                                        |\n    | ------- | -------------------------------------------------- |\n    | `day`   | the day of the month                               |\n    | `count` | the number of unique requesting hosts for that day |\n    \n    ","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>(4c) Exercise: Number of Unique Daily Hosts</h3>\n<p>For an advanced exercise, let's determine the number of unique hosts in the entire log on a day-by-day basis. This computation will give us counts of the number of unique daily hosts. We'd like a DataFrame sorted by increasing day of the month which includes the day of the month and the associated number of unique hosts for that day. Make sure you cache the resulting DataFrame <code>daily_hosts_df</code> so that we can reuse it in the next exercise.</p>\n<p>Think about the steps that you need to perform to count the number of different hosts that make requests <em>each</em> day.\n<br  /><em>Since the log only covers a single month, you can ignore the month.</em>  You may want to use the <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.dayofmonth\"><code>dayofmonth</code> function</a> in the <code>pyspark.sql.functions</code> module.</p>\n<p><strong>Description of each variable</strong></p>\n<p><strong><code>day_to_host_pair_df</code></strong></p>\n<p>A DataFrame with two columns</p>\n<pre><code>| column | explanation          |\n| ------ | -------------------- |\n| `host` | the host name        |\n| `day`  | the day of the month |\n</code></pre>\n<p>There will be one row in this DataFrame for each row in <code>logs_df</code>. Essentially, you're just trimming and transforming each row of <code>logs_df</code>. For example, for this row in <code>logs_df</code>:</p>\n<pre><code>gw1.att.com - - [23/Aug/1995:00:03:53 -0400] \"GET /shuttle/missions/sts-73/news HTTP/1.0\" 302 -\n</code></pre>\n<p>your <code>day_to_host_pair_df</code> should have:</p>\n<pre><code>gw1.att.com 23\n</code></pre>\n<p><strong><code>day_group_hosts_df</code></strong></p>\n<p>This DataFrame has the same columns as <code>day_to_host_pair_df</code>, but with duplicate (<code>day</code>, <code>host</code>) rows removed.</p>\n<p><strong><code>daily_hosts_df</code></strong></p>\n<p>A DataFrame with two columns:</p>\n<pre><code>| column  | explanation                                        |\n| ------- | -------------------------------------------------- |\n| `day`   | the day of the month                               |\n| `count` | the number of unique requesting hosts for that day |\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1542508069170_-1659434266","id":"20160722-194417_206107074","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:383"},{"text":"%pyspark\n\nfrom pyspark.sql.functions import dayofmonth\n\nday_to_host_pair_df = logs_df.select('host', dayofmonth('time').alias('day'))\n\nday_group_hosts_df = day_to_host_pair_df.distinct()\n\ndaily_hosts_df = day_group_hosts_df.groupBy('day').count()\ndaily_hosts_df.cache()\n\nprint 'Unique hosts per day:'\ndaily_hosts_df.show(30, False)\n\n","user":"anonymous","dateUpdated":"2018-11-18T11:56:41+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Unique hosts per day:\n+---+-----+\n|day|count|\n+---+-----+\n|1  |2582 |\n|3  |3222 |\n|4  |4190 |\n|5  |2502 |\n|6  |2537 |\n|7  |4106 |\n|8  |4406 |\n|9  |4317 |\n|10 |4523 |\n|11 |4346 |\n|12 |2864 |\n|13 |2650 |\n|14 |4454 |\n|15 |4214 |\n|16 |4340 |\n|17 |4385 |\n|18 |4168 |\n|19 |2550 |\n|20 |2560 |\n|21 |4134 |\n|22 |4456 |\n+---+-----+\n\n"}]},"apps":[],"jobName":"paragraph_1542508069171_2049473931","id":"20160718-212353_745338793","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:384"},{"text":"%md\n\nLet's make sure our data matches known good values.","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Let's make sure our data matches known good values.</p>\n"}]},"apps":[],"jobName":"paragraph_1542508069171_1958364809","id":"20160723-180050_1724926025","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:385"},{"text":"%pyspark\n\ndaily_hosts_list = (daily_hosts_df\n                    .map(lambda r: (r[0], r[1]))\n                    .take(30))\n                    \nassert day_to_host_pair_df.count() == total_log_entries, 'incorrect row count for day_to_host_pair_df'\nassert daily_hosts_df.count() == 21, 'incorrect daily_hosts_df.count()'\nassert daily_hosts_list == [(1, 2582), (3, 3222), (4, 4190), (5, 2502), (6, 2537), (7, 4106), (8, 4406), (9, 4317), (10, 4523), (11, 4346), (12, 2864), (13, 2650), (14, 4454), (15, 4214), (16, 4340), (17, 4385), (18, 4168), (19, 2550), (20, 2560), (21, 4134), (22, 4456)], 'incorrect daily_hosts_df'\nassert daily_hosts_df.is_cached == True, 'incorrect daily_hosts_df.is_cached'","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1542508069172_1495654579","id":"20160718-212445_2056034580","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:386"},{"text":"%md\n\n### (4d) Exercise: Visualizing the Number of Unique Daily Hosts\n\nUsing the results from the previous exercise, we will use built-in graphing to plot a line graph of the unique hosts requests by day.  We need a list of days called `days_with_hosts` and a list of the number of unique hosts for each corresponding day called `hosts`.\n\n**WARNING**: Simply calling `collect()` on your transformed DataFrame won't work, because `collect()` returns a list of Spark SQL `Row` objects. You must _extract_ the appropriate column values from the `Row` objects. Hint: A loop will help.","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>(4d) Exercise: Visualizing the Number of Unique Daily Hosts</h3>\n<p>Using the results from the previous exercise, we will use built-in graphing to plot a line graph of the unique hosts requests by day.  We need a list of days called <code>days_with_hosts</code> and a list of the number of unique hosts for each corresponding day called <code>hosts</code>.</p>\n<p><strong>WARNING</strong>: Simply calling <code>collect()</code> on your transformed DataFrame won't work, because <code>collect()</code> returns a list of Spark SQL <code>Row</code> objects. You must <em>extract</em> the appropriate column values from the <code>Row</code> objects. Hint: A loop will help.</p>\n"}]},"apps":[],"jobName":"paragraph_1542508069172_936493040","id":"20160722-194707_1722235998","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:387"},{"text":"%pyspark\n\ndays_with_hosts = []\nhosts = []\nfor row in daily_hosts_df.collect():\n    days_with_hosts.append(row[0])\n    hosts.append(row[1])\n\nprint (days_with_hosts)\nprint (hosts)","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n[2582, 3222, 4190, 2502, 2537, 4106, 4406, 4317, 4523, 4346, 2864, 2650, 4454, 4214, 4340, 4385, 4168, 2550, 2560, 4134, 4456]\n"}]},"apps":[],"jobName":"paragraph_1542508069173_523102179","id":"20160718-212614_81729699","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:388"},{"text":"%pyspark\n\ntest_days = range(1, 23)\ntest_days.remove(2)\n\nassert days_with_hosts == test_days, 'incorrect days'\nassert hosts == [2582, 3222, 4190, 2502, 2537, 4106, 4406, 4317, 4523, 4346, 2864, 2650, 4454, 4214, 4340, 4385, 4168, 2550, 2560, 4134, 4456], 'incorrect hosts'\n","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1542508069174_2069334257","id":"20160718-212842_1303069643","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:389"},{"text":"%md\n\nNow, we can write out the `daily_hosts_df` DataFrame as a temp table; then we can write a SQL select statement to plot a line or bar graph of the unique hosts requests by day.","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Now, we can write out the <code>daily_hosts_df</code> DataFrame as a temp table; then we can write a SQL select statement to plot a line or bar graph of the unique hosts requests by day.</p>\n"}]},"apps":[],"jobName":"paragraph_1542508069174_-549853689","id":"20160722-194828_1547116231","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:390"},{"text":"%pyspark\n\ndaily_hosts_df.registerTempTable(\"daily_hosts\")\nprint daily_hosts_df.count()","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"21\n"}]},"apps":[],"jobName":"paragraph_1542508069175_-702629867","id":"20160722-195054_1947786482","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:391"},{"text":"%md\n\nMake sure there is a graph of daily_hosts below.","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Make sure there is a graph of daily_hosts below.</p>\n"}]},"apps":[],"jobName":"paragraph_1542508069175_-1549450130","id":"20160723-180641_1565768790","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:392"},{"text":"%sql\n\nselect day, count from daily_hosts","user":"anonymous","dateUpdated":"2019-03-01T19:24:57+0800","config":{"colWidth":12,"editorMode":"ace/mode/sql","results":[{"graph":{"mode":"lineChart","height":300,"optionOpen":false,"keys":[{"name":"day","index":0,"aggr":"sum"}],"values":[{"name":"count","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"day","index":0,"aggr":"sum"},"yAxis":{"name":"count","index":1,"aggr":"sum"}},"setting":{"multiBarChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"stackedAreaChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"lineChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"}},"commonSetting":{}},"helium":{}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"day\tcount\n1\t2582\n3\t3222\n4\t4190\n5\t2502\n6\t2537\n7\t4106\n8\t4406\n9\t4317\n10\t4523\n11\t4346\n12\t2864\n13\t2650\n14\t4454\n15\t4214\n16\t4340\n17\t4385\n18\t4168\n19\t2550\n20\t2560\n21\t4134\n22\t4456\n"}]},"apps":[],"jobName":"paragraph_1542508069176_1405145498","id":"20160723-180450_514138223","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:393"},{"text":"%md\n\n### (4e) Exercise: Average Number of Daily Requests per Host\n\nNext, let's determine the average number of requests on a day-by-day basis.  We'd like a list by increasing day of the month and the associated average number of requests per host for that day. Make sure you cache the resulting DataFrame `avg_daily_req_per_host_df` so that we can reuse it in the next exercise.\n\nTo compute the average number of requests per host, find the total number of requests per day (across all hosts) and divide that by the number of unique hosts per day (which we found in part 4c and cached as `daily_hosts_df`).\n\n*Since the log only covers a single month, you can skip checking for the month.*","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>(4e) Exercise: Average Number of Daily Requests per Host</h3>\n<p>Next, let's determine the average number of requests on a day-by-day basis.  We'd like a list by increasing day of the month and the associated average number of requests per host for that day. Make sure you cache the resulting DataFrame <code>avg_daily_req_per_host_df</code> so that we can reuse it in the next exercise.</p>\n<p>To compute the average number of requests per host, find the total number of requests per day (across all hosts) and divide that by the number of unique hosts per day (which we found in part 4c and cached as <code>daily_hosts_df</code>).</p>\n<p><em>Since the log only covers a single month, you can skip checking for the month.</em></p>\n"}]},"apps":[],"jobName":"paragraph_1542508069176_1043896042","id":"20160723-180636_1447972081","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:394"},{"text":"%pyspark\n\ntotal_req_per_day_df_temp = logs_df.select('host', dayofmonth('time').alias('day')).groupBy('day').count()\ntotal_req_per_day_df = total_req_per_day_df_temp.withColumnRenamed('count', 'reqperday')\n\ntotal_req_per_day_df.show()","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+---------+\n|day|reqperday|\n+---+---------+\n|  1|    33996|\n|  3|    41387|\n|  4|    59554|\n|  5|    31888|\n|  6|    32416|\n|  7|    57355|\n|  8|    60142|\n|  9|    60457|\n| 10|    61245|\n| 11|    61242|\n| 12|    38070|\n| 13|    36480|\n| 14|    59873|\n| 15|    58845|\n| 16|    56651|\n| 17|    58980|\n| 18|    56244|\n| 19|    32092|\n| 20|    32963|\n| 21|    55539|\n+---+---------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1542508069177_2043082615","id":"20160718-213039_1241367186","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:395"},{"text":"%md\n\nPlease note the `join` operator below; we want to combine `hosts per day` and `requests per day` into the same DataFrame.","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Please note the <code>join</code> operator below; we want to combine <code>hosts per day</code> and <code>requests per day</code> into the same DataFrame.</p>\n"}]},"apps":[],"jobName":"paragraph_1542508069178_457146982","id":"20160723-180829_1442987636","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:396"},{"text":"%pyspark\n\n# rename column\ndaily_hosts_per_day_df = daily_hosts_df.withColumnRenamed('count', 'hostperday')\n\n# perform join operation\navg_daily_req_per_host_df = (\n  total_req_per_day_df.join(daily_hosts_per_day_df, 'day', 'inner').select('day', 'hostperday', 'reqperday')\n  )\n\n# print 'Data types for data frame: %s' % avg_daily_req_per_host_df.dtypes\navg_daily_req_per_host_df.show()","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+----------+---------+\n|day|hostperday|reqperday|\n+---+----------+---------+\n|  1|      2582|    33996|\n|  3|      3222|    41387|\n|  4|      4190|    59554|\n|  5|      2502|    31888|\n|  6|      2537|    32416|\n|  7|      4106|    57355|\n|  8|      4406|    60142|\n|  9|      4317|    60457|\n| 10|      4523|    61245|\n| 11|      4346|    61242|\n| 12|      2864|    38070|\n| 13|      2650|    36480|\n| 14|      4454|    59873|\n| 15|      4214|    58845|\n| 16|      4340|    56651|\n| 17|      4385|    58980|\n| 18|      4168|    56244|\n| 19|      2550|    32092|\n| 20|      2560|    32963|\n| 21|      4134|    55539|\n+---+----------+---------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1542508069178_-1659735300","id":"20160718-213054_1346319099","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:397"},{"text":"%pyspark\n\ndaily_req_per_host_list = (\n  avg_daily_req_per_host_df.select(\n    col('day'), col('reqperday').cast('float'), col('hostperday').cast('float')).map(\n      lambda row: (row[0], row[1] / row[2])\n  ).collect()\n)\n\nprint (daily_req_per_host_list)","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[(1, 13.166537567776917), (3, 12.845127250155183), (4, 14.213365155131266), (5, 12.745003996802557), (6, 12.777296018919984), (7, 13.968582562104238), (8, 13.650022696323196), (9, 14.00440120454019), (10, 13.540791510059695), (11, 14.091578462954441), (12, 13.292597765363128), (13, 13.766037735849057), (14, 13.442523574315222), (15, 13.964167062173706), (16, 13.053225806451612), (17, 13.450399087799315), (18, 13.494241842610364), (19, 12.585098039215687), (20, 12.876171875), (21, 13.434687953555878), (22, 12.961849192100539)]\n"}]},"apps":[],"jobName":"paragraph_1542508069179_-1707711868","id":"20160718-213133_1016933553","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:398"},{"text":"%pyspark\n\navg_daily_req_per_host_df = sqlContext.createDataFrame(daily_req_per_host_list, ['day', 'avg_reqs_per_host_per_day'])\n\navg_daily_req_per_host_df.cache()\n\nprint 'Average number of daily requests per Hosts is:\\n'\navg_daily_req_per_host_df.show(100)\n","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Average number of daily requests per Hosts is:\n\n+---+-------------------------+\n|day|avg_reqs_per_host_per_day|\n+---+-------------------------+\n|  1|       13.166537567776917|\n|  3|       12.845127250155183|\n|  4|       14.213365155131266|\n|  5|       12.745003996802557|\n|  6|       12.777296018919984|\n|  7|       13.968582562104238|\n|  8|       13.650022696323196|\n|  9|        14.00440120454019|\n| 10|       13.540791510059695|\n| 11|       14.091578462954441|\n| 12|       13.292597765363128|\n| 13|       13.766037735849057|\n| 14|       13.442523574315222|\n| 15|       13.964167062173706|\n| 16|       13.053225806451612|\n| 17|       13.450399087799315|\n| 18|       13.494241842610364|\n| 19|       12.585098039215687|\n| 20|             12.876171875|\n| 21|       13.434687953555878|\n| 22|       12.961849192100539|\n+---+-------------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1542508069179_1569980275","id":"20160718-212933_1690059352","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:399"},{"text":"%pyspark\n\n# TEST Average number of daily requests per hosts (4e)\navg_daily_req_per_host_list = (\n  avg_daily_req_per_host_df.select('day', \n  avg_daily_req_per_host_df['avg_reqs_per_host_per_day'].cast('integer').alias('avg_requests'))\n                           .collect()\n)\n\nvalues = [(row[0], row[1]) for row in avg_daily_req_per_host_list]\nprint values\n\nassert values == [(1, 13), (3, 12), (4, 14), (5, 12), (6, 12), (7, 13), (8, 13), (9, 14), (10, 13), (11, 14), (12, 13), (13, 13), (14, 13), (15, 13), (16, 13), (17, 13), (18, 13), (19, 12), (20, 12), (21, 13), (22, 12)], 'incorrect avgDailyReqPerHostDF'\nassert avg_daily_req_per_host_df.is_cached == True, 'incorrect avg_daily_req_per_host_df.is_cached'\n","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[(1, 13), (3, 12), (4, 14), (5, 12), (6, 12), (7, 13), (8, 13), (9, 14), (10, 13), (11, 14), (12, 13), (13, 13), (14, 13), (15, 13), (16, 13), (17, 13), (18, 13), (19, 12), (20, 12), (21, 13), (22, 12)]\n"}]},"apps":[],"jobName":"paragraph_1542508069180_-657956922","id":"20160718-213035_1175278862","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:400"},{"text":"%md\n\nLet's graph the results.","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Let's graph the results.</p>\n"}]},"apps":[],"jobName":"paragraph_1542508069180_128147840","id":"20160723-180952_412359229","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:401"},{"text":"%pyspark\n\navg_daily_req_per_host_df.registerTempTable(\"req_per_host\")\nprint avg_daily_req_per_host_df.count()\n","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"21\n"}]},"apps":[],"jobName":"paragraph_1542508069181_1450845284","id":"20160723-181052_1847519247","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:402"},{"text":"%sql\n\nselect day, avg_reqs_per_host_per_day from req_per_host","user":"anonymous","dateUpdated":"2018-11-18T10:28:11+0800","config":{"colWidth":12,"editorMode":"ace/mode/sql","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"day","index":0,"aggr":"sum"}],"values":[{"name":"avg_reqs_per_host_per_day","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"day","index":0,"aggr":"sum"},"yAxis":{"name":"avg_reqs_per_host_per_day","index":1,"aggr":"sum"}},"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"day":"string","avg_reqs_per_host_per_day":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"day\tavg_reqs_per_host_per_day\n1\t13.166537567776917\n3\t12.845127250155183\n4\t14.213365155131266\n5\t12.745003996802557\n6\t12.777296018919984\n7\t13.968582562104238\n8\t13.650022696323196\n9\t14.00440120454019\n10\t13.540791510059695\n11\t14.091578462954441\n12\t13.292597765363128\n13\t13.766037735849057\n14\t13.442523574315222\n15\t13.964167062173706\n16\t13.053225806451612\n17\t13.450399087799315\n18\t13.494241842610364\n19\t12.585098039215687\n20\t12.876171875\n21\t13.434687953555878\n22\t12.961849192100539\n"}]},"apps":[],"jobName":"paragraph_1542508069182_-1220055276","id":"20160723-181013_89516033","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:403"},{"text":"%pyspark\n\ndays_with_avg = []\navgs = []\nfor day, avg in avg_daily_req_per_host_df.collect():\n  days_with_avg.append(day)\n  avgs.append(avg)\n\nprint(days_with_avg)\nprint(avgs)","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n[13.166537567776917, 12.845127250155183, 14.213365155131266, 12.745003996802557, 12.777296018919984, 13.968582562104238, 13.650022696323196, 14.00440120454019, 13.540791510059695, 14.091578462954441, 13.292597765363128, 13.766037735849057, 13.442523574315222, 13.964167062173706, 13.053225806451612, 13.450399087799315, 13.494241842610364, 12.585098039215687, 12.876171875, 13.434687953555878, 12.961849192100539]\n"}]},"apps":[],"jobName":"paragraph_1542508069182_-678068617","id":"20160718-213303_164194322","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:404"},{"text":"%pyspark\n\nassert days_with_avg == [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22], 'incorrect days'\nassert [int(a) for a in avgs] == [13, 12, 14, 12, 12, 13, 13, 14, 13, 14, 13, 13, 13, 13, 13, 13, 13, 12, 12, 13, 12], 'incorrect avgs'","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1542508069183_1131102563","id":"20160718-213414_1518372465","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:405"},{"text":"%md\n\n## Part 5: 关于返回404状态码的请求分析\n\nLet's drill down and explore the error 404 status records. We've all seen those \"404 Not Found\" web pages. 404 errors are returned when the server cannot find the resource (page or object) the browser or client requested.","user":"anonymous","dateUpdated":"2019-03-01T16:47:46+0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Part 5: 关于返回404状态码的请求分析</h2>\n<p>Let&rsquo;s drill down and explore the error 404 status records. We&rsquo;ve all seen those &ldquo;404 Not Found&rdquo; web pages. 404 errors are returned when the server cannot find the resource (page or object) the browser or client requested.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542508069183_1957699869","id":"20160723-181412_1191063726","dateCreated":"2018-11-18T10:27:49+0800","dateStarted":"2019-03-01T16:47:46+0800","dateFinished":"2019-03-01T16:47:46+0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:406"},{"text":"%md\n\n### (5a) Exercise: Counting 404 Response Codes\n\nCreate a DataFrame containing only log records with a 404 status code. Make sure you `cache()` the `not_found_df` as we will use it in the rest of this exercise.\n\nHow many 404 records are in the log?","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>(5a) Exercise: Counting 404 Response Codes</h3>\n<p>Create a DataFrame containing only log records with a 404 status code. Make sure you <code>cache()</code> the <code>not_found_df</code> as we will use it in the rest of this exercise.</p>\n<p>How many 404 records are in the log?</p>\n"}]},"apps":[],"jobName":"paragraph_1542508069184_-1307360445","id":"20160723-181654_2080362007","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:407"},{"text":"%pyspark\n\nprint logs_df.columns\nprint logs_df.dtypes","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"['host', 'path', 'status', 'content_size', 'time']\n[('host', 'string'), ('path', 'string'), ('status', 'int'), ('content_size', 'int'), ('time', 'timestamp')]\n"}]},"apps":[],"jobName":"paragraph_1542508069185_-1388530106","id":"20160718-213712_1105242829","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:408"},{"text":"%pyspark\n\nfrom pyspark.sql.types import IntegerType\n\nnot_found_df = logs_df.select('*').filter(logs_df['status'] == 404)\nnot_found_df.cache()\n\nprint 'Found {0} 404 URLs'.format(not_found_df.count())","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 6185 404 URLs\n"}]},"apps":[],"jobName":"paragraph_1542508069185_1725474510","id":"20160718-213459_1837816077","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:409"},{"text":"%pyspark\n\nassert not_found_df.count() == 6185, 'incorrect not_found_df.count()'\nassert not_found_df.is_cached == True, 'incorrect not_found_df.is_cached'","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1542508069186_-300184915","id":"20160718-213533_793904823","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:410"},{"text":"%md\n\n### (5b) Exercise: Listing 404 Status Code Records\n\nUsing the DataFrame containing only log records with a 404 status code that you cached in part (5a), print out a list up to 40 _distinct_ paths that generate 404 errors.\n\n**No path should appear more than once in your list.**","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>(5b) Exercise: Listing 404 Status Code Records</h3>\n<p>Using the DataFrame containing only log records with a 404 status code that you cached in part (5a), print out a list up to 40 <em>distinct</em> paths that generate 404 errors.</p>\n<p><strong>No path should appear more than once in your list.</strong></p>\n"}]},"apps":[],"jobName":"paragraph_1542508069186_1751040507","id":"20160723-181759_431470365","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:411"},{"text":"%pyspark\n\nnot_found_paths_df = not_found_df.select('path')\nunique_not_found_paths_df = not_found_paths_df.distinct()\n\nprint '404 URLS:\\n'\nunique_not_found_paths_df.show(n=40, truncate=False)","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"404 URLS:\n\n+--------------------------------------------------------------------+\n|path                                                                |\n+--------------------------------------------------------------------+\n|/history/apollo/apollo-13/apollo-13.html.                           |\n|/www/shuttle/countdown/liftoff.html                                 |\n|/shuttle/missions/sts-25/images/images.html                         |\n|/shutttle/missions/sts-70/                                          |\n|/history/discovery                                                  |\n|/html/STS66.htm                                                     |\n|/software.winvn.winvn.html                                          |\n|/images/KSCogosmall.gif                                             |\n|/news/sci.space.shuttle/archive/sci-space-shuttle-15-mar-1995-00.txt|\n|/astronaut-candidates/pub/FTPfiles/vomit-comet.postings             |\n|/history/apollo/apollo13                                            |\n|/statistics/1995/Jul/Jul95_request.gif                              |\n|/shuttle/resources/orbiters/discovery.gif                           |\n|://                                                                 |\n|/manned                                                             |\n|/128.159.104.89/tv/tv.html                                          |\n|/elv/SCOUT/elvpage.htm                                              |\n|/images/crawlerway.gif                                              |\n|/kscarea-small.gif                                                  |\n|/shuttle/mission/sts-69/countdown.html                              |\n|/history/skylab/skylab-3.gif                                        |\n|/software/winvn/winvnhtml                                           |\n|/history/apollo/a-001/movies/                                       |\n|/history/apollo/pad-abort-test-1/pad-abort-test-1-patch-small.gif   |\n|/ads/images/wsji_ad.gif                                             |\n|/statistics/Mar.wwwstats.html                                       |\n|/statistics/1995/Jul/Jul95_byte.gif                                 |\n|/kcs.html                                                           |\n|/shuttle/technology/sts-newsref/sts-jsc.html'                       |\n|/statistics/1995/Aug/Aug95_request.gif                              |\n|/.nasa.gov/shuttle/                                                 |\n|/.../liftoff.html                                                   |\n|/facilities/mil.htm/                                                |\n|/ksk.html                                                           |\n|/shuttle/missions/51-L/mission-51-l.html                            |\n|/\\\\yahoo.com                                                        |\n|/shuttle/missions/.html                                             |\n|/history/apollo/a-004/movies/                                       |\n|/history/apollo/sa-1/docs/                                          |\n|/magazine/p08aug/08pet11.gif                                        |\n+--------------------------------------------------------------------+\nonly showing top 40 rows\n\n"}]},"apps":[],"jobName":"paragraph_1542508069187_-1672410904","id":"20160718-214551_72576099","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:412"},{"text":"%pyspark\n\nbad_unique_paths_40 = set([row[0] for row in unique_not_found_paths_df.take(40)])\n\nassert len(bad_unique_paths_40) == 40, 'bad_unique_paths_40 not distinct'","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1542508069188_-1371022152","id":"20160718-214612_397915329","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:413"},{"text":"%md\n\n### (5c) Exercise: Listing the Top Twenty 404 Response Code paths\n\nUsing the DataFrame containing only log records with a 404 response code that you cached in part (5a), print out a list of the top twenty paths that generate the most 404 errors.\n\n*Remember, top paths should be in sorted order*","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>(5c) Exercise: Listing the Top Twenty 404 Response Code paths</h3>\n<p>Using the DataFrame containing only log records with a 404 response code that you cached in part (5a), print out a list of the top twenty paths that generate the most 404 errors.</p>\n<p><em>Remember, top paths should be in sorted order</em></p>\n"}]},"apps":[],"jobName":"paragraph_1542508069188_1880454074","id":"20160723-181927_436530443","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:414"},{"text":"%pyspark\n\ntop_20_not_found_df = not_found_paths_df.groupBy('path').count().sort('count', ascending = False)\n\nprint 'Top Twenty 404 URLs:\\n'\ntop_20_not_found_df.show(n=20, truncate=False)\n","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Top Twenty 404 URLs:\n\n+-----------------------------------------------------------------+-----+\n|path                                                             |count|\n+-----------------------------------------------------------------+-----+\n|/pub/winvn/readme.txt                                            |633  |\n|/pub/winvn/release.txt                                           |494  |\n|/shuttle/missions/STS-69/mission-STS-69.html                     |430  |\n|/images/nasa-logo.gif                                            |319  |\n|/elv/DELTA/uncons.htm                                            |178  |\n|/shuttle/missions/sts-68/ksc-upclose.gif                         |154  |\n|/history/apollo/sa-1/sa-1-patch-small.gif                        |146  |\n|/images/crawlerway-logo.gif                                      |120  |\n|/://spacelink.msfc.nasa.gov                                      |117  |\n|/history/apollo/pad-abort-test-1/pad-abort-test-1-patch-small.gif|100  |\n|/history/apollo/a-001/a-001-patch-small.gif                      |97   |\n|/images/Nasa-logo.gif                                            |85   |\n|                                                                 |76   |\n|/shuttle/resources/orbiters/atlantis.gif                         |63   |\n|/history/apollo/images/little-joe.jpg                            |62   |\n|/images/lf-logo.gif                                              |59   |\n|/shuttle/resources/orbiters/discovery.gif                        |56   |\n|/shuttle/resources/orbiters/challenger.gif                       |54   |\n|/robots.txt                                                      |53   |\n|/history/apollo/pad-abort-test-2/pad-abort-test-2-patch-small.gif|38   |\n+-----------------------------------------------------------------+-----+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1542508069189_76144497","id":"20160718-214654_132253630","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:415"},{"text":"%pyspark\n\n# TEST Top twenty 404 URLs (5c)\n\ntop_20_not_found = [(row[0], row[1]) for row in top_20_not_found_df.take(20)]\ntop_20_expected = [\n (u'/pub/winvn/readme.txt', 633),\n (u'/pub/winvn/release.txt', 494),\n (u'/shuttle/missions/STS-69/mission-STS-69.html', 430),\n (u'/images/nasa-logo.gif', 319),\n (u'/elv/DELTA/uncons.htm', 178),\n (u'/shuttle/missions/sts-68/ksc-upclose.gif', 154),\n (u'/history/apollo/sa-1/sa-1-patch-small.gif', 146),\n (u'/images/crawlerway-logo.gif', 120),\n (u'/://spacelink.msfc.nasa.gov', 117),\n (u'/history/apollo/pad-abort-test-1/pad-abort-test-1-patch-small.gif', 100),\n (u'/history/apollo/a-001/a-001-patch-small.gif', 97),\n (u'/images/Nasa-logo.gif', 85),\n (u'', 76),\n (u'/shuttle/resources/orbiters/atlantis.gif', 63),\n (u'/history/apollo/images/little-joe.jpg', 62),\n (u'/images/lf-logo.gif', 59),\n (u'/shuttle/resources/orbiters/discovery.gif', 56),\n (u'/shuttle/resources/orbiters/challenger.gif', 54),\n (u'/robots.txt', 53),\n (u'/history/apollo/pad-abort-test-2/pad-abort-test-2-patch-small.gif', 38)\n]\n\nassert top_20_not_found == top_20_expected, 'incorrect top_20_not_found'\n","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1542508069189_905725310","id":"20160718-214713_1503319001","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:416"},{"text":"%md\n\n### (5d) Exercise: Listing the Top Twenty-five 404 Response Code Hosts\n\nInstead of looking at the paths that generated 404 errors, let's look at the hosts that encountered 404 errors. Using the DataFrame containing only log records with a 404 status codes that you cached in part (5a), print out a list of the top twenty-five hosts that generate the most 404 errors.","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>(5d) Exercise: Listing the Top Twenty-five 404 Response Code Hosts</h3>\n<p>Instead of looking at the paths that generated 404 errors, let's look at the hosts that encountered 404 errors. Using the DataFrame containing only log records with a 404 status codes that you cached in part (5a), print out a list of the top twenty-five hosts that generate the most 404 errors.</p>\n"}]},"apps":[],"jobName":"paragraph_1542508069190_1226851745","id":"20160723-182052_1374107064","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:417"},{"text":"%pyspark\n\nhosts_404_count_df = not_found_df.groupBy('host').count().sort('count', ascending=False)\n\nprint 'Top 25 hosts that generated errors:\\n'\nhosts_404_count_df.show(n=25, truncate=False)\n","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Top 25 hosts that generated errors:\n\n+-----------------------------+-----+\n|host                         |count|\n+-----------------------------+-----+\n|maz3.maz.net                 |39   |\n|piweba3y.prodigy.com         |39   |\n|gate.barr.com                |38   |\n|nexus.mlckew.edu.au          |37   |\n|m38-370-9.mit.edu            |37   |\n|ts8-1.westwood.ts.ucla.edu   |37   |\n|204.62.245.32                |33   |\n|163.206.104.34               |27   |\n|spica.sci.isas.ac.jp         |27   |\n|www-d4.proxy.aol.com         |26   |\n|203.13.168.17                |25   |\n|203.13.168.24                |25   |\n|www-c4.proxy.aol.com         |25   |\n|internet-gw.watson.ibm.com   |24   |\n|crl5.crl.com                 |23   |\n|piweba5y.prodigy.com         |23   |\n|scooter.pa-x.dec.com         |23   |\n|slip145-189.ut.nl.ibm.net    |22   |\n|onramp2-9.onr.com            |22   |\n|198.40.25.102.sap2.artic.edu |21   |\n|msp1-16.nas.mr.net           |20   |\n|gn2.getnet.com               |20   |\n|isou24.vilspa.esa.es         |19   |\n|tigger.nashscene.com         |19   |\n|dial055.mbnet.mb.ca          |19   |\n+-----------------------------+-----+\nonly showing top 25 rows\n\n"}]},"apps":[],"jobName":"paragraph_1542508069190_378212446","id":"20160718-214746_1184403987","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:418"},{"text":"%pyspark\n\ntop_25_404 = [(row[0], row[1]) for row in hosts_404_count_df.take(25)]\nassert len(top_25_404) == 25, 'length of errHostsTop25 is not 25'\n\nexpected = set([\n  (u'maz3.maz.net ', 39),\n  (u'piweba3y.prodigy.com ', 39),\n  (u'gate.barr.com ', 38),\n  (u'nexus.mlckew.edu.au ', 37),\n  (u'ts8-1.westwood.ts.ucla.edu ', 37),\n  (u'm38-370-9.mit.edu ', 37),\n  (u'204.62.245.32 ', 33),\n  (u'spica.sci.isas.ac.jp ', 27),\n  (u'163.206.104.34 ', 27),\n  (u'www-d4.proxy.aol.com ', 26),\n  (u'203.13.168.17 ', 25),\n  (u'203.13.168.24 ', 25),\n  (u'www-c4.proxy.aol.com ', 25),\n  (u'internet-gw.watson.ibm.com ', 24),\n  (u'crl5.crl.com ', 23),\n  (u'piweba5y.prodigy.com ', 23),\n  (u'scooter.pa-x.dec.com ', 23),\n  (u'onramp2-9.onr.com ', 22),\n  (u'slip145-189.ut.nl.ibm.net ', 22),\n  (u'198.40.25.102.sap2.artic.edu ', 21),\n  (u'msp1-16.nas.mr.net ', 20),\n  (u'gn2.getnet.com ', 20),\n  (u'tigger.nashscene.com ', 19),\n  (u'dial055.mbnet.mb.ca ', 19),\n  (u'isou24.vilspa.esa.es ', 19)\n])\n\nassert (len(set(top_25_404) - expected)) == 0, 'incorrect hosts_404_count_df'\n","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1542508069191_-2132288466","id":"20160718-214852_1529996792","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:419"},{"text":"%md\n\n### (5e) Exercise: Listing 404 Errors per Day\n\nLet's explore the 404 records temporally. Break down the 404 requests by day (cache the `errors_by_date_sorted_df` DataFrame) and get the daily counts sorted by day in `errors_by_date_sorted_df`.\n\n*Since the log only covers a single month, you can ignore the month in your checks.*","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>(5e) Exercise: Listing 404 Errors per Day</h3>\n<p>Let's explore the 404 records temporally. Break down the 404 requests by day (cache the <code>errors_by_date_sorted_df</code> DataFrame) and get the daily counts sorted by day in <code>errors_by_date_sorted_df</code>.</p>\n<p><em>Since the log only covers a single month, you can ignore the month in your checks.</em></p>\n"}]},"apps":[],"jobName":"paragraph_1542508069192_296574563","id":"20160723-182156_228944708","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:420"},{"text":"%pyspark\n\nerrors_by_date_sorted_df = not_found_df.select(dayofmonth('time').alias('day')).groupBy('day').count()\nerrors_by_date_sorted_df.cache()\n\nprint '404 Errors by day:\\n'\nerrors_by_date_sorted_df.show(100)\n","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"404 Errors by day:\n\n+---+-----+\n|day|count|\n+---+-----+\n|  1|  243|\n|  3|  303|\n|  4|  346|\n|  5|  234|\n|  6|  372|\n|  7|  532|\n|  8|  381|\n|  9|  279|\n| 10|  314|\n| 11|  263|\n| 12|  195|\n| 13|  216|\n| 14|  287|\n| 15|  326|\n| 16|  258|\n| 17|  269|\n| 18|  255|\n| 19|  207|\n| 20|  312|\n| 21|  305|\n| 22|  288|\n+---+-----+\n\n"}]},"apps":[],"jobName":"paragraph_1542508069192_1872166993","id":"20160718-214955_1574089020","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:421"},{"text":"%pyspark\n\nerrors_by_date = [(row[0], row[1]) for row in errors_by_date_sorted_df.collect()]\nexpected = [\n  (1, 243),\n  (3, 303),\n  (4, 346),\n  (5, 234),\n  (6, 372),\n  (7, 532),\n  (8, 381),\n  (9, 279),\n  (10, 314),\n  (11, 263),\n  (12, 195),\n  (13, 216),\n  (14, 287),\n  (15, 326),\n  (16, 258),\n  (17, 269),\n  (18, 255),\n  (19, 207),\n  (20, 312),\n  (21, 305),\n  (22, 288)\n]\n\nassert errors_by_date == expected, 'incorrect errors_by_date_sorted_df'\nassert errors_by_date_sorted_df.is_cached == True, 'incorrect errors_by_date_sorted_df.is_cached'\n","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1542508069193_196774984","id":"20160718-215047_1970799170","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:422"},{"text":"%md\n\n### (5f) Exercise: Visualizing the 404 Errors by Day\n\nUsing the results from the previous exercise, use `matplotlib` to plot a line or bar graph of the 404 response codes by day.\n\n**Hint**: You'll need to use the same technique you used in (4f).","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>(5f) Exercise: Visualizing the 404 Errors by Day</h3>\n<p>Using the results from the previous exercise, use <code>matplotlib</code> to plot a line or bar graph of the 404 response codes by day.</p>\n<p><strong>Hint</strong>: You'll need to use the same technique you used in (4f).</p>\n"}]},"apps":[],"jobName":"paragraph_1542508069193_807464755","id":"20160723-182256_1591901297","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:423"},{"text":"%pyspark\n\ndays_with_errors_404 = []\nerrors_404_by_day = []\nfor day, count in errors_by_date_sorted_df.collect():\n  days_with_errors_404.append(day)\n  errors_404_by_day.append(count)\n\nprint days_with_errors_404\nprint errors_404_by_day\n","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n[243, 303, 346, 234, 372, 532, 381, 279, 314, 263, 195, 216, 287, 326, 258, 269, 255, 207, 312, 305, 288]\n"}]},"apps":[],"jobName":"paragraph_1542508069194_-423366565","id":"20160718-215131_1647614447","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:424"},{"text":"%pyspark\n\nassert days_with_errors_404 == [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22], 'incorrect days_with_errors_404'\nassert errors_404_by_day == [243, 303, 346, 234, 372, 532, 381, 279, 314, 263, 195, 216, 287, 326, 258, 269, 255, 207, 312, 305, 288], 'incorrect errors_404_by_day'\n","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1542508069195_947329081","id":"20160718-215153_252344212","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:425"},{"text":"%pyspark\n\nerrors_by_date_sorted_df.show()","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-----+\n|day|count|\n+---+-----+\n|  1|  243|\n|  3|  303|\n|  4|  346|\n|  5|  234|\n|  6|  372|\n|  7|  532|\n|  8|  381|\n|  9|  279|\n| 10|  314|\n| 11|  263|\n| 12|  195|\n| 13|  216|\n| 14|  287|\n| 15|  326|\n| 16|  258|\n| 17|  269|\n| 18|  255|\n| 19|  207|\n| 20|  312|\n| 21|  305|\n+---+-----+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1542508069195_718152683","id":"20160718-215231_338454169","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:426"},{"text":"%md\n\n### (5g) Exercise: Top Five Days for 404 Errors\n\nUsing the DataFrame `errors_by_date_sorted_df` you cached in the part (5e), what are the top five days for 404 errors and the corresponding counts of 404 errors?","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>(5g) Exercise: Top Five Days for 404 Errors</h3>\n<p>Using the DataFrame <code>errors_by_date_sorted_df</code> you cached in the part (5e), what are the top five days for 404 errors and the corresponding counts of 404 errors?</p>\n"}]},"apps":[],"jobName":"paragraph_1542508069196_-1742933974","id":"20160723-182518_1865837020","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:427"},{"text":"%pyspark\n\ntop_err_date_df = errors_by_date_sorted_df.sort('count', ascending=False)\n\nprint 'Top Five Dates for 404 Requests:\\n'\ntop_err_date_df.show(5)\n","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Top Five Dates for 404 Requests:\n\n+---+-----+\n|day|count|\n+---+-----+\n|  7|  532|\n|  8|  381|\n|  6|  372|\n|  4|  346|\n| 15|  326|\n+---+-----+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1542508069197_1436999128","id":"20160718-215258_1747355705","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:428"},{"text":"%pyspark\n\nassert [(r[0], r[1]) for r in top_err_date_df.take(5)] == [(7, 532), (8, 381), (6, 372), (4, 346), (15, 326)], 'incorrect top_err_date_df'\n","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1542508069197_1122147163","id":"20160718-215317_1076899705","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:429"},{"text":"%md\n\n### (5h) Exercise: Hourly 404 Errors\n\nUsing the DataFrame `not_found_df` you cached in the part (5a) and sorting by hour of the day in increasing order, create a DataFrame containing the number of requests that had a 404 return code for each hour of the day (midnight starts at 0). Cache the resulting DataFrame `hour_records_sorted_df` and print that as a list.","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>(5h) Exercise: Hourly 404 Errors</h3>\n<p>Using the DataFrame <code>not_found_df</code> you cached in the part (5a) and sorting by hour of the day in increasing order, create a DataFrame containing the number of requests that had a 404 return code for each hour of the day (midnight starts at 0). Cache the resulting DataFrame <code>hour_records_sorted_df</code> and print that as a list.</p>\n"}]},"apps":[],"jobName":"paragraph_1542508069198_-808964569","id":"20160723-182558_1177349648","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:430"},{"text":"%pyspark\n\nfrom pyspark.sql.functions import hour\n\nhour_records_sorted_df = not_found_df.select(hour('time').alias('hour')).groupBy('hour').count()\nhour_records_sorted_df.cache()\n\nprint 'Top hours for 404 requests:\\n'\nhour_records_sorted_df.show(24)\n","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Top hours for 404 requests:\n\n+----+-----+\n|hour|count|\n+----+-----+\n|   0|  175|\n|   1|  171|\n|   2|  422|\n|   3|  272|\n|   4|  102|\n|   5|   95|\n|   6|   93|\n|   7|  122|\n|   8|  199|\n|   9|  185|\n|  10|  329|\n|  11|  263|\n|  12|  438|\n|  13|  397|\n|  14|  318|\n|  15|  347|\n|  16|  373|\n|  17|  330|\n|  18|  268|\n|  19|  269|\n|  20|  270|\n|  21|  241|\n|  22|  234|\n|  23|  272|\n+----+-----+\n\n"}]},"apps":[],"jobName":"paragraph_1542508069198_1293517156","id":"20160718-215357_990768188","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:431"},{"text":"%pyspark\n\nerrs_by_hour = [(row[0], row[1]) for row in hour_records_sorted_df.collect()]\n\nexpected = [\n  (0, 175),\n  (1, 171),\n  (2, 422),\n  (3, 272),\n  (4, 102),\n  (5, 95),\n  (6, 93),\n  (7, 122),\n  (8, 199),\n  (9, 185),\n  (10, 329),\n  (11, 263),\n  (12, 438),\n  (13, 397),\n  (14, 318),\n  (15, 347),\n  (16, 373),\n  (17, 330),\n  (18, 268),\n  (19, 269),\n  (20, 270),\n  (21, 241),\n  (22, 234),\n  (23, 272)\n]\n\nassert errs_by_hour == expected, 'incorrect errs_by_hour'\nassert hour_records_sorted_df.is_cached == True, 'incorrect hour_records_sorted_df.is_cached'\n","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1542508069198_-135363078","id":"20160718-215425_1194811262","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:432"},{"text":"%md\n\n### (5i) Exercise: Visualizing the 404 Response Codes by Hour\n\nUsing the results from the previous exercise, plot a line or bar graph of the 404 response codes by hour.","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>(5i) Exercise: Visualizing the 404 Response Codes by Hour</h3>\n<p>Using the results from the previous exercise, plot a line or bar graph of the 404 response codes by hour.</p>\n"}]},"apps":[],"jobName":"paragraph_1542508069199_485755889","id":"20160723-182645_1328282364","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:433"},{"text":"%pyspark\n\nhours_with_not_found = hour_records_sorted_df.select('hour').map(lambda row: row[0]).collect()\nnot_found_counts_per_hour = hour_records_sorted_df.select('count').map(lambda row: row[0]).collect()\n\nprint hours_with_not_found\nprint not_found_counts_per_hour\n","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n[175, 171, 422, 272, 102, 95, 93, 122, 199, 185, 329, 263, 438, 397, 318, 347, 373, 330, 268, 269, 270, 241, 234, 272]\n"}]},"apps":[],"jobName":"paragraph_1542508069199_1978088183","id":"20160718-215511_1439506524","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:434"},{"text":"%pyspark\n\nassert hours_with_not_found == [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], 'incorrect hours_with_not_found'\nassert not_found_counts_per_hour == [175, 171, 422, 272, 102, 95, 93, 122, 199, 185, 329, 263, 438, 397, 318, 347, 373, 330, 268, 269, 270, 241, 234, 272], 'incorrect not_found_counts_per_hour'\n","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1542508069200_-1448685354","id":"20160718-215528_1196284276","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:435"},{"text":"%pyspark\n\nhour_records_sorted_df.show()","user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----+-----+\n|hour|count|\n+----+-----+\n|   0|  175|\n|   1|  171|\n|   2|  422|\n|   3|  272|\n|   4|  102|\n|   5|   95|\n|   6|   93|\n|   7|  122|\n|   8|  199|\n|   9|  185|\n|  10|  329|\n|  11|  263|\n|  12|  438|\n|  13|  397|\n|  14|  318|\n|  15|  347|\n|  16|  373|\n|  17|  330|\n|  18|  268|\n|  19|  269|\n+----+-----+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1542508069200_-1024605983","id":"20160718-215604_634878316","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:436"},{"text":"%pyspark\n\nprint 'This was last run on: {0}'.format(datetime.datetime.now())","user":"anonymous","dateUpdated":"2019-03-01T16:47:55+0800","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"This was last run on: 2019-03-01 16:47:55.972843\n"}]},"apps":[],"jobName":"paragraph_1542508069200_1427357981","id":"20160718-215627_361302355","dateCreated":"2018-11-18T10:27:49+0800","dateStarted":"2019-03-01T16:47:55+0800","dateFinished":"2019-03-01T16:47:55+0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:437"},{"user":"anonymous","dateUpdated":"2018-11-18T10:27:49+0800","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[],"enabled":true,"fontSize":9,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1542508069201_-1612089379","id":"20160718-215658_906219167","dateCreated":"2018-11-18T10:27:49+0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:438"}],"name":"Workshop: Analyzing Web Logs","id":"2DUWJCVEM","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}