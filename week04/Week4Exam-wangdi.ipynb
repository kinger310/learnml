{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习集训营八期第四周(Hadoop&Spark)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2019年05月31日至06月03日期间完成，最晚提交时间下周一（06月03日12：00时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试提交方式：请同学<font color=red><b>拷贝</b></font>该试卷后，将文件更名为同学姓名拼音-exam4（例如wangwei-exam4）后<font color=red><b>移动</b></font>至/0.Teacher/Exam/4/目录下进行作答。\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分处不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:王迪\n",
    "- 批改人：   \n",
    "- 最终得分:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共5题，每题8分，共计40分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.简述Hadoop解决了单机的什么问题？Spark与之相比又有哪些优点？"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "一味依靠单机提高数据处理效率、增加存储量；显然，不仅成本高，而且提升有限，很容易遇到瓶颈。\n",
    "Hadoop的出现解决了海量数据的存储和计算问题。不仅使得大数据处理可以运行在廉价硬件，而能有高容错的数据存储、较快的计算和高效的资源调配\n",
    "\n",
    "Spark与Hadoop相比，Spark有如下优点：\n",
    "1. 数据处理速度快。Spark的数据处理速度远胜于MapReduce。因为Spark的中间输出结果可以保存在内存中，不需要再频繁读写HDFS。\n",
    "2. Spark将数据对象抽象成RDD、DataFrame和DataSet。并且支持SQL，在数据分析上可以方便地处理更复杂的逻辑。\n",
    "3. Spark提供SparkUI，Spark Streaming等工具，可以方便地进行流式计算，而MapReduce只能进行离线计算\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.请简述您对HDFS架构的理解 "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Hadoop分布式文件系统（HDFS）是一种分布式文件系统，被设计可以在廉价硬件上运行。它与现有的分布式文件系统有许多相似之处。 但是，HDFS有如下特点。第一，HDFS具有高容错能力，通过多个（通常3个）副本，部署在廉价硬件上。\n",
    "第二，HDFS提供对应用程序数据的高吞吐量访问，适用于具有大型数据集的应用程序。\n",
    "第三，HDFS放宽了一些POSIX要求，以实现对文件系统数据的流式访问。比如Spark可以运行在HDFS上。\n",
    "HDFS最初是作为Apache Nutch网络搜索引擎项目的基础设施而构建的。HDFS现在是Apache Hadoop Core项目的一部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.请简述DataFrame的含义，并写出针对DataFrame的两类操作（对应RDD的transformation与action),每类下至少三种的操作。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "DataFrame是一种以RDD为基础的分布式数据集，它的设计借鉴了R语言和pandas的DataFrame，类似传统数据库中的二维表格。\n",
    "\n",
    "\n",
    "transformation.\n",
    "select, filter, withColumn, sort, groupBy...\n",
    "\n",
    "action\n",
    "collect, first, take, show..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.请简述我们为什么优先使用DataFrame而不是RDD"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. DataFrame数据信息比RDD多。DataFrame提供了数据详细的结构信息, 使得Spark可以清楚地知道该数据集中包含哪些列, 每列的名称和类型各是什么. DataFrame多了数据的结构信息(Schema).\n",
    "2. DataFrame运行效率比RDD高。这得益于DataFrame处理数据时的广播机制。Spark对于DataFrame在执行时间和内存使用上相对于RDD都有极大的优化\n",
    "3. DataFrame与pandas类似，上手快，且书写比RDD更优雅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.Spark内置了机器学习库mllib，请写出使用该库完成一项机器学习任务的通用步骤\n",
    "- （注意：仅步骤即可，可GOOGLE）"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "主要分三步\n",
    "\n",
    "1. training. 获取训练数据集DataFrame, 通过特征工程等找到合适的feature。引入合适的模型（Estimator）和参数（Parameter）。训练模型。得到一个model。\n",
    "2. testing。 获取测试数据集的DataFrame,将训练好的model在测试集上进行transform，得到预测值。\n",
    "3. tuning. 运用交叉检验等手段，进行模型评估和调节参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验题(共2题，共计60分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.HDFS实验题(25分，共5个小题，每个小题5分)\n",
    "#### 请写出完成以下任务的HDFS对应的文件(夹)操作命令\n",
    "- 1.在hdfs根目录下新建/sxy-exam/您的学号 文件夹"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "我的本地工作目录/home/student/spark_wangdi\n",
    "数据均已复制到自己的工作目录下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-06-01T12:51:39.881Z"
    }
   },
   "outputs": [],
   "source": [
    "hadoop fs -mkdir -p /sxy-exam/460597"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.把本地文件text1.txt text2.txt放入该文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop fs -put data/text1.txt /sxy-exam/460597\n",
    "hadoop fs -put data/text2.txt /sxy-exam/460597"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3.从hdfs上取下文件old.txt(假定在/sxy-exam 下有该文件)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop fs -get /sxy-exam/old.txt myold.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4.从hdfs上取下/sxy-exam 中所有内容，并合成一个本地文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop fs -getmerge /sxy-exam/460597/* myoutput.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 5.把/sxy-exam 拷贝到/tmp下后，删除/sxy-exam 文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop fs -mkdir -p /tmp/460597/\n",
    "hadoop fs -cp  /sxy-exam/460597/* /tmp/460597/\n",
    "hadoop fs -ls /tmp/460597/\n",
    "\n",
    "hadoop fs -rm -r /sxy-exam/460597/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Spark实验题(35分)\n",
    "请在集群上操作，并在下面的输入框里填入相应的代码\n",
    "\n",
    "服务器上数据文件放在 /home/student/exam/data/2015-12-12.csv\n",
    "\n",
    "在服务器上启动pyspark或spark-shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "conf=SparkConf().setAppName(\"miniProject\").setMaster(\"local[*]\")\n",
    "sc=SparkContext.getOrCreate(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.载入数据与了解基本信息   (共10分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 载入RDD，查看RDD分区数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_content = sc.textFile(r\"file:///C:\\Users\\wangdi03\\Desktop\\fsdownload\\2015-12-12.csv\").map(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read.csv(\"file:///home/student/spark_wangdi/2015-12-12.csv\", header=True)\n",
    "# raw_content = df.rdd\n",
    "raw_content.getNumPartitions()\n",
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 通过命令统计raw_content中的记录条数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_content.count()\n",
    "# 421970"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 从raw_content这个RDD中取出前5条记录输出\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_content.take(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 从raw_content中采样出3条记录输出\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_content.takeSample(False, 3, 812)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示：你将看到类似如下的结果：\n",
    "```\n",
    "[\n",
    "[u'\"date\"', u'\"time\"', u'\"size\"', u'\"r_version\"', u'\"r_arch\"', u'\"r_os\"', u'\"package\"', u'\"version\"', u'\"country\"', u'\"ip_id\"'], \n",
    "[u'\"2015-12-12\"', u'\"13:42:10\"', u'257886', u'\"3.2.2\"', u'\"i386\"', u'\"mingw32\"', u'\"HistData\"', u'\"0.7-6\"', u'\"CZ\"', u'1'], \n",
    "[u'\"2015-12-12\"', u'\"13:24:37\"', u'1236751', u'\"3.2.2\"', u'\"x86_64\"', u'\"mingw32\"', u'\"RJSONIO\"', u'\"1.3-0\"', u'\"DE\"', u'2']\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.数据处理1   (5分)\n",
    "- 对以上数据进行清洗，把记录中带的引号去除掉，得到content_rdd\n",
    "- tip：参考map函数的用法\n",
    "\n",
    "你将得到类似以下结果的RDD：\n",
    "```\n",
    "[\n",
    "[u'date', u'time', u'size', u'r_version', u'r_arch', u'r_os', u'package', u'version', u'country', u'ip_id'], \n",
    "[u'2015-12-12', u'13:42:10', u'257886', u'3.2.2', u'i386', u'mingw32', u'HistData', u'0.7-6', u'CZ', u'1'], \n",
    "[u'2015-12-12', u'13:24:37', u'1236751', u'3.2.2', u'x86_64', u'mingw32', u'RJSONIO', u'1.3-0', u'DE', u'2'], \n",
    "[u'2015-12-12', u'13:42:35', u'2077876', u'3.2.2', u'i386', u'mingw32', u'UsingR', u'2.0-5', u'CZ', u'1']\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_rdd = raw_content.map(lambda x: [i.replace('\"', '') for i in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. 数据处理2（5分)\n",
    "- 有如下的text，请使用flatmap得到包含所有字幕的list\n",
    "- tip：请参考课程flatmap的使用，特别注意这里不同分割符的处理\n",
    "\n",
    "text=[\"a b c\", \"d e\", \"f#g#h\", \"m n q\", \"r,q,w\", \"j%r%q\"]\n",
    "\n",
    "你将得到如下结果的RDD：\n",
    "```\n",
    "[a, b, c, d, e, f, g, h, m, n, q, r, q, w, j, r, q]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def proc_str(x):\n",
    "    return re.split(r\"[%# ,]\", x)\n",
    "\n",
    "text=[\"a b c\", \"d e\", \"f#g#h\", \"m n q\", \"r,q,w\", \"j%r%q\"]\n",
    "\n",
    "my_rdd = sc.parallelize(text)\n",
    "sorted(my_rdd.flatMap(lambda x: proc_str(x)).distinct().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. 数据处理3（5分）\n",
    "- 从刚才的content_rdd中取出第7列，对不同的package类别进行统计计数\n",
    "- tip：可以使用map reduce或者pair-rdd reduceByKey\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "content_rdd.map(lambda x: (x[6],1)).reduceByKey(add).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.排序   (5分)\n",
    "- 取出数量最多的10个package和出现的频次\n",
    "- tip：注意sortbykey的使用\n",
    "\n",
    "你将看到类似如下的结果：\n",
    "```\n",
    "[(4783, u'Rcpp'),\n",
    " (3913, u'ggplot2'),\n",
    " (3748, u'stringi'),\n",
    " (3449, u'stringr'),\n",
    " (3436, u'plyr'),\n",
    " (3265, u'magrittr'),\n",
    " (3223, u'digest'),\n",
    " (3205, u'reshape2'),\n",
    " (3046, u'RColorBrewer'),\n",
    " (3007, u'scales')]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_rdd.map(lambda x: (x[6],1)).reduceByKey(add).sortBy(lambda x:x[1], ascending=False).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.过滤  (5分)\n",
    "\n",
    "- 取出top5的package对应的数据记录\n",
    "- tip：注意filter的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_pack_lst = content_rdd.map(lambda x: (x[6],1)).reduceByKey(add).sortBy(lambda x:x[1], ascending=False).take(5)\n",
    "top5_packs = [y[0] for y in top5_pack_lst]\n",
    "content_rdd.filter(lambda x: x[6] in top5_packs).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本周课程意见反馈(非必答)\n",
    "请同学围绕以下两点进行回答：\n",
    "- 自身总结：您自己在本周课程的学习，收获，技能掌握等方面进行总结，包括自身在哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n",
    "- 课程反馈：也可以就知识点，进度，难易度，教学方式，考试方式等等进行意见反馈，督促我们进行更有效的改进，为大家提供更优质的服务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
