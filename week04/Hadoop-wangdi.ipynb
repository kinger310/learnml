{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 大数据第一课 Hadoop作业\n",
    "\n",
    "####  作业提交说明：\n",
    "- 位置：作业文件统一放置于/0.Teacher/Exercise/BigData/Hadoop下\n",
    "- 文件名：请先复制该notebook文件，并重新命名为(课程名)+(您姓名的全拼)，并按要求完成后保存\n",
    "- 时间：课程结束后的第二天前提交。\n",
    "- 注意：请勿抄袭，移动，修改，删除其他同学和原始空白的练习文件。\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简答题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.为什么会诞生Hadoop，它是做什么事情的？"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "大数据时代催生了Hadoop。\n",
    "\n",
    "\n",
    "The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer, so delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures.\n",
    "Apache Hadoop是一个框架。Hadoop简单的编程模型，跨集群实现分布式处理大型数据集。它旨在从单机扩展到数千台计算机，每台计算机都提供本地计算和存储。Hadoop本身不依靠硬件来提供高可用性，而是通过检测和处理应用层的故障，从而在计算机集群（Hadoop设计的假设是每台计算机都有可能出故障）之上提供高可用性服务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.HDFS是通过什么方式保证数据不会丢失？"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "数据自动保存多个副本，副本丢失后，自动恢复\n",
    "核心设计思想：  分散均匀存储 + 备份冗余存储"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.请简述您对MAP-REDUCE这一编程模型的理解 "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Map Reduce 的核心思想就是分治。\n",
    "（1）Mapper负责“分”，即把复杂的任务分解为若干个“简单的任务”来处理。“简单的任务”包含三层含义：一是数据或计算的规模相对原任务要大大缩小；二是就近计算原则，即任务会分配到存放着所需数据的节点上进行计算；三是这些小任务可以并行计算，彼此间几乎没有依赖关系。\n",
    "\n",
    "（2）Reducer负责对map阶段的结果进行汇总。至于需要多少个Reducer，用户可以根据具体问题，通过在mapred-site.xml配置文件里设置参数mapred.reduce.tasks的值，缺省值为1。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验题\n",
    "### 请写出完成以下任务的HDFS以及MapReduce相关作业\n",
    "- 1.在hdfs根目录下新建/sxy-new/{您的七月在线学号}/input/文件夹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hadoop fs -mkdir /sxy-new/wangdi/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.把本地文件/home/student/teacher/data 下面的 test1.txt test2.txt, test3.txt 放入上一步创建的hdfs文件夹（注，test.txt与test2.txt可以是同学自建的空文件）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hadoop fs -put data/text*.txt /sxy-new/wangdi/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3.本地验证基于Python实现的MapReduce程序是否可以成功运行，运行代码在 /home/student/teacher/code 下供参考，大家可以自己手动实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "echo \"foo foo quux labs foo bar quux\" | python count_mapper.py | sort -k1,1 | python count_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4.在hdfs根目录下新建/sxy-new/{您的七月在线学号}/output/ 文件夹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hadoop fs -mkdir /sxy-new/wangdi/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 5.通过hadoop Streaming 运行基于Python编写的MapReduce程序，读取 /sxy-new/{您的七月在线学号}/input/ 下的所有文件，将统计结果写入 /sxy-new/{您的七月在线学号}/output/ 文件夹下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hadoop jar hadoop-streaming-2.8.5.jar -D mapreduce.job.name=\"wangdi008_test\" -file code/count_mapper.py  -mapper code/count_mapper.py  -file code/count_reducer.py -reducer code/count_reducer.py -input /sxy-new/wangdi/input/*  -output /sxy-new/wangdi/output/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 6. 检查作业是否成功运行，解决运行失败的错误"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hadoop fs -ls /sxy-new/wangdi/output/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 7. 将hdfs上 /sxy-new/{您的七月在线学号}/output/ 文件夹下的结果拉取到本地，合并成一个文件，该文件的位置为 ./{您的七月在线学号}/output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hadoop fs -getmerge /sxy-new/wangdi/output/1 output.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
