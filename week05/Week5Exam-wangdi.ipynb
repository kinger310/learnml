{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习集训营八期第五周(ML)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2019年06月14日至06月17日期间完成，最晚提交时间下周一（06月17日12：00时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试提交方式：请同学<font color=red><b>拷贝</b></font>该试卷后，将文件更名为同学姓名拼音-exam5（例如wangwei-exam5）后<font color=red><b>移动</b></font>至/0.Teacher/Exam/5/目录下进行作答。\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分处不用填写\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:王迪\n",
    "- 批改人：   \n",
    "- 最终得分:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共10题，每题10分，共计100分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.请分别解释统计机器学习中的输入/输出空间，特征空间，假设空间与参数空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 输入空间：输入所有可能取值的集合，记为$\\mathcal{X}$。\n",
    "- 输出空间：输出所有可能取值的集合，记为$\\mathcal{Y}$。\n",
    "- 特征空间：每个具体的输入是一个实例（instance），通常由特征向量表示。所有特征向量存在的空间成为特征空间。\n",
    "- 假设空间：由输入空间到输出空间的映射的集合，被称为假设空间。$$\\mathcal{F}= \\{f|Y=f(X)\\}$$\n",
    "  X和Y是定义在输入空间$\\mathcal{X}$和输出空间$\\mathcal{Y}$上的变量\n",
    "- 参数空间：假设空间$\\mathcal{F}$通常是由一个参数向量决定的函数族。\n",
    "    $$\n",
    "    \\mathcal{F}=\\left\\{f | Y=f_{\\theta}(X), \\theta \\in \\mathbf{R}^{n}\\right\\}\n",
    "    $$\n",
    "  参数向量$\\theta$取值于$n$维欧式空间$R^n$，称为参数空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.什么是损失函数，并举例有哪些常用的损失函数？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数是f(X)和Y的非负函数，衡量输出的预测值f(X)和真实值Y不一致的程度，也即预测错误的程度。记作$L(Y, f(X))$\n",
    "常用的损失函数\n",
    "\n",
    "0-1损失函数\n",
    "$$L(Y, f(X))=\\begin{cases} 1, & {Y \\neq f(X)} \\\\ 0, & {Y = f(X)} \\end{cases}$$ \n",
    "\n",
    "平方损失函数\n",
    "$$L(Y, f(X))= (Y - f(X))^2$$ \n",
    "\n",
    "绝对损失函数\n",
    "$$L(Y, f(X))= |Y - f(X)|$$\n",
    "\n",
    "对数损失函数\n",
    "$$L(Y, f(X))= -logP(Y|X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3请结合公式进行说明结构风险最小化的含义？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结构风险最小化（structural risk minimization）是为了防止过拟合而提出的策略。结构风险最小化等价于正则化。\n",
    "\n",
    "结构风险在经验风险上加上表示模型复杂程度的正则化项。\n",
    "\n",
    "结构风险的定义公式如下\n",
    "\n",
    "$$R_{srm}(f)=[\\frac{1}{N} \\sum_{i=1}^N {L(y_i, f(x_i))} ]+ \\lambda J(f) $$\n",
    "\n",
    "其中$J(f)$为模型的复杂度，可以是参数向量的$L_p$范数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.请解释范数，并写出L1,L2范数的定义公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在线性代数、泛函分析等数学领域中，范数是一种函数，它为向量空间中的每个向量赋予了严格为正的长度或大小（零向量除外，其被指定为零长度）。\n",
    "\n",
    "L1范数\n",
    "$$\\|\\boldsymbol{x}\\|_1 = \\sum_i{|x_i|}$$\n",
    "\n",
    "L2范数\n",
    "\n",
    "$$\n",
    "\\|\\boldsymbol{x}\\|_{2}=\\sqrt{\\sum_{i} x_{i}^{2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.请写出软件支持向量机原始优化问题，并解释其中松弛变量$\\xi_i$在不同取值时相应的样本分布情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "软间隔支持向量机的原始优化问题\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\min _{w, b, \\xi} & {\\frac{1}{2}\\|w\\|^{2}+C \\sum_{i=1}^{N} \\xi_{i}} \n",
    "\\\\ {\\text { s.t. }} & {y_{i}\\left(w \\cdot x_{i}+b\\right) \\geqslant 1-\\xi_{i}, \\quad i=1,2, \\cdots, N} \n",
    "\\\\ {} & {\\xi_{i} \\geqslant 0, \\quad i=1,2, \\cdots, N}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "松弛变量$\\xi_i=0$时，支持向量$x_i$恰好落在间隔边界上；\n",
    "\n",
    "松弛变量$0<\\xi_i<1$时，分类正确，支持向量$x_i$在间隔边界和分离超平面之间；\n",
    "\n",
    "松弛变量$\\xi_i=1$时，$x_i$在分离超平面上；\n",
    "\n",
    "松弛变量$\\xi_i>1$时，$x_i$在分离超平面分错的半空间内；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 什么是信息熵，它的公式是？什么是信息增益?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "信息熵表示随机变量不确定性的度量。设$X$是一个离散随机变量，其概率分布为\n",
    "$$\n",
    "P(X=X_i)=p_i, \\quad i=1,2,\\cdots,n\n",
    "$$\n",
    "那么随机变量$X$的熵定义为\n",
    "$$H(X)=-\\sum_{i=1}^{n}p_i \\log p_i$$\n",
    "\n",
    "信息增益表示得知特征$X$的信息使得类$Y$的信息的不确定性减少的程度。\n",
    "\n",
    "特征$A$对训练数据集$D$的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差。\n",
    "$$g(D,A) = H(D)-H(D|A)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. 贝叶斯公式是什么？您如何理解贝叶斯思想？它与频率派有哪些区别？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1). 贝叶斯公式\n",
    "\n",
    "$$\n",
    "P(Y=y_i|X=x)=\\frac{P(X=x,Y=y_i)}{P(X=x)} = \\frac{P(X=x|Y=y_i)P(Y=y_i)}{\\sum_i{P(X=x|Y=y_i)P(Y=y_i)}}\n",
    "$$\n",
    "\n",
    "(2). 贝叶斯定理和人脑的工作机制很像，我们首先对事件有一个经验判断，比如事件$A$发生的概率为$P(A)$，称之为先验概率。\n",
    "\n",
    "当事件$B$发生后，我们对事件$A$发生的概率进行修正，得到一个后验概率。\n",
    "\n",
    "即 $$后验概率＝先验概率 \\times 调整因子$$\n",
    "$$\n",
    "P(A|B)=P(A)\\frac{P(B|A)}{P(B)}\n",
    "$$\n",
    "\n",
    "(3). \n",
    "一、概率是否客观存在\n",
    "频率学派认为概率客观存在，不能主观先验地给出。频率学派从“自然”的角度出发，试图直接为“事件”本身建模。频率学派认为事件$A$在独立重复试验中发生的频率$f$趋于极限$p$，这个极限就是该事件的概率。频率学派的代表人物是Fisher和Pearson，极大似然估计、置信区间和假设检验等是频率学派的重要成果。\n",
    "\n",
    "贝叶斯学派并不试图刻画“事件”本身，而从“观察者知识不完备”这一出发点开始，构造一套在贝叶斯概率论的框架下可以对不确定知识做出推断的方法。贝叶斯派首先对事件$A$的概率进行先验估计，然后根据后续发生的事件不断调整事件$A$的概率，最终得到事件$A$的后验概率。由于先验概率极大地影响后验概率，已经计算量巨大，早期很难用贝叶斯进行统计推断。随着计算机技术的提升和机器学习的兴起，贝叶斯学派越来越受到人们的重视。\n",
    "\n",
    "二、随机的是样本空间还是参数空间\n",
    "频率学派，其特征是把需要推断的参数$\\theta$视作固定且未知的常数，而样本$X$是随机的，其着眼点在样本空间，有关的概率计算都是针对$X$的分布。\n",
    "\n",
    "贝叶斯学派，他们把参数$\\theta$视作随机变量，而样本$X$是固定的，其着眼点在参数空间，重视参数$\\theta$的分布，固定的操作模式是通过参数的先验分布结合样本信息得到参数的后验分布。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.请从最大似然的角度去解释逻辑回归,以及逻辑回归的损失函数是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设 \n",
    "$$\\pi(x) = P(Y=1|x)=\\frac{exp(w \\cdot x)}{1+exp(w \\cdot x)}$$\n",
    "$$1-\\pi(x) = P(Y=0|X)=\\frac{1}{1+exp(w \\cdot x)}$$\n",
    "\n",
    "似然函数为\n",
    "$$\n",
    "\\prod_{i=1}^{N}\\left[\\pi\\left(x_{i}\\right)\\right]^{y_{i}}\\left[1-\\pi\\left(x_{i}\\right)\\right]^{1-y_{i}}\n",
    "$$\n",
    "\n",
    "对数似然函数为\n",
    "$$\n",
    "\\begin{aligned} \n",
    "L(w) &=\\sum_{i=1}^{N}\\left[y_{i} \\log \\pi\\left(x_{i}\\right)+\\left(1-y_{i}\\right) \\log \\left(1-\\pi\\left(x_{i}\\right)\\right)\\right] \n",
    "\\\\ &=\\sum_{i=1}^{N}\\left[y_{i} \\log \\frac{\\pi\\left(x_{i}\\right)}{1-\\pi\\left(x_{i}\\right)}+\\log \\left(1-\\pi\\left(x_{i}\\right)\\right)\\right] \n",
    "\\\\ &=\\sum_{i=1}^{N}\\left[y_{i}\\left(w \\cdot x_{i}\\right)-\\log \\left(1+\\exp \\left(w \\cdot x_{i}\\right)\\right]\\right.\n",
    "\\end{aligned}\n",
    "$$\n",
    "这样，问题就变成了以对数似然函数为目标的最优化问题。\n",
    "\n",
    "逻辑回归的损失函数是对数损失函数,优化的目标函数为\n",
    "$$\\min _{w} -[y_i \\log \\pi(x_i) + (1-y_i) \\log {(1- \\pi(x_i))}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.试析Min-Max与Z-Score这两种数据缩放各自特点，和为什么树形结构不需要做缩放？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-Max的公式为\n",
    "$$\n",
    "x^{\\prime}=\\frac{x-x_{\\min }}{x_{\\max }-x_{\\min }}\n",
    "$$\n",
    "\n",
    "这种归一化方法比较适用在数值比较集中的情况。这种方法有个缺点，如果max和min不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定。实际使用中可以用经验常量值来替代max和min。\n",
    "\n",
    "\n",
    "z-score的公式\n",
    "$$\n",
    "x^{\\prime}=\\frac{x-\\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "$\\mu$为所有样本数据的均值，$\\sigma$为所有样本数据的标准差。经过处理的数据符合标准正态分布，即均值为0，标准差为1\n",
    "\n",
    "\n",
    "\n",
    "树形模型是寻找最优的切分点，数据缩放不影响最终切分点的选取。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. 简述映射函数、核函数以及核方法的作用，列出常用核函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设$\\mathcal{X}$是输入空间（欧氏空间$\\mathbf{R}^n$的子集或者离散集合），又设$\\mathcal{H}$为特征空间（希尔伯特空间）,如果存在一个从$\\mathcal{X}$到$\\mathcal{H}$的映射\n",
    "$$\n",
    "\\phi(x) : \\mathcal{X} \\rightarrow \\mathcal{H}\n",
    "$$\n",
    "使得对所有的$x,z \\in \\mathcal{X}$，函数$K(x,z)$满足条件\n",
    "$$\n",
    "K(x,z) = \\phi(x) \\cdot \\phi(z)\n",
    "$$\n",
    "则称$K(x,z)$为核函数，$\\phi(x)$为映射函数。\n",
    "\n",
    "通过映射函数$\\phi(\\cdot)$，我们将原来的输入空间变换到一个新的特征空间，当映射函数是非线性函数时，学习到的含有核函数的支持向量机是非线性分类模型。上述方法就是核技巧（核方法）。\n",
    "\n",
    "常用核函数\n",
    "\n",
    "1. 多项式核函数\n",
    "$$\n",
    "K(x, z)=(x \\cdot z+1)^{p}\n",
    "$$\n",
    "\n",
    "2. 高斯核函数\n",
    "\n",
    "$$\n",
    "K(x, z)=\\exp \\left(-\\frac{\\|x-z\\|^{2}}{2 \\sigma^{2}}\\right)\n",
    "$$\n",
    "\n",
    "3. 字符串核函数\n",
    "\n",
    "$$\n",
    "\\left[\\phi_{n}(s)\\right]_{u}=\\sum_{i:s(i)=u} \\lambda^{l(i)}\n",
    "$$\n",
    "这里$0<\\lambda \\leqslant 1$是一个衰减参数，$l(t)$表示字符串$i$的长度，求和在$s$中所有与$u$相同的子串上进行。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
