{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5_决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树是基于树结构来对实例进行决策的一种基本的分类和回归的机器学习方法。决策树由结点和有向边组成，结点分为内部结点（表示一个特征的划分）和叶子结点（表示一个类别或输出）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树学习，训练数据集\n",
    "\\begin{align*} \\\\& D = \\left\\{ \\left( \\mathbf{x}_{1}, y_{1} \\right), \\left( \\mathbf{x}_{2}, y_{2} \\right), \\cdots, \\left(\\mathbf{x}_i,y_i\\right),\\cdots,\\left( \\mathbf{x}_{N}, y_{N} \\right) \\right\\} \\end{align*}   \n",
    "其中，$\\mathbf{x}_{i}$为第$i$个特征向量（实例），$\\mathbf{x}_{i}=\\left( x^{\\left(1\\right)}_i,x^{\\left(2\\right)}_i,\\ldots ,x^{\\left(j\\right)}_i,\\ldots ,x^{\\left(n\\right)}_i\\right) ^{T} \\in \\mathcal{X} \\subseteq \\mathbb{R}^{n}$；$y_{i}$为$\\mathbf{x}_{i}$的类别标记，$y_i\\in\\{1,2,\\cdots,K\\}$。学习的目标数是根据给定的训练数据集构建一个决策树模型，使得可对实例进行正确的分类或回归。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树学习包括3个步骤：特征选择、决策树生成、决策树修剪。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5_1_特征选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 特征选择在于选取对于训练数据具有分类能力的特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "熵表示随机变量不确定性的度量。  \n",
    "设$X$是一个取有限个值的离散随机变量，其概率分布为\n",
    "\\begin{align*} P \\left( X = x_{i} \\right) = p_{i}, \\quad i =1, 2, \\cdots, n \\end{align*}  \n",
    "则随机变量$X$的熵\n",
    "\\begin{align*} H \\left( X \\right) = H \\left( p \\right) = - \\sum_{i=1}^{n} p_{i} \\log p_{i} \\end{align*}   \n",
    "其中，若$p_{i}=0$，则定义$0 \\log 0 = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若\n",
    "\\begin{align*} p_{i} = \\dfrac{1}{n} \\end{align*}   \n",
    "则\n",
    "\\begin{align*} \\\\ & H \\left( p \\right) = - \\sum_{i=1}^{n} p_{i} \\log p_{i} \n",
    "\\\\ & = - \\sum_{i=1}^{n} \\dfrac{1}{n} \\log \\dfrac{1}{n}\n",
    "\\\\ & = \\log n\\end{align*} \n",
    "由定义，得\n",
    "\\begin{align*} \\\\ & 0 \\leq H \\left( p \\right) \\leq \\log n\\end{align*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设有随机变量$\\left( X , Y \\right)$，其联合分布\n",
    "\\begin{align*} \\\\ & P \\left( X = x_{i}, Y = y_{j} \\right) = p_{ij}, \\quad i=1,2, \\cdots, n; \\quad j=1,2, \\cdots, m\\end{align*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机变量$X$给定的条件下随机变量$Y$的条件熵\n",
    "\\begin{align*} \\\\ & H \\left( Y | X \\right) = \\sum_{i=1}^{n} p_{i} H \\left( Y | X = x_{i} \\right) \\end{align*}  \n",
    "即，$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望。其中，$p_{i}=P \\left( X = x_{i} \\right), i= 1,2,\\cdots,n$。  \n",
    "条件熵$H \\left( Y | X \\right)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征$A$对训练集$D$的信息增益\n",
    "\\begin{align*} \\\\ & g \\left( D, A \\right) = H \\left( D \\right) - H \\left( D | A \\right) \\end{align*}   \n",
    "即，集合$D$的经验熵$H \\left( D \\right)$与特征$A$给定条件下$D$的经验条件熵$H \\left( D | A \\right)$之差。  \n",
    "其中，当熵和条件熵由数据估计（极大似然估计）得到时，对应的熵和条件熵分别称为经验熵和经验条件熵。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设训练数据集为$D$，$\\left| D \\right|$表示其样本容量，即样本个数。  \n",
    "设有$K$个类$C_{k}, k=1,2,\\cdots,K$，$\\left| C_{k} \\right|$为属于类$C_{k}$的样本的个数，$\\sum_{k=1}^{K} \\left| C_{k} \\right| = \\left| D \\right|$。  \n",
    "设特征$A$有$n$个不同的特征取值$\\left\\{ a_{1},a_{2},\\cdots,a_{n}\\right\\}$，根据特征$A$的取值将$D$划分为$n$个子集$D_{1},D_{2},\\cdots,D_{n}$，$\\left| D_{i} \\right|$为$D_{i}$的样本数，$\\sum_{i=1}^{n}\\left| D_{i} \\right| = \\left| D \\right|$。  \n",
    "记子集$D_{i}$中属于类$C_{k}$的样本的集合为$D_{ik}$，即$D_{ik} = D_{i} \\cap C_{k}$，$\\left| D_{ik} \\right|$为$D_{ik}$的样本个数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "信息增益算法：  \n",
    "输入：训练数据集$D$和特征$A$  \n",
    "输出：特征$A$对训练数据集$D$的信息增益$g \\left( D, A \\right) $\n",
    "1. 计算数据集$D$的经验熵$H\\left(D\\right)$  \n",
    "\\begin{align*} \\\\ &  H \\left( D \\right) = -\\sum_{k=1}^{K} \\dfrac{\\left|C_{k}\\right|}{\\left| D \\right|}\\log_{2}\\dfrac{\\left|C_{k}\\right|}{\\left| D \\right|} \\end{align*}\n",
    "2. 计算特征$A$对数据集$D$的经验条件熵$H \\left( D | A \\right)$\n",
    "\\begin{align*} \\\\ & H \\left( D | A \\right) = \\sum_{i=1}^{n} \\dfrac{\\left| D_{i} \\right|}{\\left| D \\right|} H \\left( D_{i} \\right)\n",
    "\\\\ & = \\sum_{i=1}^{n} \\dfrac{\\left| D_{i} \\right|}{\\left| D \\right|} \\sum_{k=1}^{K} \\dfrac{\\left| D_{ik} \\right|}{\\left| D_{i} \\right|} \\log_{2} \\dfrac{\\left| D_{ik} \\right|}{\\left| D_{i} \\right|}\\end{align*}\n",
    "3. 计算信息增益\n",
    "\\begin{align*} \\\\ & g \\left( D, A \\right) = H \\left( D \\right) - H \\left( D | A \\right) \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征$A$对训练集$D$的信息增益比\n",
    "\\begin{align*} \\\\ & g_{R} \\left( D, A \\right) = \\dfrac{g \\left( D, A \\right)}{H_{A} \\left(D\\right)}\\end{align*}   \n",
    "即，信息增益$g\\left( D, A \\right)$与训练数据集$D$关于特征$A$的经验熵$H_{A}\\left(D\\right)$之比。  \n",
    "其中，\n",
    "\\begin{align*} \\\\ & H_{A} \\left( D \\right) = -\\sum_{i=1}^{n} \\dfrac{\\left|D_{i}\\right|}{\\left|D\\right|}\\log_{2}\\dfrac{\\left|D_{i}\\right|}{\\left|D\\right|}\\end{align*}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5_2_决策树生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ID3算法：  \n",
    "输入：训练数据集$D$，特征集合$A$，阈值$\\varepsilon$  \n",
    "输出：决策树$T$\n",
    "1. 若$D$中所有实例属于同一类$C_{k}$，则$T$为单结点树，并将类$C_{k}$作为该结点的类标记，返回$T$； \n",
    "2. 若$A = \\emptyset$，则$T$为单结点树，并将$D$中实例数最大的类$C_{k}$作为该结点的类标记，返回$T$；\n",
    "3. 否则，计算$A$中各特征对$D$的信息增益，选择信息增益最大的特征$A_{g}$\n",
    "\\begin{align*} \\\\ & A_{g} = \\arg \\max_{A} g \\left( D, A \\right) \\end{align*}  \n",
    "4. 如果$A_{g}$的信息增益小于阈值$\\varepsilon$，则置$T$为单结点树，并将$D$中实例数量最大的类$C_{k}$作为该结点的类标记，返回$T$;\n",
    "5. 否则，对$A_{g}$的每一个可能值$a_{i}$，依$A_{g}=a_{i}$将$D$分割为若干非空子集$D_{i}$，将$D_{i}$中实例数对大的类作为标记，构建子结点，由结点及其子结点构成树$T$，返回$T$；\n",
    "6. 对第$i$个子结点，以$D_{i}$为训练集，以$A-\\left\\{A_{g}\\right\\}$为特征集，递归地调用步1.～步5.，得到子树$T_{i}$，返回$T_{i}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "C4.5算法：  \n",
    "输入：训练数据集$D$，特征集合$A$，阈值$\\varepsilon$  \n",
    "输出：决策树$T$\n",
    "1. 若$D$中所有实例属于同一类$C_{k}$，则$T$为单结点树，并将类$C_{k}$作为该结点的类标记，返回$T$； \n",
    "2. 若$A = \\emptyset$，则$T$为单结点树，并将$D$中实例数最大的类$C_{k}$作为该结点的类标记，返回$T$；\n",
    "3. 否则，计算$A$中各特征对$D$的信息增益，选择信息增益比最大的特征$A_{g}$\n",
    "\\begin{align*} \\\\ & A_{g} = \\arg \\max_{A} g_{R} \\left( D, A \\right) \\end{align*}  \n",
    "4. 如果$A_{g}$的信息增益小于阈值$\\varepsilon$，则置$T$为单结点树，并将$D$中实例数量最大的类$C_{k}$作为该结点的类标记，返回$T$;\n",
    "5. 否则，对$A_{g}$的每一个可能值$a_{i}$，依$A_{g}=a_{i}$将$D$分割为若干非空子集$D_{i}$，将$D_{i}$中实例数对大的类作为标记，构建子结点，由结点及其子结点构成树$T$，返回$T$；\n",
    "6. 对第$i$个子结点，以$D_{i}$为训练集，以$A-\\left\\{A_{g}\\right\\}$为特征集，递归地调用步1.～步5.，得到子树$T_{i}$，返回$T_{i}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5_3_决策树剪枝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树的剪枝通过极小化决策树整体的损失函数或代价函数来实现。  \n",
    "设树$T$的叶结点个数为$\\left| T \\right|$，$t$是树$T$的叶结点，该叶结点有$N_{t}$个样本点，其中$k$类的样本点有$N_{tk}$个，$k=1,2,\\cdots,K$，$H_{t}\\left(T\\right)$为叶结点$t$上的经验熵，  \n",
    "则决策树的损失函数\n",
    "\\begin{align*} \\\\ & C_{\\alpha} \\left( T \\right) = \\sum_{t=1}^{\\left| T \\right|} N_{t} H_{t} \\left( T \\right) + \\alpha \\left| T \\right| \\end{align*}   \n",
    "其中，$\\alpha \\geq 0$为参数，经验熵\n",
    "\\begin{align*} \\\\ & H_{t} \\left( T \\right) = - \\sum_{k} \\dfrac{N_{tk}}{N_{t}} \\log \\dfrac{N_{tk}}{N_{t}} \\end{align*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数中，记\n",
    "\\begin{align*} \\\\ & C \\left( T \\right) = \\sum_{t=1}^{\\left| T \\right|} N_{t} H_{t} \\left( T \\right) = - \\sum_{t=1}^{\\left| T \\right|} \\sum_{k=1}^{K} N_{tk} \\log \\dfrac{N_{tk}}{N_{t}}   \\end{align*}   \n",
    "则\n",
    "\\begin{align*} \\\\ & C_{\\alpha} \\left( T \\right) = C \\left( T \\right) + \\alpha \\left| T \\right|   \\end{align*}  \n",
    "其中，$C \\left( T \\right)$表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，$\\left| T \\right|$表示模型复杂度，参数$\\alpha \\geq 0$控制两者之间的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "树的剪枝算法：  \n",
    "输入：决策树$T$，参数$\\alpha$   \n",
    "输出：修剪后的子树$T_{\\alpha}$\n",
    "1. 计算每个结点的经验熵 \n",
    "2. 递归地从树的叶结点向上回缩  \n",
    "设一组叶结点回缩到其父结点之前与之后的整体树分别为$T_{B}$与$T_{A}$，其对应的损失函数值分别是$C_{\\alpha} \\left( T_{B} \\right)$与$C_{\\alpha} \\left( T_{A} \\right)$，如果\n",
    "\\begin{align*} \\\\ & C_{\\alpha} \\left( T_{A} \\right) \\leq C_{\\alpha} \\left( T_{B} \\right)  \\end{align*}\n",
    "则进行剪枝，即将父结点变为新的叶结点。\n",
    "3. 返回2.，直到不能继续为止，得到损失函数最小的子树$T_{\\alpha}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5_4_分类与回归树CART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5_4_1_回归树的生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "假设$X$与$Y$分别为输入和输出变量，并且$Y$是连续变量，给定训练数据集\n",
    "\\begin{align*} \\\\ & D = \\left\\{  \\left(\\mathbf{x}_{1},y_{1}\\right), \\left(\\mathbf{x}_{2},y_{2}\\right),\\cdots,\\left(\\mathbf{x}_{N},y_{N}\\right) \\right\\}   \\end{align*}    \n",
    "可选择第$j$个变量$x^{\\left(j\\right)}$及其取值$s$作为切分变量和切分点，并定义两个区域\n",
    "\\begin{align*} \\\\ & R_{1} \\left( j,s \\right) = \\left\\{ x | x^{\\left(j\\right)} \\leq s \\right\\}, \\quad R_{2} \\left( j,s \\right) = \\left\\{ x | x^{\\left(j\\right)} > s \\right\\}   \\end{align*}\n",
    "最优切分变量$x_{j}$及最优切分点$s$\n",
    "\\begin{align*} \\\\ & j,s = \\arg \\min_{j,s} \\left[ \\min_{c_{1}} \\sum_{x_{i} \\in R_{1} \\left(j,s\\right)} \\left( y_{i} - c_{1} \\right)^{2} + \\min_{c_{2}} \\sum_{x_{i} \\in R_{2} \\left(j,s\\right)} \\left( y_{i} - c_{2} \\right)^{2}\\right]   \\end{align*}  \n",
    "其中，$c_{m}$是区域$R_{m}$上的回归决策树输出，是区域$R_{m}$上所有输入实例$x_{i}$对应的输出$y_{i}$的均值\n",
    "\\begin{align*} \\\\ & c_{m} = ave \\left( y_{i} | x_{i} \\in R_{m} \\right), \\quad m=1,2   \\end{align*}  \n",
    "对每个区域$R_{1}$和$R_{2}$重复上述过程，将输入空间划分为$M$个区域$R_{1},R_{2},\\cdots,R_{M}$，在每个区域上的输出为$c_{m},m=1,2,\\cdots,M$，最小二乘回归树\n",
    "\\begin{align*} \\\\ & f \\left( x \\right) = \\sum_{m=1}^{M} c_{m} I \\left( x \\in R_{m} \\right)   \\end{align*}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最小二乘回归树生成算法：  \n",
    "输入：训练数据集$D$   \n",
    "输出：回归树$f \\left( x \\right)$\n",
    "1. 选择最优切分变量$x^{\\left(j\\right)}$与切分点$s$\n",
    "\\begin{align*} \\\\ & j,s = \\arg \\min_{j,s} \\left[ \\min_{c_{1}} \\sum_{x_{i} \\in R_{1} \\left(j,s\\right)} \\left( y_{i} - c_{1} \\right)^{2} + \\min_{c_{2}} \\sum_{x_{i} \\in R_{2} \\left(j,s\\right)} \\left( y_{i} - c_{2} \\right)^{2}\\right]   \\end{align*}  \n",
    "2. 用最优切分变量$x^{\\left(j\\right)}$与切分点$s$划分区域并决定相应的输出值 \n",
    "\\begin{align*} \\\\ & R_{1} \\left( j,s \\right) = \\left\\{ x | x^{\\left(j\\right)} \\leq s \\right\\}, \\quad R_{2} \\left( j,s \\right) = \\left\\{ x | x^{\\left(j\\right)} > s \\right\\}   \n",
    "\\\\ & c_{m} = \\dfrac{1}{N} \\sum_{x_{i} \\in R_{m} \\left( j,s \\right)} y_{i}, \\quad m=1,2\\end{align*}\n",
    "3. 继续对两个子区域调用步骤1.和2.，直到满足停止条件\n",
    "4. 将输入空间划分为$M$个区域$R_{1},R_{2},\\cdots,R_{M}$，生成决策树\n",
    "\\begin{align*} \\\\ & f \\left( x \\right) = \\sum_{m=1}^{M} c_{m} I \\left( x \\in R_{m} \\right)   \\end{align*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5_4_2_分类树的生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类问题中，假设有$K$个类，样本点属于第$k$类的概率为$p_{k}$，则概率分布的基尼指数\n",
    "\\begin{align*} \\\\ & Gini \\left( p \\right) = \\sum_{k=1}^{K} p_{k} \\left( 1 - p_{k} \\right) = 1 - \\sum_{k=1}^{K}  \\end{align*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于二分类问题，若样本点属于第1类的概率为$p$，则概率分布的基尼指数\n",
    "\\begin{align*} \\\\ & Gini \\left( p \\right) = \\sum_{k=1}^{2} p_{k} \\left( 1 - p_{k} \\right) = 2p\\left(1-p\\right) \\end{align*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于给定样本集和$D$，其基尼指数\n",
    "\\begin{align*} \\\\ & Gini \\left( D \\right) = 1 - \\sum_{k=1}^{K} \\left( \\dfrac{\\left| C_{k} \\right|}{\\left| D \\right|} \\right)^{2}\\end{align*}   \n",
    "其中，$C_{k}$是$D$中属于第$k$类的样本自己，$K$是类别个数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果样本集合$D$根据特征$A$是否取某一可能值$a$被分割成$D_{1}$和$D_{2}$两个部分，即\n",
    "\\begin{align*} \\\\ & D_{1} = \\left\\{ \\left(x,y\\right) | A\\left(x\\right)=a \\right\\}, \\quad D_{2} = D - D_{1} \\end{align*} \n",
    "则在特征$A$的条件下，集合$D$的基尼指数\n",
    "\\begin{align*} \\\\ & Gini \\left( D, A \\right) = \\dfrac{\\left| D_{1} \\right|}{\\left| D \\right|} Gini \\left( D_{1} \\right) + \\dfrac{\\left| D_{2} \\right|}{\\left| D \\right|} Gini \\left( D_{2} \\right)\\end{align*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基尼指数$Gini \\left( D \\right)$表示集合$D$的不确定性，基尼指数$Gini \\left( D,A \\right)$表示经$A=a$分割后集合$D$的不确定性。基尼指数值越大，样本集合的不确定性也越大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART生成算法：  \n",
    "输入：训练数据集$D$，特征$A$，阈值$\\varepsilon$  \n",
    "输出：CART决策树$T$\n",
    "1. 设结点的训练数据集为$D$，对每一个特征$A$，对其可能取的每个值$a$，根据样本点对$A=a$的测试为“是”或“否”将$D$分割成$D_{1}$和$D_{2}$两部分，并计算$Gini\\left(D,A\\right)$\n",
    "2. 在所有可能的特征$A$以及其所有可能的切分点$a$中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。依此从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。\n",
    "3. 对两个子结点递归地调用1.和2.，直至满足停止条件\n",
    "4. 生成CART决策树$T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5_4_3_CART树剪枝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "对整体树$T_{0}$任意内部结点$t$，以$t$为单结点树的损失函数\n",
    "\\begin{align*} \\\\ & C_{\\alpha} \\left( t \\right) = C \\left( t \\right) + \\alpha \\end{align*} \n",
    "以$t$为根结点的子树$T_{t}$的损失函数\n",
    "\\begin{align*} \\\\ & C_{\\alpha} \\left( T_{t} \\right) = C \\left( T_{t} \\right) + \\alpha \\left| T_{t} \\right| \\end{align*} \n",
    "当$\\alpha = 0$及$\\alpha$充分小时，有不等式\n",
    "\\begin{align*} \\\\ & C_{\\alpha} \\left( T_{t} \\right) <  C_{\\alpha} \\left( t \\right) \\end{align*} \n",
    "当$\\alpha$增大时，在某一$\\alpha$有\n",
    "\\begin{align*} \\\\ & \\quad\\quad C_{\\alpha} \\left( T_{t} \\right)  ＝  C_{\\alpha} \\left( t \\right) \n",
    "\\\\ & C \\left( T_{t} \\right) + \\alpha \\left| T_{t} \\right| ＝ C \\left( t \\right) + \\alpha\n",
    "\\\\ & \\quad\\quad \\alpha = \\dfrac{C\\left( t \\right) - C \\left(T_{t}\\right)} { \\left| T_{t} \\right| -1 }\\end{align*} \n",
    "即$T_{t}$与$t$有相同的损失函数值，而$t$的结点少，因此对$T_{t}$进行剪枝。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART剪枝算法：  \n",
    "输入：CART决策树$T_{0}$   \n",
    "输出：最优决策树$T_{\\alpha}$\n",
    "1. 设$k=0, T=T_{0}$\n",
    "2. 设$\\alpha=+\\infty$\n",
    "3. 自下而上地对各内部结点$t$计算$ C\\left(T_{t}\\right),\\left| T_{t} \\right|$，以及\n",
    "\\begin{align*} \\\\ & g\\left(t\\right) =  \\dfrac{C\\left( t \\right) - C \\left(T_{t}\\right)} { \\left| T_{t} \\right| -1 }\n",
    "\\\\ & \\alpha = \\min \\left( \\alpha, g\\left( t \\right) \\right) \\end{align*} \n",
    "其中，$T_{t}$表示以$t$为根结点的子树，$ C\\left(T_{t}\\right)$是对训练数据的预测误差，$\\left| T_{t} \\right|$是$T_{t}$的叶结点个数。\n",
    "4. 自下而上地访问内部结点$t$，如果有$g\\left(t\\right)=\\alpha$，则进行剪枝，并对叶结点$t$以多数表决法决定其类别，得到树$T$\n",
    "5. 设$k=k+1, \\alpha_{k}=\\alpha, T_{k}=T$\n",
    "6. 如果$T$不是由根结点单独构成的树，则回到步骤4.\n",
    "7. 采用交叉验证法在子树序列$T_{0},T_{1},\\cdots,T_{n}$中选取最优子树$T_{\\alpha}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
