{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2_广义线性模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2_2_逻辑斯谛回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2_2_1_sigmoid函数与二项逻辑斯谛回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sigmoid函数：\n",
    "$$sigmoid\\left(z\\right)=\\sigma\\left(z\\right)=\\frac{1}{1+e^{-z}}$$\n",
    "其中，$z\\in\\mathbb{R}$，$sigoid\\left(z\\right)\\in\\left(0,1\\right)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sigmoid函数的导数：\n",
    "$$\\sigma'\\left(z\\right)=\\sigma\\left(z\\right)\\left(1-\\sigma\\left(z\\right)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "二项逻辑斯谛回归模型是如下的条件概率分布：\n",
    "$$\\begin{align*}   P \\left( y = 1 | \\mathbf{x} \\right) &=\\sigma\\left(\\mathbf{w}\\cdot\\mathbf{x}+b\\right)   \\\\ \n",
    "&=  \\dfrac{1}{1+\\exp{\\left(-\\left(\\mathbf{w} \\cdot \\mathbf{x} + b \\right)\\right)}}\n",
    "\\\\ &= \\dfrac{\\exp{\\left(\\mathbf{w} \\cdot \\mathbf{x} + b \\right)}}{1+\\exp{\\left( \\mathbf{w} \\cdot \\mathbf{x} + b \\right)}}\\\\\n",
    "P \\left( y = 0 | \\mathbf{x} \\right) &=  1- \\sigma\\left(\\mathbf{w}\\cdot\\mathbf{x}+b\\right)\n",
    "\\\\ &=\\dfrac{1}{1+\\exp{\\left( \\mathbf{w} \\cdot \\mathbf{x} + b \\right)}}\\end{align*}$$\n",
    "其中，$\\mathbf{x} \\in \\mathbb{R}^{n}$，$y \\in \\left\\{ 0, 1 \\right\\}$，$\\mathbf{w} \\in \\mathbb{R}^{n}$是权值向量，$b \\in \\mathbb{R}$是偏置，$\\mathbf{w} \\cdot \\mathbf{x}$为向量内积。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "可将权值权值向量和特征向量加以扩充，即增广权值向量$\\hat{\\mathbf{w}} = \\left( w^{\\left(1\\right)},w^{\\left(2\\right)},\\cdots,w^{\\left(n\\right)},b \\right)^\\top$，增广特征向量$\\hat{\\mathbf{x}} = \\left( x^{\\left(1\\right)},x^{\\left(2\\right)},\\cdots,x^{\\left(n\\right)},1 \\right)^\\top$，则逻辑斯谛回归模型：\n",
    "\\begin{align*} \\\\&  P \\left( y = 1 | \\hat{\\mathbf{x}} \\right) = \\dfrac{\\exp{\\left(\\hat{\\mathbf{w}} \\cdot \\hat{\\mathbf{x}} \\right)}}{1+\\exp{\\left( \\hat{\\mathbf{w}} \\cdot \\hat{\\mathbf{x}}  \\right)}}\\\\&  P \\left( y = 0 | \\hat{\\mathbf{x}} \\right) =\\dfrac{1}{1+\\exp{\\left( \\hat{\\mathbf{w}} \\cdot \\hat{\\mathbf{x}}  \\right)}}\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2_2_2_二项逻辑斯谛回归参数学习——最大似然估计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定训练数据集\n",
    "\\begin{align*} \\\\& D = \\left\\{ \\left( \\hat{\\mathbf{x}}_{1}, y_{1} \\right), \\left( \\hat{\\mathbf{x}}_{2}, y_{2} \\right), \\cdots, \\left( \\hat{\\mathbf{x}}_{N}, y_{N} \\right) \\right\\} \\end{align*}   \n",
    "其中，$\\hat{\\mathbf{x}}_{i} \\in \\mathbb{R}^{n+1}, y_{i} \\in \\left\\{ 0, 1 \\right\\}, i = 1, 2, \\cdots, N$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设\n",
    "$$\\begin{align*} \\\\& P \\left( y =1 | \\hat{\\mathbf{x}} \\right)  =\\sigma \\left( \\hat{\\mathbf{w}}\\cdot\\hat{\\mathbf{x}} \\right) ,\\quad P \\left( y =0 | \\hat{\\mathbf{x}} \\right) = 1 - \\sigma \\left( \\hat{\\mathbf{w}}\\cdot\\hat{\\mathbf{x}} \\right)  \\end{align*}   $$\n",
    "似然函数\n",
    "$$\n",
    "\\begin{align*}  L \\left( \\hat{\\mathbf{w}} \\right)  &= \\prod_{i=1}^N P\\left(y_i|\\hat{\\mathbf{x}}_i\\right)  \\\\\n",
    "&= \\prod_{i=1}^{N} \\left[ \\sigma \\left( \\hat{\\mathbf{w}}\\cdot\\hat{\\mathbf{x}}_{i} \\right) \\right]^{y_{i}}\\left[ 1 - \\sigma \\left( \\hat{\\mathbf{w}}\\cdot\\hat{\\mathbf{x}}_{i} \\right) \\right]^{1 - y_{i}}\\end{align*} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对数似然函数\n",
    "$$\\begin{align*} \\\\ l \\left( \\hat{\\mathbf{w}} \\right) &= \\log L \\left( \\hat{\\mathbf{w}} \\right) \n",
    "\\\\ & = \\sum_{i=1}^{N} \\left[ y_{i} \\log \\sigma \\left( \\hat{\\mathbf{w}}\\cdot\\hat{\\mathbf{x}}_{i} \\right) + \\left( 1 - y_{i} \\right) \\log \\left( 1 - \\sigma \\left( \\hat{\\mathbf{w}}\\cdot\\hat{\\mathbf{x}}_{i} \\right) \\right) \\right]\\end{align*} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最大似然估计\n",
    "$$\\hat{\\mathbf{w}}^*=\\mathop{\\arg\\max}_{\\hat{\\mathbf{w}}} l\\left(\\hat{\\mathbf{w}}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "令$\\hat{y}_i=\\sigma\\left(\\hat{\\mathbf{w}}\\cdot\\hat{\\mathbf{x}}_i\\right)$，则对数似然函数$l\\left(\\hat{\\mathbf{w}}\\right)$关于$\\hat{\\mathbf{w}}$的偏导数\n",
    "$$\\begin{align*}\\frac{\\partial l\\left(\\hat{\\mathbf{w}}\\right)}{\\partial \\hat{\\mathbf{w}}} &=\\sum_{i=1}^N\\left(y_i\\frac{\\hat{y}_i\\left(1-\\hat{y}_i\\right)}{\\hat{y}_i}\\hat{\\mathbf{x}}_i+\\left(1-y_i\\right)\\frac{\\hat{y}_i\\left(1-\\hat{y}_i\\right)}{1-\\hat{y}_i}\\hat{\\mathbf{x}}_i\\right)\\\\\n",
    "&=\\sum_{i=1}^N\\left(y_i\\left(1-\\hat{y}_i\\right)\\hat{\\mathbf{x}}_i+\\left(1-y_i\\right)\\hat{y}_i\\hat{\\mathbf{x}}_i\\right) \\\\\n",
    "&=\\sum_{i=1}^N\\hat{\\mathbf{x}}_i\\left(y_i-\\hat{y}_i\\right)\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "采用梯度上升法，初始化$\\hat{\\mathbf{w}}=\\mathbf{0}$，进行迭代\n",
    "$$\\hat{\\mathbf{w}}_{t+1}\\gets\\hat{\\mathbf{w}}_t+\\alpha\\sum_{i=1}^N\\hat{\\mathbf{x}}_i\\left(y_i-\\hat{y}_i^{\\hat{\\mathbf{w}}_t}\\right)$$\n",
    "其中，$\\alpha$是学习率，$\\hat{y}_i^{\\hat{\\mathbf{w}}_t}$是当参数$\\hat{\\mathbf{w}}_t$时模型的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2_2_3_softmax函数与多项逻辑斯谛回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于$K$个标量$x_1,x_2,\\dots,x_K$,softmax函数：\n",
    "$$z_k=softmax\\left(x_k\\right)=\\frac{\\exp\\left(x_k\\right)}{\\sum_{i=1}^K\\exp\\left(x_i\\right)}$$\n",
    "其中，$z_k\\in\\left[0,1\\right]$，且$\\forall k,\\sum_{i=1}^K z_k=1$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于$K$维向量$\\mathbf{x}=\\left[x_1,x_2,\\dots,x_K\\right]$，softmax函数：\n",
    "$$\\mathbf{z}=softmax\\left(\\mathbf{x}\\right)=\\frac{\\exp\\left(\\mathbf{x}\\right)}{\\sum_{i=1}^K\\exp\\left(x_i\\right)}=\\frac{\\exp\\left(\\mathbf{x}\\right)}{\\mathbf{1}^\\top_K\\exp\\left(\\mathbf{x}\\right)}$$\n",
    "其中，$\\mathbf{1}_K=\\left[1,\\dots,1\\right]_{K\\times 1}$是$K$维的全1向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$K$维向量$\\mathbf{x}$的softmax函数的导数：\n",
    "$$\\frac{\\partial softmax\\left(\\mathbf{x}\\right)}{\\partial\\mathbf{x}}=diag\\left(softmax\\left(\\mathbf{x}\\right)\\right)-softmax\\left(\\mathbf{x}\\right)softmax\\left(\\mathbf{x}\\right)\\top$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "多项逻辑斯谛回归模型是如下的条件概率分布：\n",
    "$$ \\begin{align*} \\\\&  P \\left( y = k | \\hat{\\mathbf{x}} \\right) = \\dfrac{\\exp{\\left(\\hat{\\mathbf{w}}_{k} \\cdot \\hat{\\mathbf{x}} \\right)}}{\\sum_{i=1}^{K}\\exp{\\left( \\hat{\\mathbf{w}}_{i} \\cdot \\hat{\\mathbf{x}}_i  \\right)}} \\end{align*}$$\n",
    "其中，$\\hat{\\mathbf{x}} \\in \\mathbb{R}^{n+1}$是特征增广向量，$y \\in \\left\\{ 1, 2,\\dots,K \\right\\}$，$\\hat{\\mathbf{w}} \\in \\mathbb{R}^{n}$是权值增广向量，$\\hat{\\mathbf{w}} \\cdot \\hat{\\mathbf{x}}$为向量内积。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多项式逻辑回归模型的向量表示\n",
    "$$\\hat{\\mathbf{y}}=softmax\\left(W^\\top\\hat{\\mathbf{x}}\\right)=\\frac{\\exp\\left(W^\\top\\hat{\\mathbf{x}}\\right)}{\\mathbf{1}^\\top_K\\exp\\left(W^\\top\\hat{\\mathbf{x}}\\right)}$$\n",
    "其中，$W=\\left[\\hat{\\mathbf{x}}_1,\\dots,\\hat{\\mathbf{w}}_K\\right]$是由$K$个类别权重向量组成的权重矩阵，$\\mathbf{1}$是全$1$向量，$\\hat{\\mathbf{y}}\\in\\mathbb{R}^K$是所有K个类别的预测输出向量，第$k$维的值是第$k$类别的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2_2_4_多项逻辑斯谛回归参数学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定训练数据集\n",
    "\\begin{align*} \\\\& D = \\left\\{ \\left( \\hat{\\mathbf{x}}_{1}, y_{1} \\right), \\left( \\hat{\\mathbf{x}}_{2}, y_{2} \\right), \\cdots, \\left( \\hat{\\mathbf{x}}_{N}, y_{N} \\right) \\right\\} \\end{align*}   \n",
    "其中，$\\hat{\\mathbf{x}}_{i} \\in \\mathbb{R}^{n+1}, y_{i} \\in \\left\\{ 1, 2，\\dots,K \\right\\}, i = 1, 2, \\cdots, N$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对标签$y$使用$K$维的one-hot向量$\\mathbf{y}\\in\\{0,1\\}^K$表示。对于类别$c$，其向量表示为\n",
    "$$\\mathbf{y}=\\left[I\\left(k=1\\right),I\\left(k=2\\right),\\dots,I\\left(k=K\\right)\\right]\\top$$\n",
    "其中，$I\\left(\\cdot\\right)$是指示函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用交叉熵损失函数，softmax回归模型的风险函数\n",
    "$$\\mathcal{L}\\left(W\\right)=-\\frac{1}{N}\\sum_{i=1}^N\\mathbf{y}_i\\cdot\\log\\hat{\\mathbf{y}}_i=-\\frac{1}{N}\\sum_{i=1}^N\\sum_{k=1}^K\\mathbf{y}_{ik}\\log\\hat{\\mathbf{y}}_{ik}$$\n",
    "其中，$\\mathbf{y}_{ik}$是第$i$个标签one-hot向量表示的第$k$个维度元素值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "风险函数$\\mathcal{L}\\left(W\\right)$关于权值矩阵$W$的梯度为\n",
    "$$\\frac{\\partial\\mathcal{L}\\left(W\\right)}{\\partial W}=-\\frac{1}{N}\\sum_{i=1}^N\\hat{\\mathbf{x}}_i\\left(\\mathbf{y}_i-\\hat{\\mathbf{y}}_i\\right)^\\top$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "采用梯度下降法，初始化$\\mathbf{W}=\\mathbf{0}$，进行迭代\n",
    "$$\\mathbf{W}_{t+1}\\gets\\mathbf{W}_t+\\alpha\\left(\\frac{1}{N}\\sum_{i=1}^N\\hat{\\mathbf{x}}_i\\left(y_i-\\hat{y}_i^{\\mathbf{W}_t}\\right)^\\top\\right)$$\n",
    "其中，$\\alpha$是学习率，$\\hat{y}_i^{\\mathbf{w}_t}$是当参数$\\mathbf{w}_t$时模型的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2_2_5_逻辑斯谛回归模型应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:[[ 0.38705175  1.35839989 -2.12059692 -0.95444452]\n",
      " [ 0.23787852 -1.36235758  0.5982662  -1.26506299]\n",
      " [-1.50915807 -1.29436243  2.14148142  2.29611791]], intercept [ 0.23950369  1.14559506 -1.0941717 ]\n",
      "Score: 0.97\n",
      "Coefficients:[[-0.38353372  0.86198376 -2.26983035 -0.97472053]\n",
      " [ 0.34380687 -0.37905055 -0.03125303 -0.86849329]\n",
      " [ 0.03972686 -0.48293322  2.30108338  1.84321382]], intercept [  8.75854734   2.49443506 -11.2529824 ]\n",
      "Score: 1.00\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn import model_selection\n",
    "\n",
    "def load_data():\n",
    "    iris=datasets.load_iris() \n",
    "    X_train=iris.data\n",
    "    y_train=iris.target\n",
    "    return model_selection.train_test_split(X_train, y_train,test_size=0.25,random_state=0,stratify=y_train)\n",
    "\n",
    "def test_LogisticRegression(*data):\n",
    "    X_train,X_test,y_train,y_test=data\n",
    "    regr = linear_model.LogisticRegression()\n",
    "    regr.fit(X_train, y_train)\n",
    "    print('Coefficients:%s, intercept %s'%(regr.coef_,regr.intercept_))\n",
    "    print('Score: %.2f' % regr.score(X_test, y_test))\n",
    "    \n",
    "def test_LogisticRegression_multinomial(*data):\n",
    "    X_train,X_test,y_train,y_test=data\n",
    "    regr = linear_model.LogisticRegression(multi_class='multinomial',solver='lbfgs')\n",
    "    regr.fit(X_train, y_train)\n",
    "    print('Coefficients:%s, intercept %s'%(regr.coef_,regr.intercept_))\n",
    "    print('Score: %.2f' % regr.score(X_test, y_test))\n",
    "    \n",
    "def test_LogisticRegression_C(*data):\n",
    "    X_train,X_test,y_train,y_test=data\n",
    "    Cs=np.logspace(-2,4,num=100)\n",
    "    scores=[]\n",
    "    for C in Cs:\n",
    "        regr = linear_model.LogisticRegression(C=C)\n",
    "        regr.fit(X_train, y_train)\n",
    "        scores.append(regr.score(X_test, y_test))\n",
    "    \n",
    "    fig=plt.figure()\n",
    "    ax=fig.add_subplot(1,1,1)\n",
    "    ax.plot(Cs,scores)\n",
    "    ax.set_xlabel(r\"C\")\n",
    "    ax.set_ylabel(r\"score\")\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_title(\"LogisticRegression\")\n",
    "    plt.show()\n",
    "\n",
    "if __name__=='__main__':\n",
    "    X_train,X_test,y_train,y_test=load_data() \n",
    "    test_LogisticRegression(X_train,X_test,y_train,y_test) \n",
    "    test_LogisticRegression_multinomial(X_train,X_test,y_train,y_test) \n",
    "    test_LogisticRegression_C(X_train,X_test,y_train,y_test) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
