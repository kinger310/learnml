{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习集训营八期第八周(深度学习)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2019年7月13日至7月15日期间完成，最晚提交时间（7月15日（周一）12时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试提交方式：请同学<font color=red><b>拷贝</b></font>该试卷后，将文件更名为同学姓名拼音-exam8（例如wangwei-exam8）后，移动至/0.Teacher/Exam/8/目录下后，再进行作答。\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅抄袭，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分处不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名: 王迪\n",
    "- 批改人： \n",
    "- 最终得分:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、简答题(共10题，1-8题每题5分，最后两题每题10分。共计60分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.请问深度学习与传统机器学习的相似与不同之处，以及进行深度学习任务的通用流程？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度学习是一种特殊的机器学习。\n",
    "与机器学习区别的是\n",
    "1. 数据依赖。随着数据规模的增加，深度学习性能也不断增长，而机器学习的性能会遇到瓶颈\n",
    "2. 计算依赖。深度学习需要大量的计算资源。如GPU等\n",
    "3. 特征处理。深度学习尝试从数据中直接获取高等级的特征，从而减少了机器学习的特征工程量\n",
    "4. 解释性。深度学习的模型可解释性通常比机器学习差\n",
    "\n",
    "深度学习通用流程\n",
    "1. 获取数据集，处理数据\n",
    "2. 确定数据维度，初识化参数\n",
    "3. 选用合适的网络、优化器和损失函数\n",
    "4. 训练模型。（一般采用mini-batch SGD）\n",
    "5. 将训练好的模型预测，评估模型。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.简要介绍下您了解的keras框架? 并简述如何使用这个框架进行任务的基本流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras是一个高层神经网络API，Keras由纯Python编写而成并基Tensorflow、Theano以及CNTK后端。具有如下特点：\n",
    "- 简易和快速的原型设计（keras具有高度模块化，极简，和可扩充特性）\n",
    "- 支持CNN和RNN，或二者的结合\n",
    "- 无缝CPU和GPU切换\n",
    "\n",
    "Keras的核心数据结构是“模型”，模型是一种组织网络层的方式。Keras中主要的模型是Sequential模型，Sequential是一系列网络层按顺序构成的栈。Keras训练模型的基本流程：\n",
    "1. 初始化模型。构建Sequential模型\n",
    "2. 选择合适的网络模型。将一些网络层通过.add()堆叠起来\n",
    "3. 用.compile()方法来编译模型，编译时需要指明损失函数、优化器和评价方法等\n",
    "4. 在训练数据上按batch进行一定次数的迭代来训练网络\n",
    "5. 对我们的模型进行评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.谈谈您对深度学习中的自适应学习率的了解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自适应学习率算法 \n",
    "当梯度下降算法刚开始时，需要一个较大的学习率，能够快速找到梯度下降的方向；\n",
    "而梯度下降算法接近收敛时，需要较小的学习率，能够减少下降的震荡，最终稳定在目标值。\n",
    "\n",
    "\n",
    "常用算法：\n",
    "\n",
    "AdaGrad算法根据自变量在每个维度的梯度值的大小来调整各个维度上的学习率，从而避免统一的学习率难以适应所有维度的问题。\n",
    "\n",
    "RMSProp算法解决这AdaGrad算法的这样一个问题：\n",
    "当学习率在迭代早期降得比较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能难找到一个有用的解。\n",
    "RMSProp算法将梯度按元素平方做指数加权移动平均。\n",
    "\n",
    "Adam算法在RMSProp算法的基础上，对小批量随机梯度也做了指数加权移动平均。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.请解释cross entropy 与 square error的不同之处？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross entropy 是交叉熵，是用来度量两个概率之间的相似度。square error 是平方误差。\n",
    "\n",
    "回归问题，用MSE；如果是分类问题，一般用CE。因为MSE容易发生梯度消失问题，而CE则不会。\n",
    "\n",
    "具体可以通过分类问题为例，假设我们的类别数量是T，最后一层使用softmax。\n",
    "\n",
    "对一条样本(x,c)而言，其label为c。在神经网络softmax之前那一层，共有T个神经元，对第c个神经元，有\n",
    "$$\n",
    "\\operatorname{softmax} \\rightarrow \\frac{e^{y_c}}{\\sum_{i=1}^{T} e^{y_{i}}},记作SOFT_c\n",
    "$$\n",
    "我们希望$y_c$越大越好，而与其并列的其他神经元输出值越小越好\n",
    "\n",
    "如果是MSE,则该样本的误差为:\n",
    "$$\n",
    "\\left(\\frac{e^{y_c}}{\\sum_{i=1}^{T} e^{y_i}}-1\\right)^{2}+\\sum_{j=1, j \\neq c}^{T}\\left(\\frac{e^{y_{c}}}{\\sum_{i=1}^{T} e^{y^{i}}}-1\\right)^{2}\n",
    "$$\n",
    "\n",
    "不妨对第一项求导\n",
    "$$\n",
    "\\frac{\\partial\\left(\\frac{e^{y_{c}}}{\\sum_{i=1}^{T} e^{y^{i}}}-1\\right)^{2}}{\\partial w} = 2\\left(\\frac{e^{y_{c}}}{\\sum_{i=1}^{T} e^{y_{i}}}-1\\right) \\cdot \\frac{e^{y_{c}}}{\\sum_{i=1}^{T} e^{y_{i}}} \\cdot\\left(1-\\frac{e^{y_{c}}}{\\sum_{i=1}^{T} e^{y_{i}}}\\right) \\cdot \\frac{\\partial y_{c}}{\\partial w}\n",
    "$$\n",
    "\n",
    "我们想让$SOFT_c$尽可能的接近于1.但是当$SOFT_c$接近于0时,梯度也就接近于0，这是通过梯度下降的变化微乎其微。即产生梯度消失效应。\n",
    "\n",
    "而对CE\n",
    "$$\n",
    "-\\log \\frac{e^{y_{c}}}{\\sum_{i=1}^{T} e^{y_{i}}}\n",
    "$$\n",
    "对w求导\n",
    "$$\n",
    "\\frac{\\partial\\left(-\\log \\frac{e^{y_{c}}}{\\sum_{i=1}^{T} e^{y_{c}}}\\right)}{\\partial w}=\\left(\\frac{e^{y_{c}}}{\\sum_{i=1}^{T} e^{y_{i}}}-1\\right) \\frac{\\partial y_{c}}{\\partial w}\n",
    "$$\n",
    "CE梯度趋近于0仅发生在目标几乎达成的时候.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.  使用激活函数解决了深度学习任务中的什么问题？ 常用的激活函数有哪些？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（一）通过多层感知机（Multilayer Perceptron）可以知道，即使神经网络中添加了隐藏层，其依然等价于一个单层神经网络。\n",
    "\n",
    "问题的根源在于，全连接层只是对数据做了仿射变换（线性变换加平移），多个仿射变换的叠加仍然是仿射变换。\n",
    "\n",
    "解决问题的方法就是引入一个非线性函数，这个非线性函数就是激活函数（Activation function）。\n",
    "\n",
    "（二）常用的激活函数是\n",
    "\n",
    "ReLu函数，又称Hinge function.\n",
    "$$ReLu(x) = \\max(x, 0)$$\n",
    "\n",
    "sigmoid 函数\n",
    "$$sigmoid(x) = \\frac {1} {1+ e^{-x}}$$\n",
    "\n",
    "tanh函数（双曲正切函数）\n",
    "$$tanh(x) = \\frac {1-e^{-2x}} {1+ e^{-2x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.工业界在训练深度学习模型时，采用训练方式多为SGD（mini-batch），请简述这种方式较其它方式的优点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小批量随机梯度下降每次随机均匀采样一个小批量训练样本来计算梯度。\n",
    "\n",
    "实验中，小批量随机梯度下降的学习率可以在迭代过程中自我衰减。\n",
    "\n",
    "通常，小批量随机梯度下降在一个迭代周期里的耗时介于梯度下降和随机梯度下降之间。\n",
    "即$Time of Epoch:  GD < miniBatch SGD < SGD$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.谈谈您了解的深度学习中的Momentum动量调整？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "动量法使用了指数加权平均的思想。它将过去时间步的梯度做了加权平均，且权重按时间步指数衰减。\n",
    "\n",
    "动量法使得相邻时间步的自变量更新在方向上更加一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.请简述深度学习进行自然语言处理任务的通用流程?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 读取数据集，数据预处理（分词等）\n",
    "2. 预训练词向量（word embedding）\n",
    "3. 选用模型（神经网络模型、CNN、RNN等）\n",
    "4. 训练模型\n",
    "5. 输出问题（编码器-解码器模型，束搜索，注意力机制等）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.请简要说明CNN网络与RNN网络的框架结构？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积神经网络是含有卷积层的神经网络。最常见的是二维卷积层，有高和宽两个空间维度。在二维卷积层中，一个二维输入数组和一个二维核数组通过互相关运算，输出一个二维数组。这个二维核数组又称为卷积核或过滤器。我们可以通过数据来学习卷积核。\n",
    "\n",
    "\n",
    "循环神经网络是使用循环计算的网络。它是为了更好地处理时序信息而设计。它引入状态变量来存储过去的信息，并用其与当前的输入共同决定当前的输出。RNN的隐藏状态可以捕捉当前时间步的序列的历史信息。RNN模型参数数量不随时间步的增加而增长。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.请简述神经网络使用和训练有哪些注意点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 模型应关注降低泛化误差。注意欠拟合问题和过拟合问题。欠拟合指无法得到较低的训练误差，过拟合指训练误差远小于测试集上的误差\n",
    "\n",
    "2. 可以使用权重衰减和丢弃法解决过拟合问题\n",
    "\n",
    "3. 在训练深度学习模型时，正向传播和反向传播相互依赖\n",
    "\n",
    "4. 数值稳定性问题。梯度衰减和梯度爆炸。\n",
    "\n",
    "5. 随机初识化模型参数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、实验题(共1题，共计40分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 请使用keras框架，构建CNN网络完成对MNIST数据集的训练，评估及预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T16:23:50.418589Z",
     "start_time": "2019-07-14T16:23:48.424988Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist # 从keras中导入mnist数据集\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T16:23:50.736488Z",
     "start_time": "2019-07-14T16:23:50.421741Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data() # 下载mnist数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T16:23:50.746052Z",
     "start_time": "2019-07-14T16:23:50.739694Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T16:23:50.979717Z",
     "start_time": "2019-07-14T16:23:50.748161Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "im = plt.imshow(x_train[0], cmap='gray')\n",
    "plt.show()\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T16:23:51.248108Z",
     "start_time": "2019-07-14T16:23:50.983065Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据处理\n",
    "x_train = x_train.reshape(60000,784) # 将每张图片处理成一个向量\n",
    "x_test = x_test.reshape(10000,784) \n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(x_train[0].shape)\n",
    "# 归一化\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# 标签处理\n",
    "y_train = keras.utils.to_categorical(y_train,10)\n",
    "y_test = keras.utils.to_categorical(y_test,10)\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T16:23:51.254417Z",
     "start_time": "2019-07-14T16:23:51.250918Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential # 导入序贯模型\n",
    "from keras.layers import Dense  # 导入全连接层\n",
    "from keras.optimizers import SGD # 导入优化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T16:23:51.321812Z",
     "start_time": "2019-07-14T16:23:51.256502Z"
    }
   },
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "model = Sequential()\n",
    "# 添加神经网络层\n",
    "model.add(Dense(512,activation='relu',input_shape=(784,)))\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T16:23:51.368790Z",
     "start_time": "2019-07-14T16:23:51.324014Z"
    }
   },
   "outputs": [],
   "source": [
    "# 编译模型，定义优化器、损失函数、评估标准\n",
    "model.compile(optimizer=SGD(),loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T16:24:35.742958Z",
     "start_time": "2019-07-14T16:23:51.371189Z"
    }
   },
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "model.fit(x_train,y_train,batch_size=64,epochs=10, validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T16:24:36.266164Z",
     "start_time": "2019-07-14T16:24:35.745269Z"
    }
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test,y_test)\n",
    "print(\"loss:\",score[0])\n",
    "print(\"accuracy:\",score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本周课程意见反馈(非必答)\n",
    "请同学围绕以下两点进行回答：\n",
    "- 自身总结：您自己在本周课程的学习，收获，技能掌握等方面进行总结，包括自身在哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n",
    "- 课程反馈：也可以就知识点，进度，难易度，教学方式，考试方式等等进行意见反馈，督促我们进行更有效的改进，为大家提供更优质的服务。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
