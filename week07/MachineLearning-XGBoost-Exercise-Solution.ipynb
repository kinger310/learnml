{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## MachineLearning XGBoost作业\n",
    "\n",
    "####  作业提交说明：\n",
    "- 位置：作业文件统一放置于/0.Teacher/Exercise/MachineLearning-XGBoost/下\n",
    "- 文件名：请先复制该notebook文件，并重新命名为(课程名)+(您姓名的全拼)，并按要求完成后保存\n",
    "- 时间：课程结束后的第二天前提交。\n",
    "- 注意：请勿抄袭，移动，修改，删除其他同学和原始空白的练习文件。\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简答题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 简述Boosting集成方法中的加法模型和前向分布算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp; &emsp;加法模型\n",
    "$$f\\left(x\\right)=\\sum_{m=1}^M\\beta_m b\\left(x;\\gamma_m\\right) $$\n",
    "其中，$b\\left(x;\\gamma_m\\right)$为基函数，$\\gamma_m$为基函数的参数，$\\beta_m$为基函数的系数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 算法 前向分步算法\n",
    "输入：训练数据集$T=\\{\\left(x_1,y_1\\right),\\left(x_2,y_2\\right),\\dots,\\left(x_N,y_N\\right)\\}$； 损失函数$L\\left(y,f\\left(x\\right)\\right)$；基函数集合$\\{b\\left(x;\\gamma\\right)\\}$；   \n",
    "输出：加法模型$f\\left(x\\right)$  \n",
    "（1）初始化$f_0\\left(x\\right)=0$  \n",
    "（2）对$m=1,2,\\dots,M$  \n",
    "（a）极小化损失函数\n",
    "$$\\left(\\beta_m,\\gamma_m\\right)=\\mathop{\\arg\\min}_{\\beta,\\gamma} \\sum_{i=1}^N L\\left(y_i, f_{m-1}\\left(x_i\\right)+\\beta b\\left(x_i;\\gamma\\right)\\right) $$  \n",
    "得到参数$\\beta_m$，$\\gamma_m$  \n",
    "（b）更新\n",
    "$$f_m\\left(x\\right)=f_{m-1}\\left(x\\right)+\\beta_m b\\left(x;\\gamma_m\\right) $$  \n",
    "（3）得到加法模型  \n",
    "$$f\\left(x\\right)=f_M\\left(x\\right)=\\sum_{m=1}^M\\beta_m b\\left(x;\\gamma_m\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 简述残差定义以及在提升决策树（BDT）中的作用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T07:32:18.613753Z",
     "start_time": "2019-03-13T07:32:18.609088Z"
    }
   },
   "source": [
    "&emsp;&emsp;当采用平方误差损失函数时，\n",
    "$$L\\left(y,f\\left(x\\right)\\right)=\\left(y-f\\left(x\\right)\\right)^2$$\n",
    "其损失变为\n",
    "$$\\begin{align}\n",
    "L\\left(y,f_{m-1}\\left(x\\right)+T\\left(x;\\Theta_m\\right)\\right) \n",
    "&=\\left[y-f_{m-1}\\left(x\\right)-T\\left(x;\\Theta_m\\right)\\right]^2 \\\\\n",
    "&=\\left[r-T\\left(x;\\Theta_m\\right)\\right]^2\n",
    "\\end{align}$$\n",
    "其中，\n",
    "$$r=y-f_{m-1}\\left(x\\right) $$\n",
    "是当前模型拟合数据的残差（residual）。对回归问题的提升决策树，只需要简单地拟合当前模型的残差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. XGBoost中决策树是如何定义的？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;训练数据集$\\mathcal{D}=\\{\\left(\\mathbf{x}_i,y_i\\right)\\}$，其中$\\mathbf{x}_i\\in\\mathbb{R}^m,y_i\\in\\mathbb{R},\\left|\\mathcal{D}\\right|=n$。\n",
    "&emsp;&emsp;决策树模型\n",
    "$$f\\left(\\mathbf{x}\\right)=w_{q\\left(\\mathbf{x}\\right)} $$\n",
    "其中，$q:\\mathbb{R}^m\\to \\{1,\\dots,T\\},w\\in\\mathbb{R}^T$,$T$为决策树叶子节点数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. XGBoost中正则化目标函数是如何定义的？并给出正则化项的说明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;正则化目标函数\n",
    "$$\\mathcal{L}\\left(\\phi\\right)=\\sum_i l\\left(\\hat{y}_i,y_i\\right)+\\sum_k \\Omega\\left(f_k\\right) $$\n",
    "其中，$\\Omega\\left(f\\right)=\\gamma T+\\frac{1}{2}\\lambda\\|w\\|^2=\\gamma T+\\frac{1}{2}\\lambda\\sum_{j=1}^T w_j^2$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 简述第$t$轮目标函数的泰勒二次展开，及其对最优解的求解过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;第$t$轮目标函数\n",
    "$$\\mathcal{L}^{\\left(t\\right)}=\\sum_{i=1}^n l\\left(y_i,\\hat{y}^{\\left(t-1\\right)}_i+f_t\\left(\\mathbf{x}_i\\right)\\right)+\\Omega\\left(f_t\\right) $$\n",
    "&emsp;&emsp;第$t$轮目标函数在$\\hat{y}^{\\left(t-1\\right)}$处的二阶泰勒展开\n",
    "$$\\mathcal{L}^{\\left(t\\right)}\\simeq\\sum_{i=1}^n\\left[l\\left(y_i,\\hat{y}^{\\left(t-1\\right)}\\right)+g_i f_t\\left(\\mathbf{x}_i\\right)+\\frac{1}{2}h_i f^2_t\\left(\\mathbf{x}_i\\right)\\right]+\\Omega\\left(f_t\\right) $$\n",
    "其中，$g_i=\\partial_{\\hat{y}^{\\left(t-1\\right)}}l\\left(y_i,\\hat{y}^{\\left(t-1\\right)}\\right),h_i=\\partial^2_{\\hat{y}^{\\left(t-1\\right)}}l\\left(y_i,\\hat{y}^{\\left(t-1\\right)}\\right)$。\n",
    "&emsp;&emsp;第$t$轮目标函数的二阶泰勒展开移除关于\n",
    "$f_t\\left(\\mathbf{x}_i\\right)$常数项\n",
    "$$\\begin{align}\n",
    "\\tilde{\\mathcal{L}}^{\\left(t\\right)}&=\\sum_{i=1}^n\\left[g_i f_t\\left(\\mathbf{x}_i\\right)+\\frac{1}{2}h_i f^2_t\\left(\\mathbf{x}_i\\right)\\right]+\\Omega\\left(f_t\\right)  \\\\\n",
    "&=\\sum_{i=1}^n\\left[g_i f_t\\left(\\mathbf{x}_i\\right)+\\frac{1}{2}h_i f^2_t\\left(\\mathbf{x}_i\\right)\\right]+\\gamma T+\\frac{1}{2}\\lambda\\sum_{j=1}^T w_j^2\n",
    "\\end{align} \\\\$$  \n",
    "定义叶结点$j$上的样本的下标集合$I_j=\\{i|q\\left(\\mathbf{x}_i\\right)=j\\}$，则目标函数可表示为按叶结点累加的形式\n",
    "$$\\tilde{\\mathcal{L}}^{\\left(t\\right)}=\\sum_{j=1}^T\\left[\\left(\\sum_{i\\in I_j}g_i\\right)w_j+\\frac{1}{2}\\left(\\sum_{i\\in I_j}h_i+\\lambda\\right)w_j^2\\right]+\\gamma T $$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于$$w_j^*=\\mathop{\\arg\\min}_{w_j}\\tilde{\\mathcal{L}}^{\\left(t\\right)}$$\n",
    "可令$$\\frac{\\partial\\tilde{\\mathcal{L}}^{\\left(t\\right)}}{\\partial w_j}=0$$\n",
    "得到每个叶结点$j$的最优分数为\n",
    "$$w_j^*=-\\frac{\\sum_{i\\in I_j}g_i}{\\sum_{i\\in I_j} h_i+\\lambda} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.简述XGBoost中特征分裂点的选择依据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代入每个叶结点$j$的最优分数，得到最优化目标函数值\n",
    "$$\\tilde{\\mathcal{L}}^{\\left(t\\right)}\\left(q\\right)=-\\frac{1}{2}\\sum_{j=1}^T \n",
    "\\frac{\\left(\\sum_{i\\in I_j} g_i\\right)^2}{\\sum_{i\\in I_j} h_i+\\lambda}+\\gamma T $$\n",
    "&emsp;&emsp;假设$I_L$和$I_R$分别为分裂后左右结点的实例集，令$I=I_L\\cup I_R$，则分裂后损失减少量由下式得出\n",
    "$$\\mathcal{L}_{split}=\\frac{1}{2}\\left[\\frac{\\left(\\sum_{i\\in I_L} g_i\\right)^2}{\\sum_{i\\in I_L}h_i+\\lambda}+\\frac{\\left(\\sum_{i\\in I_R} g_i\\right)^2}{\\sum_{i\\in I_R}h_i+\\lambda}-\\frac{\\left(\\sum_{i\\in I} g_i\\right)^2}{\\sum_{i\\in I}h_i+\\lambda}\\right]-\\gamma $$\n",
    "用以评估待分裂结点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.XGBoost的模型参数主要有那三类？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 通用参数：控制整体功能；\n",
    "2. 提升器参数：在每一步控制单个提升器（tree、regression）；\n",
    "3. 学习任务参数：控制最优化执行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.说明booster参数的意义。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "booster [default=gbtree]  \n",
    "选择每次迭代的模型，有两个选择：\n",
    "+ gbtree：基于树的模型；\n",
    "+ gbliner：线性模型。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. 列举出三种控制拟合程度的模型参数，并说明其各自的含义。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T07:27:50.976486Z",
     "start_time": "2019-03-13T07:27:50.970919Z"
    }
   },
   "source": [
    "min_child_weight [default=1]\n",
    "+ 定义最小叶子节点样本权重和；\n",
    "+ 用于控制过拟合。较大的值可以避免模型学习到局部的特殊样本；\n",
    "+ 太大的值会导致欠拟合。  \n",
    "\n",
    "max_depth [default=6]\n",
    "+ 树的最大深度；\n",
    "+ 用于控制过拟合。较大的值模型会学到更具体更局部的样本；\n",
    "+ 典型值为3-10。  \n",
    "\n",
    "max_leaf_nodes\n",
    "+ 树中终端节点或叶子的最大数目；\n",
    "+ 可以代替max_depth参数。由于创建的是二叉树，一个深度为$n$的树最多生成$2^n$个叶子；\n",
    "+ 如果该参数被定义，max_depth参数将被忽略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.说明objective参数的意义。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "objective [default=reg:linear]  \n",
    "该参数定义需要被最小化的损失函数。常用值有：\n",
    "+ binary:logistic 二分类的逻辑回归，返回预测的概率(不是类别)；\n",
    "+ multi:softmax 使用softmax的多分类器，返回预测的类别(不是概率)。在这种情况下，你还需要多设一个参数：num_class(类别数目)；\n",
    "+ multi:softprob 和multi:softmax参数一样，但是返回的是每个数据属于各个类别的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
