{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习集训营八期第七周(集成学习与NLP)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：本周试卷请同学在2019年7月7日至7月10日期间完成，最晚提交时间下周三12：00前结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试提交方式：请同学<font color=red><b>拷贝</b></font>该试卷后，将文件更名为同学姓名拼音-exam7（例如wangwei-exam7）后，移动至/0.Teacher/Exam/7/目录下后，再进行作答。\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅抄袭，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分处不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名: 王迪\n",
    "- 批改人： \n",
    "- 最终得分:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简答题(共4题，每题15分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.工业界上模型融合有三大类方式？试简述每类方式其思想？（面试题）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型融合 bagging, boosting, stacking和blending\n",
    "\n",
    "bagging， bootstrap aggregating.基于每个采样集训练出一个基学习器，再将这些基学习器结合，在对预测输出进行结合时，Bagging通常对分类任务使用简单投票法，对回归任务采用简单平均法。Bagging主要减少variance。\n",
    "\n",
    "boosting分类器通过串行训练获得，通过关注已有分类器错分的那些数据,提升他们的权重，来获得新的分类器。Boosting主要减少bias。\n",
    "\n",
    "Stacking先从初始数据集训练出初级学习器（个体学习器），然后“生成”一个新数据集用于训练次级学习器（用于结合的学习器，也称元学习器meta-learner）。新的数据集中，初级学习器的输出被当作样例输入特征。次级学习器的输入属性表示和次级学习算法对Stacking集成的泛化性能有很大影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.我们可以将xgboost的中众多参数分类为哪三类？请分别写出哪些参数可以用什么方式用来控制过拟合？"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "通用参数：控制整体功能；\n",
    "提升器参数：在每一步控制单个提升器（tree、regression）；\n",
    "学习任务参数：控制最优化执行。\n",
    "\n",
    "\n",
    "min_child_weight [default=1]\n",
    "定义最小叶子节点样本权重和；\n",
    "用于控制过拟合。较大的值可以避免模型学习到局部的特殊样本；\n",
    "太大的值会导致欠拟合。\n",
    "\n",
    "max_depth [default=6]\n",
    "树的最大深度；\n",
    "用于控制过拟合。较大的值模型会学到更具体更局部的样本；\n",
    "典型值为3-10。\n",
    "\n",
    "max_leaf_nodes\n",
    "树中终端节点或叶子的最大数目；\n",
    "可以代替max_depth参数。由于创建的是二叉树，一个深度为 𝑛 的树最多生成 2𝑛 个叶子；\n",
    "如果该参数被定义，max_depth参数将被忽略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 请详细说明NLP任务中会常用到的nltk中的那些方法，每个方法的具体作用？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词频统计 FreqDist \n",
    "\n",
    "停用词 stopwords\n",
    "\n",
    "从 WordNet 获取同义词\n",
    "\n",
    "词干提取 stemm\n",
    "\n",
    "词性标注\n",
    "\n",
    "分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "special:2\n",
      "cases:1\n",
      "enough:1\n",
      "break:1\n",
      "rules.:1\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "\n",
    "text = \"Special cases aren't special enough to break the rules.\"\n",
    "\n",
    "tokens = [t.lower() for t in text.split()] \n",
    "clean_tokens = tokens[:] \n",
    "sr = stopwords.words('english')\n",
    "\n",
    "for token in tokens: \n",
    "    if token in stopwords.words('english'): \n",
    "        clean_tokens.remove(token) \n",
    "freq = nltk.FreqDist(clean_tokens) \n",
    "for key,val in freq.items(): \n",
    "    print (str(key) + ':' + str(val))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "increas\n",
      "increase\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer() \n",
    "print(stemmer.stem('increases'))    \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('increases'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 简述TF-IDF算法的工作原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$TFIDF_{ij} = TF*IDF$$\n",
    "\n",
    "\n",
    "TF term frequency. 单词频率，单词在文本中出现的频率。\n",
    "\n",
    "$$TF = \\frac {t_{ij}}{t_j}$$   \n",
    "\n",
    "$t_{ij}$是单词$w_{i}$出现在文本$d_j$的频数,$t_{j}$是文本$d_j$的所有单词频数之和。\n",
    "\n",
    "IDF inverse document frequency.逆文本频率。\n",
    "\n",
    "$$IDF = \\log \\frac {df}{df_i}$$\n",
    "\n",
    "$df$是文本集合的文本个数。$df_i$表示含有单词$w_i$的文本数。\n",
    "\n",
    "\n",
    "TF表示单词在文本中出现的频率，频率越高，重要度越高。IDF衡量单词的区分度，IDF越大，单词越能表示其所在文本的特点。一个单词在一个文本中的TF-IDF表示其综合重要度。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验题(共2题，每题20分，共计40分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 使用XGBoost内置方式，导入iris数据完成分类问题\n",
    "- （要求以不同参数设置xgboost运行并比对），最后给出实验总结报告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score, roc_auc_score\n",
    "\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集 (100, 4)\n",
      "测试集 (50, 4)\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=50, random_state=310)\n",
    "\n",
    "print('训练集',X_train.shape)\n",
    "print('测试集',X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDM=xgb.DMatrix(data=X_train, label=y_train)\n",
    "testDM=xgb.DMatrix(data=X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-merror:0.02\ttrain-merror:0.03\n",
      "[1]\teval-merror:0.04\ttrain-merror:0.05\n",
      "[2]\teval-merror:0.04\ttrain-merror:0.05\n",
      "[3]\teval-merror:0.04\ttrain-merror:0.03\n",
      "[4]\teval-merror:0.04\ttrain-merror:0.05\n",
      "[5]\teval-merror:0.04\ttrain-merror:0.05\n",
      "accuracy: 0.96\n",
      "recall: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "watch_list = [(testDM, 'eval'), (trainDM, 'train')]\n",
    "param = {\n",
    "    'max_depth': 3, \n",
    "    'eta': 0.01, \n",
    "    'silent': 1, \n",
    "    'objective': 'multi:softmax', \n",
    "    'num_class': 3, \n",
    "    'subsample':0.8\n",
    "}\n",
    "\n",
    "model = xgb.train(param,trainDM, num_boost_round=6, evals=watch_list)\n",
    "y_hat = model.predict(testDM)\n",
    "acc_score = accuracy_score(y_test,y_hat)\n",
    "recall = recall_score(y_test,y_hat,average='macro')\n",
    "\n",
    "\n",
    "print(\"accuracy:\",acc_score)\n",
    "print(\"recall:\",recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, eval_metric='merror',\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "       nthread=None, num_class=3, objective='multi:softmax',\n",
       "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=None, silent=None, subsample=1, verbosity=1),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'max_depth': [2, 3, 4, 5], 'learning_rate': [0.01, 0.03, 0.1, 0.3], 'n_estimators': [10, 20, 30, 40, 50]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GridSearchCV\n",
    "param_grid = {'max_depth': [2,3,4,5], 'learning_rate':[0.01, 0.03, 0.1, 0.3],'n_estimators':[10,20,30,40,50]}\n",
    "mdl = xgb.XGBClassifier(objective=\"multi:softmax\", num_class=3, eval_metric='merror')\n",
    "grid_model = GridSearchCV(mdl, param_grid=param_grid, cv=5)\n",
    "grid_model.fit(X_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 10}\n",
      "0.94\n"
     ]
    }
   ],
   "source": [
    "print(grid_model.best_params_)    \n",
    "print(grid_model.best_score_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.98\n",
      "recall: 0.9833333333333334\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid_model.predict(X_test)\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(\"accuracy:\",acc_score)\n",
    "print(\"recall:\",recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结:\n",
    "Iris分类是一个经典的3分类问题。\n",
    "\n",
    "由于各个类别的数据分布较为明显，所以上面几乎没有怎么调参都能够达到很好的准确率和召回率 \n",
    "\n",
    "使用GridSearchCV进行网格搜索时，可得最佳的参数是{'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 10}\n",
    "测试集上达到了98%的准确率和召回率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 2.使用NLTK，spaCy等相应库,按NLP任务基本流程完成一段文本的处理\n",
    "- 数据位于/0.Teacher/Data/car_news.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>啊啊啊本报济南1月2日电 (记者潘俊强)日前，备受关注的济南“专车案”一审宣判。济南市市...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015年1月，陈超在使用专车软件开“专车”送客时，被济南市客管中心认定为非法运营的“黑...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>济南市市中区人民法院审理认为，本案中，陈超在与乘客通过网络约车软件取得联系后，使用未取得...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>同时，虽然被告对未经许可擅自从事出租汽车客运的行为可以依法进行处罚，但原告在本案所涉道路...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>本案宣判后，原告当庭表示不上诉，被告未当庭表示是否上诉。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content\n",
       "0  　　啊啊啊本报济南1月2日电 (记者潘俊强)日前，备受关注的济南“专车案”一审宣判。济南市市...\n",
       "1  　　2015年1月，陈超在使用专车软件开“专车”送客时，被济南市客管中心认定为非法运营的“黑...\n",
       "2  　　济南市市中区人民法院审理认为，本案中，陈超在与乘客通过网络约车软件取得联系后，使用未取得...\n",
       "3  　　同时，虽然被告对未经许可擅自从事出租汽车客运的行为可以依法进行处罚，但原告在本案所涉道路...\n",
       "4                     　　本案宣判后，原告当庭表示不上诉，被告未当庭表示是否上诉。"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_news = pd.read_csv(\"data/car_news.csv\", index_col=0)\n",
    "car_news.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set()\n",
    "with open('data/chinese_stopwords.txt', encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        stopwords.add(line.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\wangdi03\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.895 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>process_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>啊啊啊本报济南1月2日电 (记者潘俊强)日前，备受关注的济南“专车案”一审宣判。济南市市...</td>\n",
       "      <td>本报 济南 月 日电   记者 潘俊强  日前  备受 关注 济南  专车 案  一审 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015年1月，陈超在使用专车软件开“专车”送客时，被济南市客管中心认定为非法运营的“黑...</td>\n",
       "      <td>年 月  陈超 在 使用 专车 软件 开  专车  送客 时  济南市 客管 中心 认定...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>济南市市中区人民法院审理认为，本案中，陈超在与乘客通过网络约车软件取得联系后，使用未取得...</td>\n",
       "      <td>济南市 市中区 人民法院 审理 认为  本案 中  陈超 在 与 乘客 通过 网络 约车...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>同时，虽然被告对未经许可擅自从事出租汽车客运的行为可以依法进行处罚，但原告在本案所涉道路...</td>\n",
       "      <td>同时  虽然 被告 未经许可 擅自 从事 出租汽车 客运 行为 可以 依法 进行 处罚 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>本案宣判后，原告当庭表示不上诉，被告未当庭表示是否上诉。</td>\n",
       "      <td>本案 宣判 后  原告 当庭 表示 不 上诉  被告 未 当庭 表示 是否 上诉</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  　　啊啊啊本报济南1月2日电 (记者潘俊强)日前，备受关注的济南“专车案”一审宣判。济南市市...   \n",
       "1  　　2015年1月，陈超在使用专车软件开“专车”送客时，被济南市客管中心认定为非法运营的“黑...   \n",
       "2  　　济南市市中区人民法院审理认为，本案中，陈超在与乘客通过网络约车软件取得联系后，使用未取得...   \n",
       "3  　　同时，虽然被告对未经许可擅自从事出租汽车客运的行为可以依法进行处罚，但原告在本案所涉道路...   \n",
       "4                     　　本案宣判后，原告当庭表示不上诉，被告未当庭表示是否上诉。   \n",
       "\n",
       "                                     process_content  \n",
       "0    本报 济南 月 日电   记者 潘俊强  日前  备受 关注 济南  专车 案  一审 ...  \n",
       "1    年 月  陈超 在 使用 专车 软件 开  专车  送客 时  济南市 客管 中心 认定...  \n",
       "2    济南市 市中区 人民法院 审理 认为  本案 中  陈超 在 与 乘客 通过 网络 约车...  \n",
       "3    同时  虽然 被告 未经许可 擅自 从事 出租汽车 客运 行为 可以 依法 进行 处罚 ...  \n",
       "4          本案 宣判 后  原告 当庭 表示 不 上诉  被告 未 当庭 表示 是否 上诉   "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(text):\n",
    "    words = jieba.lcut(text)\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        w = word.strip()\n",
    "        if w not in stopwords and (not w.isdigit()):\n",
    "            new_words.append(re.sub(r'\\W','',w))\n",
    "\n",
    "    result = ' '.join(new_words)\n",
    "    return result\n",
    "\n",
    "car_news[\"process_content\"] = car_news[\"content\"].astype(str).apply(lambda x: preprocess(x))\n",
    "car_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11959x33786 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 350265 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#特征构建，\n",
    "tfVec = TfidfVectorizer()\n",
    "TF = tfVec.fit_transform(car_news['process_content'])\n",
    "TF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本周课程意见反馈(非必答)\n",
    "请同学围绕以下两点进行回答：\n",
    "- 自身总结：您自己在本周课程的学习，收获，技能掌握等方面进行总结，包括自身在哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n",
    "- 课程反馈：也可以就知识点，进度，难易度，教学方式，考试方式等等进行意见反馈，督促我们进行更有效的改进，为大家提供更优质的服务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
